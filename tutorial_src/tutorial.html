<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.4.1: http://docutils.sourceforge.net/" />
<title>Tutorial</title>
<meta name="author" content="Pietro Berkes and Tiziano Zito" />
<meta name="copyright" content="This document has been placed in the public domain." />
<meta content="Modular toolkit for Data Processing (MDP) Tutorial" name="description" />
<meta content="data processing, toolkit, scipy, python, SFA, ICA, PCA" name="keywords" />
<style type="text/css">

body {
	background:white;
	/*font-family:'Trebuchet MS',Verdana,sans-serif;*/
	font-family:sans-serif;
	text-align:justify;
	margin-bottom:50px;
	}

a:visited, a:link { 
	   color:#878F30; 
	   text-decoration:none;
}

a:hover {
	background: #DAE55C;
	color: black;
}

/* tables */
table { border-style:none; }
td { vertical-align:top; }
.screentable { text-align:center; }
.td_header { vertical-align:middle;
	     padding-left:15px;
	}

/* lists */

UL { padding-left:1em; 
list-style-image: url('blkpearl.png');
   }

UL LI {
	margin:0.5em 0.5em 0.5em 0.5em;
}

OL LI {
	margin:0.5em 0.5em 0.5em 0.5em;
}

UL.nice { list-style-type:none; }

UL.nice LI {
        background: #F0F8FF;
	border: 1px solid #878F30;
	padding:0.5em 0.5em 0.5em 0.5em;
}

OL.nice LI {
        background: #F0F8FF;
	border: 1px solid #878F30;
	padding:0.5em 0.5em 0.5em 0.5em;
	list-style-type:lower-roman;
}

DT { font-weight:bold; }

H1 {
	font-family: serif;
}

.news {
	background-color:#E0F060
	}

#news-box {
  margin-top: 28px;
  /*margin-right: 35%;*/
  padding: .5em; 
  border: 1px solid #878F30;
  font-size:smaller;
}

.highlight {
	font-weight:bold;
/*	background-color:#E0F060;*/
}

.smaller_font {
	font-size:smaller;
}

code {
	text-align:left;
	color:#878F30;
}

pycode {
	text-align:left;
	color:#878F30;
	margin-left:2em;
	font-weight:bold;
}

pre.literal-block {
  overflow: auto;
  background: #DAE55C; 
  padding: .5em; 
  margin: 1em;
  border: 1px solid #878F30;
}

.pycomment {
	color:maroon;
	font-weight:bold;
}

.faqquest {
	font-size:larger;
	font-weight:bold;
}

.faqansw {
	font-style:normal;
}

.faqcit {
	text-align:justify;
	font-size:smaller;
	font-style:italic;
	padding:0.5em 0.5em 0.5em 0.5em;
}

.soft {
	font-weight:bold;
}

#header {
	background:#DAE55C;
	text-align:center;
    border: 1px solid #878F30;
	padding:5px;
	text-decoration:none;
	font-size:250%;
	font-weight:bold;
    color:#878F30;
	/*margin-left:3.5em;*/
}

IMG.whiteb  { 	
	border-width:2px;
	border-color:white;
	border-style:solid;
 }

.img_header a:hover {
background:#DAE55C;
}

#sidebar{ 
clear: left;
float: left;
width: 10em;
margin: 10px 10px 10px 0;
padding: 0;
/*font-size: 0.9em;*/
}

#sidebar ul {	
list-style: none;
width: auto;
margin: 18px 0 20px 0;
padding: 0;
text-align:center; 
/*font-size:1em;*/
}	

#sidebar li {
margin-top:4px;
margin-bottom: 4px;
}

#sidebar li a {
font-weight: bold;
/*height: 20px;*/
text-decoration: none;
color: #878F30;
display: block;
padding: 4px 0px 4px 0px;
background:#DAE55C;
border: 1px solid #878F30;
}
	
#sidebar li a:hover {
color:black;
}

#container {
	padding-right:30px;
	padding-left:30px;
	margin-right:auto;
	margin-left:auto;
	max-width:800px; 
}

#content {
    margin-left:11em;
}

#footer {
}


/*------------ docutils ---------------*/
.first {
  margin-top: 0 ! important }

.last {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: 1px solid #708090;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }


div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font-family: serif ;
  font-size: 100% }

pre.line-block {
  font-family: serif ;
  font-size: 100% }

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

table.citation {
  border-left: solid thin gray }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid thin black }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

tt.docutils { 
  font-weight: bold ;
  /*border: 1px solid #708090;*/
  /*background-color: #5A6375#f9c852;*/ 
}

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body><div id="container">
<div id="header">

<table width="100%">
<tr>
<td align="left" class="td_header">Modular toolkit for<br>Data Processing</td>
<td align="right" class="img_header">
<a href="logo_animation.html">
<img src="logo.png" alt="MDP logo" title="click to see the animated logo!" class="whiteb"/></a></td></tr>
</table>
</div>

<div id="sidebar">
<ul>
<li><a href="index.html">Home</a>
<li><a href="tutorial.html">Tutorial</a>
<li><a href="index.html#DOWINS">Download</a>
<li><a href="tutorial.html#node-list">Node list</a>
<li><a href="docs/api/index.html">API</a>
<li><a href="http://sourceforge.net/mail/?group_id=116959">Mailing list</a>
<li><a href="symeig.html">symeig</a>
</ul>
</div>

<div id="content">

<div class="document" id="tutorial">
<h1 class="title">Tutorial</h1>
<table class="docinfo" frame="void" rules="none">
<col class="docinfo-name" />
<col class="docinfo-content" />
<tbody valign="top">
<tr><th class="docinfo-name">Author:</th>
<td>Pietro Berkes and Tiziano Zito</td></tr>
<tr class="field"><th class="docinfo-name">Homepage:</th><td class="field-body"><a class="reference" href="http://mdp-toolkit.sourceforge.net">http://mdp-toolkit.sourceforge.net</a></td>
</tr>
<tr><th class="docinfo-name">Copyright:</th>
<td>This document has been placed in the public domain.</td></tr>
<tr><th class="docinfo-name">Version:</th>
<td>2.2</td></tr>
</tbody>
</table>
This document is also available as <a href="http://prdownloads.sourceforge.net/mdp-toolkit/MDP2_2_tutorial.pdf?download">pdf file</a> (260 KB).<p>This is a guide to basic and some more advanced features of
the MDP library. Besides the present tutorial, you can learn
more about MDP by using the standard Python tools.
All MDP nodes have doc-strings, the public
attributes and methods have telling names: All information about a
node can be obtained using  the <tt class="docutils literal"><span class="pre">help</span></tt> and <tt class="docutils literal"><span class="pre">dir</span></tt> functions within
the Python interpreter. In addition to that, an automatically generated
<a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/index.html">API</a> is
available.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p>Code snippets throughout the script will be denoted by:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; print &quot;Hello world!&quot;
Hello world!
</pre>
<p>To run the following code examples don't forget to import mdp
in your Python session with:</p>
<pre class="literal-block">
&gt;&gt;&gt; import mdp
</pre>
<p class="last">You'll find all the code of this tutorial within the <tt class="docutils literal"><span class="pre">demo</span></tt> directory
in the MDP installation path.</p>
</div>
<div class="contents topic">
<p class="topic-title first"><a id="contents" name="contents">Contents</a></p>
<ul class="simple">
<li><a class="reference" href="#introduction" id="id4" name="id4">Introduction</a></li>
<li><a class="reference" href="#quick-start" id="id5" name="id5">Quick Start</a></li>
<li><a class="reference" href="#nodes" id="id6" name="id6">Nodes</a><ul>
<li><a class="reference" href="#node-instantiation" id="id7" name="id7">Node Instantiation</a></li>
<li><a class="reference" href="#node-training" id="id8" name="id8">Node Training</a></li>
<li><a class="reference" href="#node-execution" id="id9" name="id9">Node Execution</a></li>
<li><a class="reference" href="#node-inversion" id="id10" name="id10">Node Inversion</a></li>
<li><a class="reference" href="#writing-your-own-nodes-subclassing-node" id="id11" name="id11">Writing your own nodes: subclassing Node</a></li>
</ul>
</li>
<li><a class="reference" href="#flows" id="id12" name="id12">Flows</a><ul>
<li><a class="reference" href="#flow-instantiation-training-and-execution" id="id13" name="id13">Flow instantiation, training and execution</a></li>
<li><a class="reference" href="#flow-inversion" id="id14" name="id14">Flow inversion</a></li>
<li><a class="reference" href="#flows-are-container-type-objects" id="id15" name="id15">Flows are container type objects</a></li>
<li><a class="reference" href="#crash-recovery" id="id16" name="id16">Crash recovery</a></li>
</ul>
</li>
<li><a class="reference" href="#iterators" id="id17" name="id17">Iterators</a></li>
<li><a class="reference" href="#checkpoints" id="id18" name="id18">Checkpoints</a></li>
<li><a class="reference" href="#hierarchical-networks" id="id19" name="id19">Hierarchical Networks</a></li>
<li><a class="reference" href="#a-real-life-example-logistic-maps" id="id20" name="id20">A real life example (Logistic maps)</a></li>
<li><a class="reference" href="#another-real-life-example-growing-neural-gas" id="id21" name="id21">Another real life example (Growing neural gas)</a></li>
<li><a class="reference" href="#node-list" id="id22" name="id22">Node List</a></li>
<li><a class="reference" href="#additional-utilities" id="id23" name="id23">Additional utilities</a><ul>
<li><a class="reference" href="#graph-module" id="id24" name="id24">Graph module</a></li>
</ul>
</li>
<li><a class="reference" href="#to-do" id="id25" name="id25">To Do</a></li>
<li><a class="reference" href="#contributors" id="id26" name="id26">Contributors</a></li>
</ul>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id4" id="introduction" name="introduction">Introduction</a></h1>
<p>Modular toolkit for Data Processing (MDP) is a data processing
framework written in Python.</p>
<p>From the user's perspective, MDP consists of a collection of trainable
supervised and unsupervised algorithms or other data processing units
(nodes) that can be combined into data processing flows and more
complex feed-forward network architectures. Given a
sequence of input data, MDP takes care of successively training or
executing all nodes in the network. This structure allows to specify
complex algorithms as a sequence of simpler data processing steps in a
natural way. Training can be performed using small chunks of input
data, so that the use of very large data sets becomes possible while
reducing the memory requirements. Memory usage can also be minimized
by defining the internals of the nodes to be single precision.</p>
<p>The base of readily available algorithms includes Principal Component
Analysis (PCA and NIPALS), three flavors of Independent Component
Analysis (CuBICA, FastICA, and JADE), Slow Feature Analysis,
Independent Slow Feature Analysis, Gaussian Classifiers, Growing
Neural Gas, Fisher Discriminant Analysis, Factor Analysis, Restricted
Boltzmann Machine, and many more.  The full list of implemented nodes
can be found in the <a class="reference" href="#node-list">Node List</a> section.</p>
<p>From the developer's perspective, MDP is a framework to make the
implementation of new supervised and unsupervised algorithms easier.
The basic class <tt class="docutils literal"><span class="pre">Node</span></tt> takes
care of tedious tasks like numerical type and dimensionality checking,
leaving the developer free to concentrate on the implementation of the
training and execution phases. The node then automatically integrates
with the rest of the library and can be used in a flow together with
other nodes. A node can have multiple training phases and even an
undetermined number of phases. This allows for example the
implementation of algorithms that need to collect some statistics on
the whole input before proceeding with the actual training, or others
that need to iterate over a training phase until a convergence
criterion is satisfied. The ability to train each phase using chunks
of input data is maintained if the chunks are generated with
iterators. Moreover, crash recovery is optionally available: in case
of failure, the current state of the flow is saved for later
inspection.</p>
<p>MDP has been written in the context of theoretical research in
neuroscience, but it has been designed to be helpful in any context
where trainable data processing algorithms are used. Its simplicity on
the user side together with the reusability of the implemented nodes
make it also a valid educational tool.</p>
<p>As its user and contributor base is steadily increasing, MDP appears as a good
candidate for becoming a common repository of user-supplied, freely
available, Python implemented data processing algorithms.</p>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id5" id="quick-start" name="quick-start">Quick Start</a></h1>
<p>Using MDP is as easy as:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; import mdp
&gt;&gt;&gt; # perform pca on some data x
...
&gt;&gt;&gt; y = mdp.pca(x)
&gt;&gt;&gt; # perform ica on some data x using single precision
...
&gt;&gt;&gt; y = mdp.fastica(x, dtype='float32')
</pre>
<p>A complete list of all short-cut functions like <tt class="docutils literal"><span class="pre">pca</span></tt> or <tt class="docutils literal"><span class="pre">fastica</span></tt>
can be obtained as follows:</p>
<pre class="literal-block">
&gt;&gt;&gt; dir(mdp.helper_funcs)
['__builtins__', '__doc__', '__file__', '__name__',
'cubica', 'factor_analysis', 'fastica', 'get_eta',
'isfa', 'mdp', 'pca', 'sfa', 'sfa2', 'whitening']
</pre>
<p>MDP is of course much more than this: it allows to combine different
algorithms and other data processing elements (nodes) into data
processing sequences (flows), and more general feed-forward architectures
(with the new <tt class="docutils literal"><span class="pre">hinet</span></tt> subpackage).
Moreover, it provides a framework that
makes the implementation of new algorithms easy and intuitive.</p>
<p>MDP requires the numerical Python extensions <a class="reference" href="http://numpy.scipy.org/">numpy</a> or <a class="reference" href="http://www.scipy.org/">scipy</a>.
In its namespace MDP offers references
to the main modules <tt class="docutils literal"><span class="pre">numpy</span></tt> or <tt class="docutils literal"><span class="pre">scipy</span></tt>, and the subpackages
<tt class="docutils literal"><span class="pre">linalg</span></tt>, <tt class="docutils literal"><span class="pre">random</span></tt>, and <tt class="docutils literal"><span class="pre">fft</span></tt>
as <tt class="docutils literal"><span class="pre">mdp.numx</span></tt>, <tt class="docutils literal"><span class="pre">mdp.numx_linalg</span></tt>, <tt class="docutils literal"><span class="pre">mdp.numx_rand</span></tt>, and
<tt class="docutils literal"><span class="pre">mdp.numx_fft</span></tt>. This is done to possibly support additional
numerical extensions in the future. At import time MDP will select
<tt class="docutils literal"><span class="pre">scipy</span></tt> if available, otherwise <tt class="docutils literal"><span class="pre">numpy</span></tt> will be loaded. You can
force the use of a numerical extension by setting the environment
variable <tt class="docutils literal"><span class="pre">MDPNUMX=numpy</span></tt> or <tt class="docutils literal"><span class="pre">MDPNUMX=scipy</span></tt>.</p>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id6" id="nodes" name="nodes">Nodes</a></h1>
<p>A node is the basic unit in MDP and it represents a data processing
element, like for example a learning algorithm, a filter, a
visualization step, etc. Each node can have one or more training
phases, during which the internal structures are learned from training
data (e.g. the weights of a neural network are adapted or the
covariance matrix is estimated) and an execution phase, where new data
can be processed forwards (by processing the data through the node) or
backwards (by applying the inverse of the transformation computed by
the node if defined). The <tt class="docutils literal"><span class="pre">Node</span></tt> class is designed to make the
implementation of new algorithms easy and intuitive, for example by
setting automatically input and output dimension and by casting the
data to match the numerical type (e.g. float or double) of the
internal structures. <tt class="docutils literal"><span class="pre">Node</span></tt> was designed to be applied to arbitrarily
long sets of data: the internal structures can be updated
incrementally by sending chunks of the input data (this is equivalent
to online learning if the chunks consists of single observations, or
to batch learning if the whole data is sent in a single chunk).
A <tt class="docutils literal"><span class="pre">Node</span></tt> can be copied or saved using the corresponding <tt class="docutils literal"><span class="pre">copy</span></tt> and
<tt class="docutils literal"><span class="pre">save</span></tt> methods.</p>
<div class="section">
<h2><a class="toc-backref" href="#id7" id="node-instantiation" name="node-instantiation">Node Instantiation</a></h2>
<p>Nodes can be obtained by creating an instance of the <tt class="docutils literal"><span class="pre">Node</span></tt> class.
Each node is characterized by an input dimension, that corresponds
to the dimensionality of the input vectors, an output dimension, and
a <tt class="docutils literal"><span class="pre">dtype</span></tt>, which determines the numerical type of the internal structures
and of the output signal. These three attributes are inherited from
the input data if left unspecified. Input dimension and <tt class="docutils literal"><span class="pre">dtype</span></tt>
can usually be specified when an instance of the node class
is created.
The constructor of each node class can require other task-specific
arguments. The full documentation is available in the
doc-string of the node's class.</p>
<p>Some examples of node instantiation:</p>
<ul>
<li><p class="first">Create a node that performs Principal Component Analysis (PCA)
whose input dimension and <tt class="docutils literal"><span class="pre">dtype</span></tt>
are inherited from the input data during training. Output dimensions
default to input dimensions.</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1 = mdp.nodes.PCANode()
&gt;&gt;&gt; pcanode1
PCANode(input_dim=None, output_dim=None, dtype=None)
</pre>
</li>
<li><p class="first">Setting <tt class="docutils literal"><span class="pre">output_dim</span> <span class="pre">=</span> <span class="pre">10</span></tt> means that the node will keep only the
first 10 principal components of the input.</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode2 = mdp.nodes.PCANode(output_dim = 10)
&gt;&gt;&gt; pcanode2
PCANode(input_dim=None, output_dim=10, dtype=None)
</pre>
<p>The output dimensionality can also be specified in terms of the explained
variance. If we want to keep the number of principal components which can
account for 80% of the input variance, we set:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode3 = mdp.nodes.PCANode(output_dim = 0.8)
&gt;&gt;&gt; pcanode3.desired_variance
0.80000000000000004
</pre>
</li>
<li><p class="first">If <tt class="docutils literal"><span class="pre">dtype</span></tt> is set to <tt class="docutils literal"><span class="pre">float32</span></tt> (32-bit float), the input
data is cast to single precision when received and the internal
structures are also stored as <tt class="docutils literal"><span class="pre">float32</span></tt>. <tt class="docutils literal"><span class="pre">dtype</span></tt> influences the
memory space necessary for a node and the precision with which the
computations are performed.</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode4 = mdp.nodes.PCANode(dtype = 'float32')
&gt;&gt;&gt; pcanode4
PCANode(input_dim=None, output_dim=None, dtype='float32')
</pre>
<p>You can obtain a list of the numerical types supported by a node
lookng at its <tt class="docutils literal"><span class="pre">supported_dtypes</span></tt> property:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode4.supported_dtypes
[dtype('float32'), dtype('float64')]
</pre>
<p>This method returns a list of <tt class="docutils literal"><span class="pre">numpy.dtype</span></tt> objects
(see the <tt class="docutils literal"><span class="pre">numpy</span></tt> documentation for more details.</p>
</li>
<li><p class="first">A <tt class="docutils literal"><span class="pre">PolynomialExpansionNode</span></tt> expands its input in the space
of polynomals of a given degree by computing all monomials up
to the specified degree. Its constructor needs as first argument
the degree of the polynomials space (3 in this case).</p>
<pre class="literal-block">
&gt;&gt;&gt; expnode = mdp.nodes.PolynomialExpansionNode(3)
</pre>
</li>
</ul>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id8" id="node-training" name="node-training">Node Training</a></h2>
<p>Some nodes need to be trained to perform their task. This can
be done during a training phases by calling the <tt class="docutils literal"><span class="pre">train</span></tt> method.
MDP supports both supervised and unsupervised training, and
algorithms with multiple training phases.</p>
<p>Some examples of node training:</p>
<ul>
<li><p class="first">Create some random data and update the internal structures
(i.e. mean and covariance matrix) of the <tt class="docutils literal"><span class="pre">PCANode</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 25))  # 25 variables, 100 observations
&gt;&gt;&gt; pcanode1.train(x)
</pre>
<p>At this point the input dimension and the <tt class="docutils literal"><span class="pre">dtype</span></tt> have been
inherited from <tt class="docutils literal"><span class="pre">x</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1
PCANode(input_dim=25, output_dim=None, dtype='float64')
</pre>
</li>
<li><p class="first">We can train our node with more than one chunk of data. This
is especially useful when the input data is too long to
be stored in memory or when it has to be created on-the-fly.
(See also the <a class="reference" href="#iterators">Iterators</a> section):</p>
<pre class="literal-block">
&gt;&gt;&gt; for i in range(100):
...     x = mdp.numx_rand.random((100, 25))
...     pcanode1.train(x)
&gt;&gt;&gt;
</pre>
</li>
<li><p class="first">Some nodes don't need to or cannot be trained:</p>
<pre class="literal-block">
&gt;&gt;&gt; expnode.is_trainable()
False
</pre>
<p>Trying to train them anyway would raise
an <tt class="docutils literal"><span class="pre">IsNotTrainableException</span></tt>.</p>
</li>
<li><p class="first">The training phase ends when the <tt class="docutils literal"><span class="pre">stop_training</span></tt>, <tt class="docutils literal"><span class="pre">execute</span></tt>,
<tt class="docutils literal"><span class="pre">inverse</span></tt>, and possibly some other node-specific methods are called.
For example we can stop the training
of <tt class="docutils literal"><span class="pre">pcanode1</span></tt> (at this point the principal components are computed):</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1.stop_training()
</pre>
</li>
<li><p class="first">If the <tt class="docutils literal"><span class="pre">PCANode</span></tt> was declared to have a number of output components
dependent on the input variance to be explained, we can check after
training the number of output components and the actually explained variance:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode3.train(x)
&gt;&gt;&gt; pcanode3.stop_training()
&gt;&gt;&gt; pcanode3.output_dim
16
&gt;&gt;&gt; pcanode3.explained_variance
0.85261144755506446
</pre>
<p>It is now possible to access the trained internal data. In general,
a list of the interesting internal attributes can be found in the
class documentation.</p>
<pre class="literal-block">
&gt;&gt;&gt; avg = pcanode1.avg            # mean of the input data
&gt;&gt;&gt; v = pcanode1.get_projmatrix() # projection matrix
</pre>
</li>
<li><p class="first">Some nodes, namely the one corresponding to supervised algorithms, e.g.
Fisher Discriminant Analysis (FDA), may need some labels or other
supervised signals to be passed
during training. Detailed information about the signature of the
<tt class="docutils literal"><span class="pre">train</span></tt> method can be read in its doc-string.</p>
<pre class="literal-block">
&gt;&gt;&gt; fdanode = mdp.nodes.FDANode()
&gt;&gt;&gt; for label in ['a', 'b', 'c']:
...     x = mdp.numx_rand.random((100, 25))
...     fdanode.train(x, label)
&gt;&gt;&gt;
</pre>
</li>
<li><p class="first">A node could also require multiple training phases. For example,
the training of <tt class="docutils literal"><span class="pre">fdanode</span></tt> is not complete yet, since it has
two training phases. We need to stop the first phase and train
the second:</p>
<pre class="literal-block">
&gt;&gt;&gt; fdanode.stop_training()
&gt;&gt;&gt; for label in ['a', 'b', 'c']:
...     x = mdp.numx_rand.random((100, 25))
...     fdanode.train(x, label)
&gt;&gt;&gt;
</pre>
<p>The easiest way to train multiple phase nodes is using <a class="reference" href="#flows">Flows</a> ,
which automatically handle multiple phases.</p>
</li>
</ul>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id9" id="node-execution" name="node-execution">Node Execution</a></h2>
<p>After the training phase it is possible to execute the node:</p>
<ul>
<li><p class="first">The input data is projected on the principal components learned
in the training phase:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 25))
&gt;&gt;&gt; y_pca = pcanode1.execute(x)
</pre>
</li>
<li><p class="first">Calling a node instance is equivalent to executing it:</p>
<pre class="literal-block">
&gt;&gt;&gt; y_pca = pcanode1(x)
</pre>
</li>
<li><p class="first">The input data is expanded in the space of polynomials of
degree 3:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 5))
&gt;&gt;&gt; y_exp = expnode(x)
</pre>
</li>
<li><p class="first">The input data is projected to the directions learned by FDA:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 25))
&gt;&gt;&gt; y_fda = fdanode(x)
</pre>
</li>
<li><p class="first">Some nodes may allow for optional arguments in the <tt class="docutils literal"><span class="pre">execute</span></tt> method,
as always the complete information is given in the doc-string.</p>
</li>
</ul>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id10" id="node-inversion" name="node-inversion">Node Inversion</a></h2>
<p>If the operation computed by the node is invertible, it is possible
to compute the inverse transformation:</p>
<ul>
<li><p class="first">Given the output data, compute the inverse projection to
the input space for the PCA node:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1.is_invertible()
True
&gt;&gt;&gt; x = pcanode1.inverse(y_pca)
</pre>
</li>
<li><p class="first">The expansion node in not invertible:</p>
<pre class="literal-block">
&gt;&gt;&gt; expnode.is_invertible()
False
</pre>
<p>Trying to compute the inverse would raise an <tt class="docutils literal"><span class="pre">IsNotInvertibleException</span></tt>.</p>
</li>
</ul>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id11" id="writing-your-own-nodes-subclassing-node" name="writing-your-own-nodes-subclassing-node">Writing your own nodes: subclassing Node</a></h2>
<p>MDP tries to make it easy to write new data processing elements
that fit with the existing elements. To expand the MDP library of
implemented nodes with your own nodes you can subclass
the Node class, overriding some of the methods according
to your needs.</p>
<p>It is recommended to refer to the <tt class="docutils literal"><span class="pre">numpy</span></tt> or <tt class="docutils literal"><span class="pre">scipy</span></tt> numerical
extensions
through the MDP aliases <tt class="docutils literal"><span class="pre">mdp.numx</span></tt>, <tt class="docutils literal"><span class="pre">mdp.numx_linalg</span></tt>,
<tt class="docutils literal"><span class="pre">mdp.numx_fft</span></tt>, and
<tt class="docutils literal"><span class="pre">mdp.numx_rand</span></tt> when writing <tt class="docutils literal"><span class="pre">Node</span></tt> subclasses. This shall ensure
that your nodes can be used without modifications should MDP support
alternative numerical extensions in the future.</p>
<p>We'll illustrate this with some toy examples.</p>
<ul>
<li><p class="first">We start by defining a node that multiplies its input by 2.</p>
<p>Define the class as a subclass of Node:</p>
<pre class="literal-block">
&gt;&gt;&gt; class TimesTwoNode(mdp.Node):
</pre>
<p>This node cannot be trained. To specify this, one has to overwrite
the <tt class="docutils literal"><span class="pre">is_trainable</span></tt> method to return False:</p>
<pre class="literal-block">
...     def is_trainable(self): return False
</pre>
<p>Execute only needs to multiply x by 2</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return 2*x
</pre>
<p>Note that the <tt class="docutils literal"><span class="pre">execute</span></tt> method, which should never be overwritten
and which is inherited from the <tt class="docutils literal"><span class="pre">Node</span></tt> parent class, will perform
some tests, for example to make sure that <tt class="docutils literal"><span class="pre">x</span></tt> has the right rank,
dimensionality and casts it to have the right <tt class="docutils literal"><span class="pre">dtype</span></tt>.  After that
the user-supplied <tt class="docutils literal"><span class="pre">_execute</span></tt> method is called.  Each subclass has
to handle the <tt class="docutils literal"><span class="pre">dtype</span></tt> defined by the user or inherited by the
input data, and make sure that internal structures are stored
consistently. To help with this the <tt class="docutils literal"><span class="pre">Node</span></tt> base class has a method
called <tt class="docutils literal"><span class="pre">_refcast(array,</span> <span class="pre">dtype)</span></tt> that casts an array only when its
<tt class="docutils literal"><span class="pre">dtype</span></tt> is different from the requested one.</p>
<p>The inverse of the multiplication by 2 is of course the division by 2:</p>
<pre class="literal-block">
...     def _inverse(self, y):
...         return y/2
...
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = TimesTwoNode(dtype = 'int32')
&gt;&gt;&gt; x = mdp.numx.array([[1.0, 2.0, 3.0]])
&gt;&gt;&gt; y = node(x)
&gt;&gt;&gt; print x, '* 2 =  ', y
[ [ 1.  2.  3.]] * 2 =   [ [2 4 6]]
&gt;&gt;&gt; print y, '/ 2 =', node.inverse(y)
[ [2 4 6]] / 2 = [ [1 2 3]]
</pre>
</li>
<li><p class="first">We then define a node that raises the input to the power specified
in the initializer:</p>
<pre class="literal-block">
&gt;&gt;&gt; class PowerNode(mdp.Node):
</pre>
<p>We redefine the init method to take the power as first argument.
In general one should always give the possibility to set the <tt class="docutils literal"><span class="pre">dtype</span></tt>
and the input dimensions. The default value is <tt class="docutils literal"><span class="pre">None</span></tt>, which means that
the exact value is going to be inherited from the input data:</p>
<pre class="literal-block">
...     def __init__(self, power, input_dim=None, dtype=None):
</pre>
<p>Initialize the parent class:</p>
<pre class="literal-block">
...         super(PowerNode, self).__init__(input_dim=input_dim, dtype=dtype)
</pre>
<p>Store the power:</p>
<pre class="literal-block">
...         self.power = power
</pre>
<p><tt class="docutils literal"><span class="pre">PowerNode</span></tt> is not trainable...</p>
<pre class="literal-block">
...     def is_trainable(self): return False
</pre>
<p>... nor invertible:</p>
<pre class="literal-block">
...     def is_invertible(self): return False
</pre>
<p>It is possible to overwrite the function <tt class="docutils literal"><span class="pre">_get_supported_dtypes</span></tt>
to return a list of <tt class="docutils literal"><span class="pre">dtype</span></tt> supported by the node:</p>
<pre class="literal-block">
...     def _get_supported_dtypes(self):
...         return ['float32', 'float64']
</pre>
<p>The supported types can be specified in any format allowed by
<tt class="docutils literal"><span class="pre">numpy.dtype</span></tt>. The interface method <tt class="docutils literal"><span class="pre">get_supported_dtypes</span></tt>
converts them and sets the property <tt class="docutils literal"><span class="pre">supported_dtypes</span></tt>, which is
a list of <tt class="docutils literal"><span class="pre">dtype</span></tt> objects.</p>
<p>The <tt class="docutils literal"><span class="pre">_execute</span></tt> method:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return self._refcast(x**self.power)
...
&gt;&gt;&gt;
</pre>
<p>Test the new node</p>
<pre class="literal-block">
&gt;&gt;&gt; node = PowerNode(3)
&gt;&gt;&gt; x = mdp.numx.array([[1.0, 2.0, 3.0]])
&gt;&gt;&gt; y = node.execute(x)
&gt;&gt;&gt; print x, '**', node.power, '=', node(x)
[ [ 1.  2.  3.]] ** 3 = [ [  1.   8.  27.]]
</pre>
</li>
<li><p class="first">We now define a node that needs to be trained. The <tt class="docutils literal"><span class="pre">MeanFreeNode</span></tt>
computes the mean of its training data and subtracts it from the input
during execution:</p>
<pre class="literal-block">
&gt;&gt;&gt; class MeanFreeNode(mdp.Node):
...     def __init__(self, input_dim=None, dtype=None):
...         super(MeanFreeNode, self).__init__(input_dim=input_dim,
...                                            dtype=dtype)
</pre>
<p>We store the mean of the input data in an attribute. We initialize it
to <tt class="docutils literal"><span class="pre">None</span></tt> since we still don't know how large is an input vector:</p>
<pre class="literal-block">
...         self.avg = None
</pre>
<p>Same for the number of training points:</p>
<pre class="literal-block">
...         self.tlen = 0
</pre>
<p>The subclass only needs to overwrite the <tt class="docutils literal"><span class="pre">_train</span></tt> method, which
will be called by the parent <tt class="docutils literal"><span class="pre">train</span></tt> after some testing and casting has
been done:</p>
<pre class="literal-block">
...     def _train(self, x):
...         # Initialize the mean vector with the right
...         # size and dtype if necessary:
...         if self.avg is None:
...             self.avg = mdp.numx.zeros(self.input_dim,
...                                       dtype=self.dtype)
</pre>
<p>Update the mean with the sum of the new data:</p>
<pre class="literal-block">
...         self.avg += mdp.numx.sum(x, axis=0)
</pre>
<p>Count the number of points processed:</p>
<pre class="literal-block">
...         self.tlen += x.shape[0]
</pre>
<p>Note that <tt class="docutils literal"><span class="pre">train</span></tt> method can have further arguments, which might be
useful to implement algorithms that require supervised learning.
For example, if you want to define a node that performs some form
of classification you can define a <tt class="docutils literal"><span class="pre">_train(self,</span> <span class="pre">data,</span> <span class="pre">labels)</span></tt>
method. The parent <tt class="docutils literal"><span class="pre">train</span></tt> checks <tt class="docutils literal"><span class="pre">data</span></tt> and takes care to pass
the <tt class="docutils literal"><span class="pre">labels</span></tt> on (cf. for example <tt class="docutils literal"><span class="pre">mdp.nodes.FDANode</span></tt>).</p>
<p>The <tt class="docutils literal"><span class="pre">_stop_training</span></tt> function is called by the parent <tt class="docutils literal"><span class="pre">stop_training</span></tt>
method when the training phase is over. We divide the sum of the training
data by the number of training vectors to obtain the mean:</p>
<pre class="literal-block">
...     def _stop_training(self):
...         self.avg /= self.tlen
...         if self.output_dim is None:
...             self.output_dim = self.input_dim
</pre>
<p>Note that we <tt class="docutils literal"><span class="pre">input_dim</span></tt> are set autoamtically by the <tt class="docutils literal"><span class="pre">train</span></tt> method,
and we want to ensure that the node has <tt class="docutils literal"><span class="pre">output_dim</span></tt> set after training.
For nodes that do not need training, the setting is performed automatically
upon execution. The <tt class="docutils literal"><span class="pre">_execute</span></tt> and <tt class="docutils literal"><span class="pre">_inverse</span></tt> methods:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return x - self.avg
...     def _inverse(self, y):
...         return y + self.avg
...
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = MeanFreeNode()
&gt;&gt;&gt; x = mdp.numx_rand.random((10,4))
&gt;&gt;&gt; node.train(x)
&gt;&gt;&gt; y = node.execute(x)
&gt;&gt;&gt; print 'Mean of y (should be zero): ', mdp.numx.mean(y, 0)
Mean of y (should be zero):  [  0.00000000e+00   2.22044605e-17
-2.22044605e-17   1.11022302e-17]
</pre>
</li>
<li><p class="first">It is also possible to define nodes with multiple training phases.
In such a case, calling the <tt class="docutils literal"><span class="pre">train</span></tt> and <tt class="docutils literal"><span class="pre">stop_training</span></tt> functions
multiple times is going to execute successive training phases
(this kind of node is much easier to train using <a class="reference" href="#flows">Flows</a>).
Here we'll define a node that returns a meanfree, unit variance signal.
We define two training phases: first we compute the mean of the
signal and next we sum the squared, meanfree input to compute
the standard deviation  (of course it is possible to solve this
problem in one single step - remeber this is just a toy example).</p>
<pre class="literal-block">
&gt;&gt;&gt; class UnitVarianceNode(mdp.Node):
...     def __init__(self, input_dim=None, dtype=None):
...         super(UnitVarianceNode, self).__init__(input_dim=input_dim,
...                                                dtype=dtype)
...         self.avg = None # average
...         self.std = None # standard deviation
...         self.tlen = 0
</pre>
<p>The training sequence is defined by the user-supplied function
<tt class="docutils literal"><span class="pre">_get_train_seq</span></tt>, that returns a list of tuples, one for each
training phase. The tuples contain references to the training
and stop-training functions of each of them. The default output
of this function is <tt class="docutils literal"><span class="pre">[(_train,</span> <span class="pre">_stop_training)]</span></tt>, which explains
the standard behavior illustrated above. We overwrite the function to
return the list of our training functions:</p>
<pre class="literal-block">
...     def _get_train_seq(self):
...         return [(self._train_mean, self._stop_mean),
...                 (self._train_std, self._stop_std)]
</pre>
<p>Next we define the training functions. The first phase is identical
to the one in the previous example:</p>
<pre class="literal-block">
...     def _train_mean(self, x):
...         if self.avg is None:
...             self.avg = mdp.numx.zeros(self.input_dim,
...                                       dtype=self.dtype)
...         self.avg += mdp.numx.sum(x, 0)
...         self.tlen += x.shape[0]
...     def _stop_mean(self):
...         self.avg /= self.tlen
</pre>
<p>The second one is only marginally different and does not require many
explanations:</p>
<pre class="literal-block">
...     def _train_std(self, x):
...         if self.std is None:
...             self.tlen = 0
...             self.std = mdp.numx.zeros(self.input_dim,
...                                       dtype=self.dtype)
...         self.std += mdp.numx.sum((x - self.avg)**2., 0)
...         self.tlen += x.shape[0]
...     def _stop_std(self):
...         # compute the standard deviation
...         self.std = mdp.numx.sqrt(self.std/(self.tlen-1))
</pre>
<p>The <tt class="docutils literal"><span class="pre">_execute</span></tt> and <tt class="docutils literal"><span class="pre">_inverse</span></tt> methods are not surprising, either:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return (x - self.avg)/self.std
...     def _inverse(self, y):
...         return y*self.std + self.avg
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = UnitVarianceNode()
&gt;&gt;&gt; x = mdp.numx_rand.random((10,4))
&gt;&gt;&gt; # loop over phases
... for phase in range(2):
...     node.train(x)
...     node.stop_training()
...
...
&gt;&gt;&gt; # execute
... y = node.execute(x)
&gt;&gt;&gt; print 'Standard deviation of y (should be one): ', mdp.numx.std(y, axis=0)
Standard deviation of y (should be one):  [ 1.  1.  1.  1.]
</pre>
</li>
<li><p class="first">In our last example we'll define a node that returns two copies of its input.
The output is going to have twice as many dimensions.</p>
<pre class="literal-block">
&gt;&gt;&gt; class TwiceNode(mdp.Node):
...     def is_trainable(self): return False
...     def is_invertible(self): return False
</pre>
<p>When <tt class="docutils literal"><span class="pre">Node</span></tt> inherits the input dimension, output dimension, and <tt class="docutils literal"><span class="pre">dtype</span></tt>
from the input data, it calls the methods <tt class="docutils literal"><span class="pre">set_input_dim</span></tt>,
<tt class="docutils literal"><span class="pre">set_output_dim</span></tt>, and <tt class="docutils literal"><span class="pre">set_dtype</span></tt>. Those are the setters for
<tt class="docutils literal"><span class="pre">input_dim</span></tt>, <tt class="docutils literal"><span class="pre">output_dim</span></tt> and <tt class="docutils literal"><span class="pre">dtype</span></tt>, which are Python
<a class="reference" href="http://www.python.org/2.2/descrintro.html">properties</a>.
If a subclass needs to change the default behaviour, the internal methods
<tt class="docutils literal"><span class="pre">_set_input_dim</span></tt>, <tt class="docutils literal"><span class="pre">_set_output_dim</span></tt> and <tt class="docutils literal"><span class="pre">_set_dtype</span></tt> can
be overwritten. The property setter will call the internal method after
some basic testing and internal settings. The private methods
<tt class="docutils literal"><span class="pre">_set_input_dim</span></tt>, <tt class="docutils literal"><span class="pre">_set_output_dim</span></tt> and <tt class="docutils literal"><span class="pre">_set_dtype</span></tt> are responsible
for setting the private attributes <tt class="docutils literal"><span class="pre">_input_dim</span></tt>, <tt class="docutils literal"><span class="pre">_output_dim</span></tt>,
and <tt class="docutils literal"><span class="pre">_dtype</span></tt> that contain the actual value.</p>
<p>Here we overwrite
<tt class="docutils literal"><span class="pre">_set_input_dim</span></tt> to automatically set the output dimension to be twice the
input one, and <tt class="docutils literal"><span class="pre">_set_output_dim</span></tt> to raise an exception, since
the output dimension should not be set explicitly.</p>
<pre class="literal-block">
...     def _set_input_dim(self, n):
...         self._input_dim = n
...         self._output_dim = 2*n
...     def _set_output_dim(self, n):
...         raise mdp.NodeException, &quot;Output dim can not be set explicitly!&quot;
</pre>
<p>The <tt class="docutils literal"><span class="pre">_execute</span></tt> method:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return mdp.numx.concatenate((x, x), 1)
...
&gt;&gt;&gt;
</pre>
<p>Test the new node</p>
<pre class="literal-block">
&gt;&gt;&gt; node = TwiceNode()
&gt;&gt;&gt; x = mdp.numx.zeros((5,2))
&gt;&gt;&gt; x
array([[0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0]])
&gt;&gt;&gt; node.execute(x)
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]])
</pre>
</li>
</ul>
</div>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id12" id="flows" name="flows">Flows</a></h1>
<p>A flow consists in an acyclic graph of nodes (currently only
node sequences are implemented). The data is sent to an
input node and is successively processed by the following
nodes on the graph. The general flow implementation automatizes
the training, execution, and inverse execution (if defined) of
the whole graph. Training can be supervised and can consist of
multiple phases.
Crash recovery is optionally available: in case of failure the current
state of the flow is saved for later inspection. A subclass of the
basic flow class (<tt class="docutils literal"><span class="pre">CheckpointFlow</span></tt>) allows user-supplied checkpoint
functions to be executed at the end of each phase, for example to save
the internal structures of a node for later analysis.
Flow objects are Python containers. Most of the builtin <tt class="docutils literal"><span class="pre">list</span></tt>
methods are available. A <tt class="docutils literal"><span class="pre">Flow</span></tt> can be saved or copied using the
corresponding <tt class="docutils literal"><span class="pre">save</span></tt> and <tt class="docutils literal"><span class="pre">copy</span></tt> methods.</p>
<div class="section">
<h2><a class="toc-backref" href="#id13" id="flow-instantiation-training-and-execution" name="flow-instantiation-training-and-execution">Flow instantiation, training and execution</a></h2>
<p>Suppose we have an input signal with an high number of dimensions,
on which we would like to perform ICA. To make the problem affordable,
we first need to reduce its dimensionality with PCA. Finally, we would
like to find out the data that produces local maxima in the output
on a new test set. This information could be used to characterize
the input-output filters.</p>
<p>We start by generating some input signal at random (which makes the
example useless, but it's just for illustration...).  Generate 1000
observations of 20 independent source signals:</p>
<pre class="literal-block">
&gt;&gt;&gt; inp = mdp.numx_rand.random((1000, 20))
</pre>
<p>Rescale x to have zero mean and unit variance:</p>
<pre class="literal-block">
&gt;&gt;&gt; inp = (inp - mdp.numx.mean(inp, 0))/mdp.numx.std(inp, 0)
</pre>
<p>We reduce the variance of the last 15 components, so that they are
going to be eliminated by PCA:</p>
<pre class="literal-block">
&gt;&gt;&gt; inp[:,5:] /= 10.0
</pre>
<p>Mix the input signals linearly:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.utils.mult(inp,mdp.numx_rand.random((20, 20)))
</pre>
<p><cite>x</cite> is now the training data for our simulation. In the same way
we also create a test set <cite>x_test</cite>.</p>
<pre class="literal-block">
&gt;&gt;&gt; inp_test = mdp.numx_rand.random((1000, 20))
&gt;&gt;&gt; inp_test = (inp_test - mdp.numx.mean(inp_test, 0))/mdp.numx.std(inp_test, 0)
&gt;&gt;&gt; inp_test[:,5:] /= 10.0
&gt;&gt;&gt; x_test = mdp.utils.mult(inp_test, mdp.numx_rand.random((20, 20)))
</pre>
<ul>
<li><p class="first">We could now perform our analysis using only nodes, that's the
lenghty way...</p>
<p>1. Perform PCA:</p>
<pre class="literal-block">
&gt;&gt;&gt; pca = mdp.nodes.PCANode(output_dim=5)
&gt;&gt;&gt; pca.train(x)
&gt;&gt;&gt; out1 = pca.execute(x)
</pre>
<p>2. Perform ICA using CuBICA algorithm:</p>
<pre class="literal-block">
&gt;&gt;&gt; ica = mdp.nodes.CuBICANode()
&gt;&gt;&gt; ica.train(out1)
&gt;&gt;&gt; out2 = ica.execute(out1)
</pre>
<p>3. Find the three largest local maxima in the output of the ICA node
when applied to the test data, using a <tt class="docutils literal"><span class="pre">HitParadeNode</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; out1_test = pca.execute(x_test)
&gt;&gt;&gt; out2_test = ica.execute(out1_test)
&gt;&gt;&gt; hitnode = mdp.nodes.HitParadeNode(3)
&gt;&gt;&gt; hitnode.train(out2_test)
&gt;&gt;&gt; maxima, indices = hitnode.get_maxima()
</pre>
</li>
<li><p class="first">... or we could use flows, which is the best way:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([mdp.nodes.PCANode(output_dim=5), mdp.nodes.CuBICANode()])
&gt;&gt;&gt; flow.train(x)
</pre>
<p>Now the training phase of PCA and ICA are completed. Next we append
a <tt class="docutils literal"><span class="pre">HitParadeNode</span></tt> which we want to train on the test data:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.append(mdp.nodes.HitParadeNode(3))
&gt;&gt;&gt; flow.train(x_test)
&gt;&gt;&gt; maxima, indices = flow[2].get_maxima()
</pre>
<p>Just to check that everything works
properly, we can calculate covariance between the generated sources and
the output (should be approximately 1):</p>
<pre class="literal-block">
&gt;&gt;&gt; out = flow.execute(x)
&gt;&gt;&gt; cov = mdp.numx.amax(abs(mdp.utils.cov2(inp[:,:5], out)))
&gt;&gt;&gt; print cov
[ 0.98992083  0.99244511  0.99227319  0.99663185  0.9871812 ]
</pre>
<p>The <tt class="docutils literal"><span class="pre">HitParadeNode</span></tt> is an analysis node and as such does not
interfere with the data flow.</p>
</li>
</ul>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id14" id="flow-inversion" name="flow-inversion">Flow inversion</a></h2>
<p>Flows can be inverted by calling their <tt class="docutils literal"><span class="pre">inverse</span></tt> method.
In the case where the flow contains non-invertible nodes,
trying to invert it would raise an exception.
In this case, however, all nodes are invertible.
We can reconstruct the mix by inverting the flow:</p>
<pre class="literal-block">
&gt;&gt;&gt; rec = flow.inverse(out)
</pre>
<p>Calculate covariance between input mix and reconstructed mix:
(should be approximately 1)</p>
<pre class="literal-block">
&gt;&gt;&gt; cov = mdp.numx.amax(abs(mdp.utils.cov2(x/mdp.numx.std(x,axis=0),
...                                        rec/mdp.numx.std(rec,axis=0))))
&gt;&gt;&gt; print cov
[ 0.99839606  0.99744461  0.99616208  0.99772863  0.99690947
  0.99864056  0.99734378  0.98722502  0.98118101  0.99407939
  0.99683096  0.99756988  0.99664384  0.99723419  0.9985529
  0.99829763  0.9982712   0.99721741  0.99682906  0.98858858]
</pre>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id15" id="flows-are-container-type-objects" name="flows-are-container-type-objects">Flows are container type objects</a></h2>
<p>Flows are Python container type objects, very much like lists,
i.e., you can loop through them:</p>
<pre class="literal-block">
&gt;&gt;&gt; for node in flow:
...     print repr(node)
...
PCANode(input_dim=20, output_dim=5, dtype='float64')
CuBICANode(input_dim=5, output_dim=5, dtype='float64')
HitParadeNode(input_dim=5, output_dim=5, dtype='float64')
&gt;&gt;&gt;
</pre>
<p>You can get slices, <tt class="docutils literal"><span class="pre">pop</span></tt>, <tt class="docutils literal"><span class="pre">insert</span></tt>, and <tt class="docutils literal"><span class="pre">append</span></tt> nodes like you
would do with lists:</p>
<pre class="literal-block">
&gt;&gt;&gt; len(flow)
3
&gt;&gt;&gt; print flow[::2]
[PCANode, HitParadeNode]
&gt;&gt;&gt; nodetoberemoved = flow.pop(-1)
&gt;&gt;&gt; nodetoberemoved
HitParadeNode(input_dim=5, output_dim=5, dtype='float64')
&gt;&gt;&gt; len(flow)
2
</pre>
<p>Finally, you can concatenate flows:</p>
<pre class="literal-block">
&gt;&gt;&gt; dummyflow = flow[1:].copy()
&gt;&gt;&gt; longflow = flow + dummyflow
&gt;&gt;&gt; len(longflow)
3
</pre>
<p>The returned flow must always be consistent, i.e. input and
output dimensions of successive nodes always have to match. If
you try to create an inconsistent flow you'll get an error.</p>
</div>
<div class="section">
<h2><a class="toc-backref" href="#id16" id="crash-recovery" name="crash-recovery">Crash recovery</a></h2>
<p>If a node in a flow fails, you'll get a traceback that tells you which
node has failed. You can also switch the crash recovery capability on. If
something goes wrong you'll end up with a pickle dump of the flow, that
can be later inspected.</p>
<p>To see how it works let's define a bogus node that always throws an
<tt class="docutils literal"><span class="pre">Exception</span></tt> and put it into a flow:</p>
<pre class="literal-block">
&gt;&gt;&gt; class BogusExceptNode(mdp.Node):
...    def train(self,x):
...        self.bogus_attr = 1
...        raise Exception, &quot;Bogus Exception&quot;
...    def execute(self,x):
...        raise Exception, &quot;Bogus Exception&quot;
...
&gt;&gt;&gt; flow = mdp.Flow([BogusExceptNode()])
</pre>
<p>Switch on crash recovery:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.set_crash_recovery(1)
</pre>
<p>Attempt to train the flow:</p>
<blockquote>
<!-- ignore --></blockquote>
<pre class="literal-block">
&gt;&gt;&gt; flow.train(x)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in ?
  [...]
mdp.linear_flows.FlowExceptionCR:
----------------------------------------
! Exception in node #0 (BogusExceptNode):
Node Traceback:
Traceback (most recent call last):
  [...]
Exception: Bogus Exception
----------------------------------------
A crash dump is available on: &quot;/tmp/MDPcrash_LmISO_.pic&quot;
</pre>
<p>You can give a file name to tell the flow where to save the dump:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.set_crash_recovery('/home/myself/mydumps/MDPdump.pic')
</pre>
</div>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id17" id="iterators" name="iterators">Iterators</a></h1>
<p>Python allows user-defined classes to support iteration,
as described in the
<a class="reference" href="http://docs.python.org/lib/typeiter.html">Python docs</a>.
A convenient implementation of the iterator protocol is provided
by generators:
see <a class="reference" href="http://linuxgazette.net/100/pramode.html">this article</a> for an
introduction, and the
<a class="reference" href="http://www.python.org/peps/pep-0255.html">official PEP</a> for a
complete description.</p>
<p>Let us define two bogus node classes to be used as examples of nodes:</p>
<pre class="literal-block">
&gt;&gt;&gt; class BogusNode(mdp.Node):
...     &quot;&quot;&quot;This node does nothing.&quot;&quot;&quot;
...     def _train(self, x):
...         pass
...
&gt;&gt;&gt; class BogusNode2(mdp.Node):
...     &quot;&quot;&quot;This node does nothing. But it's not trainable nor invertible.
...     &quot;&quot;&quot;
...     def is_trainable(self): return False
...     def is_invertible(self): return False
...
&gt;&gt;&gt;
</pre>
<p>This generator generates <tt class="docutils literal"><span class="pre">blocks</span></tt> input blocks to be used as training set.
In this example one block is a 2-dimensional time-series. The first variable
is [2,4,6,....,1000] and the second one [0,1,3,5,...,999].
All blocks are equal, this of course would not be the case in a real-life
example.</p>
<p>In this example we use a progress bar to get progress information.</p>
<pre class="literal-block">
&gt;&gt;&gt; def gen_data(blocks):
...     for i in mdp.utils.progressinfo(xrange(blocks)):
...         block_x = mdp.numx.atleast_2d(mdp.numx.arange(2,1001,2))
...         block_y = mdp.numx.atleast_2d(mdp.numx.arange(1,1001,2))
...         # put variables on columns and observations on rows
...         block = mdp.numx.transpose(mdp.numx.concatenate([block_x,block_y]))
...         yield block
...
&gt;&gt;&gt;
</pre>
<p>Let's define a bogus flow consisting of 2 <tt class="docutils literal"><span class="pre">BogusNode</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode(),BogusNode()], verbose=1)
</pre>
<p>Train the first node with 5000 blocks and the second node with 3000 blocks.
Note that the only allowed argument to <tt class="docutils literal"><span class="pre">train</span></tt> is a sequence (list or tuple)
of iterators. In case you don't want or need to use incremental learning and
want to do a one-shot training, you can use as argument to <tt class="docutils literal"><span class="pre">train</span></tt> a single
array of data:</p>
<p><strong>block-mode training</strong></p>
<blockquote>
<pre class="literal-block">
&gt;&gt;&gt; flow.train([gen_data(5000),gen_data(3000)])
Training node #0 (BogusNode)
[===================================100%==================================&gt;]

Training finished
Training node #1 (BogusNode)
[===================================100%==================================&gt;]

Training finished
Close the training phase of the last node
</pre>
</blockquote>
<p><strong>one-shot training</strong> using one single set of data for both nodes</p>
<blockquote>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode(),BogusNode()])
&gt;&gt;&gt; block_x = mdp.numx.atleast_2d(mdp.numx.arange(2,1001,2))
&gt;&gt;&gt; block_y = mdp.numx.atleast_2d(mdp.numx.arange(1,1001,2))
&gt;&gt;&gt; single_block = mdp.numx.transpose(mdp.numx.concatenate([block_x,block_y]))
&gt;&gt;&gt; flow.train(single_block)
</pre>
</blockquote>
<p>If your flow contains non-trainable nodes, you must specify a <tt class="docutils literal"><span class="pre">None</span></tt> iterator
for the non-trainable nodes:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode2(),BogusNode()], verbose=1)
&gt;&gt;&gt; flow.train([None, gen_data(5000)])
Training node #0 (BogusNode2)
Training finished
Training node #1 (BogusNode)
[===================================100%==================================&gt;]

Training finished
Close the training phase of the last node
</pre>
<p>You can use the one-shot training:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode2(),BogusNode()], verbose=1)
&gt;&gt;&gt; flow.train(single_block)
Training node #0 (BogusNode2)
Training finished
Training node #1 (BogusNode)
Training finished
Close the training phase of the last node
</pre>
<p>Iterators can be used also for execution (and inversion):</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode(),BogusNode()], verbose=1)
&gt;&gt;&gt; flow.train([gen_data(1), gen_data(1)])
Training node #0 (BogusNode2)
Training finished
Training node #1 (IdentityNode)
[===================================100%==================================&gt;]

Training finished
Close the training phase of the last node
&gt;&gt;&gt; output = flow.execute(gen_data(1000))
[===================================100%==================================&gt;]
&gt;&gt;&gt; output = flow.inverse(gen_data(1000))
[===================================100%==================================&gt;]
</pre>
<p>Execution and inversion can be done in one-shot mode also. Note that
since training is finished you are not going to get a warning</p>
<pre class="literal-block">
&gt;&gt;&gt; output = flow.execute(single_block)
&gt;&gt;&gt; output = flow.inverse(single_block)
</pre>
<p>If a node requires multiple training phases (e.g., <tt class="docutils literal"><span class="pre">GaussianClassifierNode</span></tt>),
<tt class="docutils literal"><span class="pre">Flow</span></tt> automatically takes care of reusing the iterator multiple times.
In this case generators are not allowed, since they <em>expire</em> after
yielding the last data block. If you try to restart them, they raise
a <tt class="docutils literal"><span class="pre">StopIteration</span></tt> exception. General iterators, instead, can always be
restarted. For example, you can loop over a list as many times as you need.</p>
<p>However, it is fairly easy to wrap a generator in a simple iterator if you need to:</p>
<pre class="literal-block">
&gt;&gt;&gt; class SimpleIterator(object):
...     def __init__(self, blocks):
...         self.blocks = blocks
...     def __iter__(self):
...         # this is a generator
...         for i in range(self.blocks):
...             yield generate_some_data()
&gt;&gt;&gt;
</pre>
<p>Note that if you use random numbers within the iterator, you usually
would like to reset the random number generator to produce the
same sequence every time:</p>
<pre class="literal-block">
&gt;&gt;&gt; class RandomIterator(object):
...     def __init__(self):
...         self.state = None
...     def __iter__(self):
...         if self.state is None:
...             self.state = mdp.numx_rand.get_state()
...         else:
...             mdp.numx_rand.set_state(self.state)
...         for i in range(2):
...             yield mdp.numx_rand.random((1,4))
&gt;&gt;&gt; iterator = RandomIterator()
&gt;&gt;&gt; for x in iterator: print x
...
[[ 0.99586495  0.53463386  0.6306412   0.09679571]]
[[ 0.51117469  0.46647448  0.95089738  0.94837122]]
&gt;&gt;&gt; for x in iterator: print x
...
[[ 0.99586495  0.53463386  0.6306412   0.09679571]]
[[ 0.51117469  0.46647448  0.95089738  0.94837122]]
</pre>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id18" id="checkpoints" name="checkpoints">Checkpoints</a></h1>
<p>It can sometimes be useful to execute arbitrary functions at the end
of the training or execution phase, for example to save the internal
structures of a node for later analysis. This can easily be done
by defining a <tt class="docutils literal"><span class="pre">CheckpointFlow</span></tt>. As an example imagine the following
situation: you want to perform Principal Component Analysis (PCA) on
your data to reduce the dimensionality. After this you want to expand
the signals into a nonlinear space and then perform Slow Feature
Analysis to extract slowly varying signals. As the expansion will increase
the number of components, you don't want to run out of memory, but at the same
time you want to keep as much information as possible after the dimensionality
reduction. You could do that by specifying the percentage of
the total input variance that has to be conserved in the dimensionality
reduction. As the number of output components of the PCA node now can become
as large as the that of the input components, you want to check, after training the
PCA node, that this number is below a certain threshold. If this is not
the case you want to abort the execution and maybe start again requesting
less variance to be kept.</p>
<p>Let start defining a generator to be used through the whole example:</p>
<pre class="literal-block">
&gt;&gt;&gt; def gen_data(blocks,dims):
...     mat = mdp.numx_rand.random((dims,dims))-0.5
...     for i in xrange(blocks):
...         # put variables on columns and observations on rows
...         block = mdp.utils.mult(mdp.numx_rand.random((1000,dims)), mat)
...         yield block
...
&gt;&gt;&gt;
</pre>
<p>Define a <tt class="docutils literal"><span class="pre">PCANode</span></tt> which reduces dimensionality of the input,
a <tt class="docutils literal"><span class="pre">PolynomialExpansionNode</span></tt> to expand the signals in the space
of polynomials of degree 2 and a <tt class="docutils literal"><span class="pre">SFANode</span></tt> to perform SFA:</p>
<pre class="literal-block">
&gt;&gt;&gt; pca = mdp.nodes.PCANode(output_dim=0.9)
&gt;&gt;&gt; exp = mdp.nodes.PolynomialExpansionNode(2)
&gt;&gt;&gt; sfa = mdp.nodes.SFANode()
</pre>
<p>As you see we have set the output dimension of the <tt class="docutils literal"><span class="pre">PCANode</span></tt> to be <tt class="docutils literal"><span class="pre">0.9</span></tt>.
This means that we want to keep at least 90% of the variance of the original signal.
We define a <tt class="docutils literal"><span class="pre">PCADimensionExceededException</span></tt> that has to be thrown when
the number of output components exceeds a certain threshold:</p>
<pre class="literal-block">
&gt;&gt;&gt; class PCADimensionExceededException(Exception):
...     &quot;&quot;&quot;Exception base class for PCA exceeded dimensions case.&quot;&quot;&quot;
...     pass
...
&gt;&gt;&gt;
</pre>
<p>Then, write a <tt class="docutils literal"><span class="pre">CheckpointFunction</span></tt> that checks the number of output
dimensions of the <tt class="docutils literal"><span class="pre">PCANode</span></tt> and aborts if this number is larger than <tt class="docutils literal"><span class="pre">max_dim</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; class CheckPCA(mdp.CheckpointFunction):
...     def __init__(self,max_dim):
...         self.max_dim = max_dim
...     def __call__(self,node):
...         node.stop_training()
...         act_dim = node.get_output_dim()
...         if act_dim &gt; self.max_dim:
...             errstr = 'PCA output dimensions exceeded maximum '+\
...                      '(%d &gt; %d)'%(act_dim,self.max_dim)
...             raise PCADimensionExceededException, errstr
...         else:
...             print 'PCA output dimensions = %d'%(act_dim)
...
&gt;&gt;&gt;
</pre>
<p>Define the CheckpointFlow:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.CheckpointFlow([pca, exp, sfa])
</pre>
<p>To train it we have to supply 3 generators and 3 checkpoint functions:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; flow.train([gen_data(10, 50), None, gen_data(10, 50)],
...            [CheckPCA(10), None, None])
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 2, in ?
  [...]
__main__.PCADimensionExceededException: PCA output dimensions exceeded maximum (25 &gt; 10)
</pre>
<p>The training fails with a <tt class="docutils literal"><span class="pre">PCADimensionExceededException</span></tt>.
If we only had 12 input dimensions instead of 50 we would have passed
the checkpoint:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow[0] = mdp.nodes.PCANode(output_dim=0.9)
&gt;&gt;&gt; flow.train([gen_data(10, 12), None, gen_data(10, 12)],
...            [CheckPCA(10), None, None])
PCA output dimensions = 6
</pre>
<p>We could use the built-in <tt class="docutils literal"><span class="pre">CheckpoinSaveFunction</span></tt> to save the <tt class="docutils literal"><span class="pre">SFANode</span></tt>
and analyze the results later :</p>
<pre class="literal-block">
&gt;&gt;&gt; pca = mdp.nodes.PCANode(output_dim=0.9)
&gt;&gt;&gt; exp = mdp.nodes.PolynomialExpansionNode(2)
&gt;&gt;&gt; sfa = mdp.nodes.SFANode()
&gt;&gt;&gt; flow = mdp.CheckpointFlow([pca, exp, sfa])
&gt;&gt;&gt; flow.train([gen_data(10, 12), None, gen_data(10, 12)],
...            [CheckPCA(10),
...             None,
...             mdp.CheckpointSaveFunction('dummy.pic',
...                                        stop_training = 1,
...                                        protocol = 0)])
...
PCA output dimensions = 7
</pre>
<p>We can now reload and analyze the <tt class="docutils literal"><span class="pre">SFANode</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; fl = file('dummy.pic')
&gt;&gt;&gt; import cPickle
&gt;&gt;&gt; sfa_reloaded = cPickle.load(fl)
&gt;&gt;&gt; sfa_reloaded
SFANode(input_dim=35, output_dim=35, dtype='d')
</pre>
<p>Don't forget to clean the rubbish:</p>
<pre class="literal-block">
&gt;&gt;&gt; fl.close()
&gt;&gt;&gt; import os
&gt;&gt;&gt; os.remove('dummy.pic')
</pre>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id19" id="hierarchical-networks" name="hierarchical-networks">Hierarchical Networks</a></h1>
<p>TODO!</p>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id20" id="a-real-life-example-logistic-maps" name="a-real-life-example-logistic-maps">A real life example (Logistic maps)</a></h1>
<p>We show an application of Slow Feature Analysis to the analysis of
non-stationary time series. We consider a chaotic time series generated
by the logistic map based on the logistic equation (a demographic model
of the population biomass of species in the presence of limiting factors
such as food supply or disease), and extract the slowly varying parameter
that is hidden behind the time series.
This example reproduces some of the results reported in:
Laurenz Wiskott, <cite>Estimating Driving Forces of Nonstationary Time Series
with Slow Feature Analysis</cite>. arXiv.org e-Print archive,
<a class="reference" href="http://arxiv.org/abs/cond-mat/0312317">http://arxiv.org/abs/cond-mat/0312317</a></p>
<p>Generate the slowly varying driving force,
a combination of three sine waves (freqs: 5, 11, 13 Hz), and define a function
to generate the logistic map</p>
<pre class="literal-block">
&gt;&gt;&gt; p2 = mdp.numx.pi*2
&gt;&gt;&gt; t = mdp.numx.linspace(0,1,10000,endpoint=0) # time axis 1s, samplerate 10KHz
&gt;&gt;&gt; dforce = mdp.numx.sin(p2*5*t) + mdp.numx.sin(p2*11*t) + mdp.numx.sin(p2*13*t)
&gt;&gt;&gt; def logistic_map(x,r):
...     return r*x*(1-x)
...
&gt;&gt;&gt;
</pre>
<p>Note that we define <tt class="docutils literal"><span class="pre">series</span></tt> to be a two-dimensional array.
Inputs to MDP must be two-dimensional arrays with variables
on columns and observations on rows. In this case we have only
one variable:</p>
<pre class="literal-block">
&gt;&gt;&gt; series = mdp.numx.zeros((10000,1),'d')
</pre>
<p>Fix the initial condition:</p>
<pre class="literal-block">
&gt;&gt;&gt; series[0] = 0.6
</pre>
<p>Generate the time-series using the logistic equation
the driving force modifies the logistic equation parameter <tt class="docutils literal"><span class="pre">r</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; for i in range(1,10000):
...     series[i] = logistic_map(series[i-1],3.6+0.13*dforce[i])
...
&gt;&gt;&gt;
</pre>
<p>If you have a plotting package <tt class="docutils literal"><span class="pre">series</span></tt> should look like this:</p>
<img alt="chaotic time series" src="series.png" style="width: 700px;" />
<p>Define a flow to perform SFA in the space of polynomials of degree 3.
We need a node that embeds the time-series in a 10 dimensional
space, where different variables correspond to time-delayed copies
of the original time-series: the <tt class="docutils literal"><span class="pre">TimeFramesNode(10)</span></tt>.
Then we need a node that expands the new signal in the space
of polynomials of degree 3: the <tt class="docutils literal"><span class="pre">PolynomialExpansionNode(3)</span></tt>.
Finally we perform SFA onto the expanded signal
and keep the slowest feature: <tt class="docutils literal"><span class="pre">SFANode(output_dim=1)</span></tt>.
We also measure the <em>slowness</em> of the input time-series and
of the slow feature obtained by SFA. Therefore we put at the
beginning and at the end of the sequence an <em>analysis node</em>
that computes the <em>eta-value</em> (a measure of slowness)
of its input (see docs for the definition of eta-value): the <tt class="docutils literal"><span class="pre">EtaComputerNode()</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; sequence = [mdp.nodes.EtaComputerNode(),
...             mdp.nodes.TimeFramesNode(10),
...             mdp.nodes.PolynomialExpansionNode(3),
...             mdp.nodes.SFANode(output_dim=1),
...             mdp.nodes.EtaComputerNode()]
...
&gt;&gt;&gt;
&gt;&gt;&gt; flow = mdp.Flow(sequence, verbose=1)
</pre>
<p>Since the time-series is short enough to be kept in memory
we don't need to define generators and we can feed the flow
directly with the whole signal:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.train(series)
</pre>
<p>Since the second and the third nodes are not trainable we are
going to get two warnings (<tt class="docutils literal"><span class="pre">Training</span> <span class="pre">Interrupted</span></tt>). We can safely
ignore them. Execute the flow to get the slow feature</p>
<pre class="literal-block">
&gt;&gt;&gt; slow = flow(series)
</pre>
<p>The slow feautre should match the driving force
up to a scaling factor, a constant offset and the sign.
To allow a comparison we rescale the driving force
to have zero mean and unit variance:</p>
<pre class="literal-block">
&gt;&gt;&gt; resc_dforce = (dforce - mdp.numx.mean(dforce,0))/mdp.numx.std(dforce,0)
</pre>
<p>Print covariance between the rescaled driving force and
the slow feature. Note that embedding the time-series with
10 time frames leads to a time-series with 9 observations less:</p>
<pre class="literal-block">
&gt;&gt;&gt; mdp.utils.cov2(resc_dforce[:-9],slow)
0.99992501533859179
</pre>
<p>Print the <em>eta-values</em> of the chaotic time-series and of
the slow feature</p>
<pre class="literal-block">
&gt;&gt;&gt; print 'Eta value (time-series): ', flow[0].get_eta(t=10000)
Eta value (time-series):  [ 3002.53380245]
&gt;&gt;&gt; print 'Eta value (slow feature): ', flow[-1].get_eta(t=9996)
Eta value (slow feature):  [ 10.2185087]
</pre>
<p>If you have a plotting package you could plot <tt class="docutils literal"><span class="pre">resc_dforce</span></tt> together with
<tt class="docutils literal"><span class="pre">slow</span></tt> and see that they match perfectly:</p>
<img alt="SFA estimate" src="results.png" style="width: 700px;" />
</div>
<div class="section">
<h1><a class="toc-backref" href="#id21" id="another-real-life-example-growing-neural-gas" name="another-real-life-example-growing-neural-gas">Another real life example (Growing neural gas)</a></h1>
<p>We generate uniformly distributed random data points confined on different
2-D geometrical objects. The Growing Neural Gas Node builds a graph with the
same topological structure.</p>
<p>Fix the random seed to obtain reproducible results:</p>
<pre class="literal-block">
&gt;&gt;&gt; mdp.numx_rand.seed(1266090063)
</pre>
<p>Some functions to generate uniform probability distributions on
different geometrical objects:</p>
<pre class="literal-block">
&gt;&gt;&gt; def uniform(min_, max_, dims):
...     &quot;&quot;&quot;Return a random number between min_ and max_ .&quot;&quot;&quot;
...     return mdp.numx_rand.random(dims)*(max_-min_)+min_
...
&gt;&gt;&gt; def circumference_distr(center, radius, n):
...     &quot;&quot;&quot;Return n random points uniformly distributed on a circumference.&quot;&quot;&quot;
...     phi = uniform(0, 2*mdp.numx.pi, (n,1))
...     x = radius*mdp.numx.cos(phi)+center[0]
...     y = radius*mdp.numx.sin(phi)+center[1]
...     return mdp.numx.concatenate((x,y), axis=1)
...
&gt;&gt;&gt; def circle_distr(center, radius, n):
...     &quot;&quot;&quot;Return n random points uniformly distributed on a circle.&quot;&quot;&quot;
...     phi = uniform(0, 2*mdp.numx.pi, (n,1))
...     sqrt_r = mdp.numx.sqrt(uniform(0, radius*radius, (n,1)))
...     x = sqrt_r*mdp.numx.cos(phi)+center[0]
...     y = sqrt_r*mdp.numx.sin(phi)+center[1]
...     return mdp.numx.concatenate((x,y), axis=1)
...
&gt;&gt;&gt; def rectangle_distr(center, w, h, n):
...     &quot;&quot;&quot;Return n random points uniformly distributed on a rectangle.&quot;&quot;&quot;
...     x = uniform(-w/2., w/2., (n,1))+center[0]
...     y = uniform(-h/2., h/2., (n,1))+center[1]
...     return mdp.numx.concatenate((x,y), axis=1)
...
&gt;&gt;&gt; N = 2000
</pre>
<p>Explicitly collect random points from some distributions:</p>
<ul>
<li><p class="first">Circumferences:</p>
<pre class="literal-block">
&gt;&gt;&gt; cf1 = circumference_distr([6,-0.5], 2, N)
&gt;&gt;&gt; cf2 = circumference_distr([3,-2], 0.3, N)
</pre>
</li>
<li><p class="first">Circles:</p>
<pre class="literal-block">
&gt;&gt;&gt; cl1 = circle_distr([-5,3], 0.5, N/2)
&gt;&gt;&gt; cl2 = circle_distr([3.5,2.5], 0.7, N)
</pre>
</li>
<li><p class="first">Rectangles:</p>
<pre class="literal-block">
&gt;&gt;&gt; r1 = rectangle_distr([-1.5,0], 1, 4, N)
&gt;&gt;&gt; r2 = rectangle_distr([+1.5,0], 1, 4, N)
&gt;&gt;&gt; r3 = rectangle_distr([0,+1.5], 2, 1, N/2)
&gt;&gt;&gt; r4 = rectangle_distr([0,-1.5], 2, 1, N/2)
</pre>
</li>
</ul>
<p>Shuffle the points to make the statistics stationary</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx.concatenate([cf1, cf2, cl1, cl2, r1,r2,r3,r4], axis=0)
&gt;&gt;&gt; x = mdp.numx.take(x,mdp.numx_rand.permutation(x.shape[0]), axis=0)
</pre>
<p>If you have a plotting package <tt class="docutils literal"><span class="pre">x</span></tt> should look like this:</p>
<img alt="GNG starting distribution" src="gng_distribution.png" style="width: 700px;" />
<p>Create a <tt class="docutils literal"><span class="pre">GrowingNeuralGasNode</span></tt> and train it:</p>
<pre class="literal-block">
&gt;&gt;&gt; gng = mdp.nodes.GrowingNeuralGasNode(max_nodes=75)
</pre>
<p>The initial distribution of nodes is randomly chosen:</p>
<img alt="GNG starting condition" src="gng_initial.png" style="width: 700px;" />
<p>The training is performed in small chunks in order to visualize
the evolution of the graph:</p>
<pre class="literal-block">
&gt;&gt;&gt; STEP = 500
&gt;&gt;&gt; for i in range(0,x.shape[0],STEP):
...     gng.train(x[i:i+STEP])
...     # [...] plotting instructions
...
&gt;&gt;&gt; gng.stop_training()
</pre>
<p>See <a class="reference" href="animated_training.gif">here</a> the animation of training.</p>
<p>Visualizing the neural gas network, we'll see that it is
adapted to the topological structure of the data distribution:</p>
<img alt="GNG final condition" src="gng_final.png" style="width: 700px;" />
<p>Calculate the number of connected components:</p>
<pre class="literal-block">
&gt;&gt;&gt; n_obj = len(gng.graph.connected_components())
&gt;&gt;&gt; print n_obj
5
</pre>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id22" id="node-list" name="node-list">Node List</a></h1>
<p>Here is the complete list of implemented nodes.
Refer to the
<a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/index.html">API</a>
for the full documentation and interface description.</p>
<ul>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.CuBICANode-class.html">CuBICANode</a></dt>
<dd><p class="first">Perform Independent Component Analysis using the CuBICA algorithm.</p>
<p class="last">Reference: Blaschke, T. and Wiskott, L. (2003).
<em>CuBICA: Independent Component Analysis by Simultaneous Third- and
Fourth-Order Cumulant Diagonalization</em>.
IEEE Transactions on Signal Processing, 52(5), pp. 1250-1256.
More information about ICA can be found among others in
Hyvarinen A., Karhunen J., Oja E. (2001). <em>Independent Component Analysis</em>,
Wiley.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.EtaComputerNode-class.html">EtaComputerNode</a></dt>
<dd><p class="first">Compute the eta values of the normalized training data.
The delta value of a signal is a measure of its temporal
variation, and is defined as the mean of the derivative squared,
i.e. <tt class="docutils literal"><span class="pre">delta(x)</span> <span class="pre">=</span> <span class="pre">mean(dx/dt(t)^2)</span></tt>. <tt class="docutils literal"><span class="pre">delta(x)</span></tt> is zero if
'x' is a constant signal, and increases if the temporal variation
of the signal is bigger.
The eta value is a more intuitive measure of temporal variation,
defined as <tt class="docutils literal"><span class="pre">eta(x)</span> <span class="pre">=</span> <span class="pre">T/(2*pi)</span> <span class="pre">*</span> <span class="pre">sqrt(delta(x))</span></tt>.
If 'x' is a signal of length 'T' which consists of a sine function
that accomplishes exactly 'N' oscillations, then <tt class="docutils literal"><span class="pre">eta(x)</span> <span class="pre">=</span> <span class="pre">N</span></tt>.</p>
<p class="last">Reference: Wiskott, L. and Sejnowski, T.J. (2002).
<em>Slow Feature Analysis:
Unsupervised Learning of Invariances</em>, Neural Computation,
14(4):715-770.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.FANode-class.html">FANode</a></dt>
<dd><p class="first last">Perform Factor Analysis. The current implementation should be most
efficient for long data sets: the sufficient statistics are
collected in the training phase, and all EM-cycles are performed at
its end. More information about Factor Analysis can be found in
<a class="reference" href="http://www.ics.uci.edu/~welling/classnotes/classnotes.html">Max Welling's classnotes</a>
in the chapter &quot;Linear Models&quot;.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.FastICANode-class.html">FastICANode</a></dt>
<dd><p class="first">Perform Independent Component Analysis using the FastICA algorithm.</p>
<p class="last">Reference:
Aapo Hyvarinen (1999).
<em>Fast and Robust Fixed-Point Algorithms for Independent Component Analysis</em>,
IEEE Transactions on Neural Networks, 10(3):626-634.
More information about ICA can be found among others in
Hyvarinen A., Karhunen J., Oja E. (2001). <em>Independent Component Analysis</em>,
Wiley.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.FDANode-class.html">FDANode</a></dt>
<dd><p class="first">Perform a (generalized) Fisher Discriminant Analysis of its
input. It is a supervised node that implements FDA using a
generalized eigenvalue approach.</p>
<p class="last">More information on Fisher Discriminant Analysis can be found for
example in C. Bishop, <em>Neural Networks for Pattern Recognition</em>,
Oxford Press, pp. 105-112.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.GaussianClassifierNode-class.html">GaussianClassifierNode</a></dt>
<dd><p class="first last">Perform a supervised Gaussian classification.  Given a set of
labelled data, the node fits a gaussian distribution to each
class.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.GrowingNeuralGasNode-class.html">GrowingNeuralGasNode</a></dt>
<dd><p class="first">Learn the topological structure of the input data by building a corresponding
graph approximation.</p>
<p class="last">More information about the Growing Neural Gas algorithm can be found in B.
Fritzke, <em>A Growing Neural Gas Network Learns Topologies</em>, in G. Tesauro, D. S.
Touretzky, and T. K. Leen (editors), <em>Advances in Neural Information
Processing Systems 7</em>, pages 625-632. MIT Press, Cambridge MA, 1995.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.HitParadeNode-class.html">HitParadeNode</a></dt>
<dd><p class="first last">Collect the first 'n' local maxima and minima of the training signal
which are separated by a minimum gap 'd'.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.ISFANode-class.html">ISFANode</a></dt>
<dd><p class="first">Perform Independent Slow Feature Analysis on the input data.</p>
<p class="last">More information about ISFA can be found in:
Blaschke, T. , Zito, T., and Wiskott, L.
<em>Independent Slow Feature Analysis and Nonlinear Blind Source Separation.</em>
Neural Computation 19(4):994-1021 (2007).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.contrib.JADENode-class.html">JADENode</a></dt>
<dd><p class="first">Original code contributed by Gabriel Beckers.</p>
<p>Perform Independent Component Analysis using the JADE algorithm.</p>
<p class="last">References:
Cardoso, J.-F, and Souloumiac, A.
<em>Blind beamforming for non Gaussian signals.</em>
Radar and Signal Processing, IEE Proceedings F, 140(6): 362-370 (1993), and
Cardoso, J.-F.
<em>High-order contrasts for independent component analysis.</em>
Neural Computation, 11(1): 157-192 (1999).
More information about ICA can be found among others in
Hyvarinen A., Karhunen J., Oja E. (2001). <em>Independent Component Analysis</em>,
Wiley.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.contrib.NIPALSNode-class.html">NIPALSNode</a></dt>
<dd><p class="first">Original code contributed by Michael Schmuker, Susanne Lezius, and Farzad Farkhooi.</p>
<p>Perform Principal Component Analysis using the NIPALS algorithm.
This algorithm is particularyl useful if you have more variable than
observations, or in general when the number of variables is huge and
calculating a full covariance matrix may be unfeasable. It's also more
efficient of the standard PCANode if you expect the number of significant
principal components to be a small. In this case setting output_dim to be
a certain fraction of the total variance, say 90%, may be of some help.</p>
<p>Reference for NIPALS (Nonlinear Iterative Partial Least Squares):
Wold, H.
<em>Nonlinear estimation by iterative least squares procedures.</em>
in David, F. (Editor), Research Papers in Statistics, Wiley,
New York, pp 411-444 (1966).</p>
<p class="last">More information about Principal Component Analysis, a.k.a. discrete
Karhunen-Loeve transform can be found among others in
I.T. Jolliffe, <em>Principal Component Analysis</em>, Springer-Verlag (1986).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.NoiseNode-class.html">NoiseNode</a></dt>
<dd><p class="first">Original code contributed by Mathias Franzius.</p>
<p class="last">Inject multiplicative or additive noise into the input data.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.PCANode-class.html">PCANode</a></dt>
<dd><p class="first">Filter the input data throug the most significatives of its
principal components.</p>
<p class="last">More information about Principal Component Analysis, a.k.a. discrete
Karhunen-Loeve transform can be found among others in
I.T. Jolliffe, <em>Principal Component Analysis</em>, Springer-Verlag (1986).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.PolynomialExpansionNode-class.html">PolynomialExpansionNode</a></dt>
<dd><p class="first last">Perform expansion in a polynomial space.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.QuadraticExpansionNode-class.html">QuadraticExpansionNode</a></dt>
<dd><p class="first last">Perform expansion in the space formed by all linear and quadratic
monomials</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.RBMNode-class.html">RBMNode</a></dt>
<dd><p class="first">Implementation of a Restricted Boltzmann Machine.</p>
<p class="last">For more information on RBMs, see
Geoffrey E. Hinton (2007) <a class="reference" href="http://www.scholarpedia.org/article/Boltzmann_machine">Boltzmann machine.</a>
Scholarpedia, 2(5):1668</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.RBMWithLabelsNode-class.html">RBMWithLabelsNode</a></dt>
<dd><p class="first">Implementation of a Restricted Boltzmann Machine with softmax labels.</p>
<p>For more information on RBMs, see
Geoffrey E. Hinton (2007) <a class="reference" href="http://www.scholarpedia.org/article/Boltzmann_machine">Boltzmann machine</a>
Scholarpedia, 2(5):1668</p>
<p class="last">Hinton, G. E, Osindero, S., and Teh, Y. W. <em>A fast learning
algorithm for deep belief nets</em>, Neural Computation, 18:1527-1554 (2006).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.SFANode-class.html">SFANode</a></dt>
<dd><p class="first">Extract the slowly varying components from the input data.</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., <em>Slow Feature Analysis: Unsupervised
Learning of Invariances</em>, Neural Computation, 14(4):715-770 (2002).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.SFA2Node-class.html">SFA2Node</a></dt>
<dd><p class="first">Get an input signal, expand it in the space of
inhomogeneous polynomials of degree 2 and extract its slowly varying
components. The <tt class="docutils literal"><span class="pre">get_quadratic_form</span></tt> method returns the input-output
function of one of the learned unit as a <tt class="docutils literal"><span class="pre">mdp.utils.QuadraticForm</span></tt> object.</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., <em>Slow Feature Analysis: Unsupervised
Learning of Invariances</em>, Neural Computation, 14(4):715-770 (2002).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.TimeFramesNode-class.html">TimeFramesNode</a></dt>
<dd><p class="first">Copy delayed version of the input signal on the space dimensions.</p>
<!-- ignore --><pre class="last literal-block">
For example, for time_frames=3 and gap=2:

[ X(1) Y(1)        [ X(1) Y(1) X(3) Y(3) X(5) Y(5)
  X(2) Y(2)          X(2) Y(2) X(4) Y(4) X(6) Y(6)
  X(3) Y(3)   --&gt;    X(3) Y(3) X(5) Y(5) X(7) Y(7)
  X(4) Y(4)          X(4) Y(4) X(6) Y(6) X(8) Y(8)
  X(5) Y(5)          ...  ...  ...  ...  ...  ... ]
  X(6) Y(6)
  X(7) Y(7)
  X(8) Y(8)
  ...  ...  ]
</pre>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.WhiteningNode-class.html">WhiteningNode</a></dt>
<dd><p class="first last">'Whiten' the input data by filtering it through the most
significatives of its principal components. All output
signals have zero mean, unit variance and are decorrelated.</p>
</dd>
</dl>
</li>
</ul>
<div class="admonition-didn-t-you-find-what-you-were-looking-for admonition">
<p class="first admonition-title">Didn't you find what you were looking for?</p>
<p class="last">If you want to contribute some code or a new
algorithm, please do not hesitate to submit it!</p>
</div>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id23" id="additional-utilities" name="additional-utilities">Additional utilities</a></h1>
<p>MDP offers some additional utilities of general interest
in the <tt class="docutils literal"><span class="pre">mdp.utils</span></tt> module. Refer to the
<a class="reference" href="http://mdp-toolkit.sourceforge.net/docs/api/index.html">API</a>
for the full documentation and interface description.</p>
<dl class="docutils">
<dt><strong>CovarianceMatrix</strong></dt>
<dd><p class="first">This class stores an empirical covariance matrix that can be updated
incrementally. A call to the <tt class="docutils literal"><span class="pre">fix</span></tt> method returns the current state
of the covariance matrix, the average and the number of observations,
and resets the internal data.</p>
<p class="last">Note that the internal sum is a standard <tt class="docutils literal"><span class="pre">__add__</span></tt> operation. We are not
using any of the fancy sum algorithms to avoid round off errors when
adding many numbers. If you want to contribute a <tt class="docutils literal"><span class="pre">CovarianceMatrix</span></tt>
class that uses such algorithms we would be happy to include it in
MDP.  For a start see the <a class="reference" href="http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/393090">Python recipe</a>
by Raymond Hettinger. For a
review about floating point arithmetic and its pitfalls see
this <a class="reference" href="http://docs.sun.com/source/806-3568/ncg_goldberg.html">interesting article</a>.</p>
</dd>
<dt><strong>DelayCovarianceMatrix</strong></dt>
<dd>This class stores an empirical covariance matrix between the signal and
time delayed signal that can be updated incrementally.</dd>
<dt><strong>MultipleCovarianceMatrices</strong></dt>
<dd>Container class for multiple covariance matrices to easily
execute operations on all matrices at the same time.</dd>
<dt><strong>dig_node(node)</strong></dt>
<dd>Crawl recursively an MDP <tt class="docutils literal"><span class="pre">Node</span></tt> looking for arrays.
Return (dictionary, string), where the dictionary is:
{ attribute_name: (size_in_bytes, array_reference)}
and string is a nice string representation of it.</dd>
<dt><strong>get_node_size(node)</strong></dt>
<dd>Get 'node' total byte-size using <tt class="docutils literal"><span class="pre">cPickle</span></tt> with protocol=2.
(The byte-size is related the memory needed by the node).</dd>
<dt><strong>progressinfo(sequence, length, style, custom)</strong></dt>
<dd><p class="first">A fully configurable text-mode progress info box.
To get a progress info box for your loops use it like this:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; for i in progressinfo(sequence):
...     do_something(i)
</pre>
<p>You can also use it with generators, files or any other iterable object,
but in this case you have to specify the total length of the sequence:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; for line in progressinfo(open_file, nlines):
...     do_something(line)
</pre>
<p>A few examples of the available layouts:</p>
<!-- ignore --><pre class="last literal-block">
[===================================73%==============&gt;...................]

Progress:  67%[======================================&gt;                   ]

23% [02:01:28] - [00:12:37]
</pre>
</dd>
<dt><strong>QuadraticForm</strong></dt>
<dd>Define an inhomogeneous quadratic form as <tt class="docutils literal"><span class="pre">1/2</span> <span class="pre">x'Hx</span> <span class="pre">+</span> <span class="pre">f'x</span> <span class="pre">+</span> <span class="pre">c</span></tt>.
This class implements the quadratic form analysis methods
presented in:
Berkes, P. and Wiskott, L. On the analysis and interpretation
of inhomogeneous quadratic forms as receptive fields. <em>Neural
Computation</em>, 18(8): 1868-1895. (2006).</dd>
<dt><strong>refcast(array, dtype)</strong></dt>
<dd>Cast the array to 'dtype' only if necessary,
otherwise return a reference.</dd>
<dt><strong>rotate(mat, angle, columns, units)</strong></dt>
<dd><p class="first">Rotate in-place a NxM data matrix in the plane defined by the 'columns'
when observation are stored on rows. Observations are rotated
counterclockwise. This corresponds to the following matrix-multiplication
for each data-point (unchanged elements omitted):</p>
<!-- ignore --><pre class="last literal-block">
[  cos(angle) -sin(angle)     [ x_i ]
   sin(angle)  cos(angle) ] * [ x_j ]
</pre>
</dd>
<dt><strong>random_rot(dim, dtype)</strong></dt>
<dd>Return a random rotation matrix, drawn from the Haar distribution
(the only uniform distribution on SO(n)).
The algorithm is described in the paper
Stewart, G.W., <em>The efficient generation of random orthogonal
matrices with an application to condition estimators</em>, SIAM Journal
on Numerical Analysis, 17(3), pp. 403-409, 1980.
For more information see this <a class="reference" href="http://en.wikipedia.org/wiki/Orthogonal_matrix#Randomization">Wikipedia entry</a>.</dd>
<dt><strong>symrand(dim_or_eigv, dtype)</strong></dt>
<dd>Return a random symmetric (Hermitian) matrix with eigenvalues
uniformly distributed on (0,1].</dd>
</dl>
<div class="section">
<h2><a class="toc-backref" href="#id24" id="graph-module" name="graph-module">Graph module</a></h2>
<p>MDP contains <tt class="docutils literal"><span class="pre">mdp.graph</span></tt>, a lightweight package to handle directed graphs.</p>
<dl class="docutils">
<dt><strong>Graph</strong></dt>
<dd><p class="first">Represent a directed graph. This class contains several methods
to create graph structures and manipulate them, among which</p>
<ul class="last">
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">add_tree</span></tt>: Add a tree to the graph.</dt>
<dd><p class="first">The tree is specified with a nested list of tuple, in a LISP-like
notation. The values specified in the list become the values of
the single nodes.
Return an equivalent nested list with the nodes instead of the values.</p>
<p>Example:</p>
<!-- ignore --><pre class="last literal-block">
&gt;&gt;&gt; a=b=c=d=e=None
&gt;&gt;&gt; g.add_tree( (a, b, (c, d ,e)) )
# corresponds to this tree structure, with all node values set to None:

        a
       / \
      b   c
         / \
        d   e
</pre>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">topological_sort</span></tt>: Perform a topological sort of the nodes.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">dfs</span></tt>, <tt class="docutils literal"><span class="pre">undirected_dfs</span></tt>: Perform Depth First sort.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">bfs</span></tt>, <tt class="docutils literal"><span class="pre">undirected_bfs</span></tt>: Perform Breadth First sort.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">connected_components</span></tt>: Return a list of lists containing</dt>
<dd><p class="first last">the nodes of all connected components of the graph.</p>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">is_weakly_connected</span></tt>: Return True if the graph is weakly connected.</p>
</li>
</ul>
</dd>
<dt><strong>GraphEdge</strong></dt>
<dd>Represent a graph edge and all information attached to it.</dd>
<dt><strong>GraphNode</strong></dt>
<dd>Represent a graph node and all information attached to it.</dd>
<dt><strong>recursive_map(func, seq)</strong></dt>
<dd>Apply a function recursively on a sequence and all subsequences.</dd>
<dt><strong>recursive_reduce(func, seq, *argv)</strong></dt>
<dd>Apply <tt class="docutils literal"><span class="pre">reduce(func,</span> <span class="pre">seq)</span></tt> recursively to a sequence and all its
subsequences.</dd>
</dl>
</div>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id25" id="to-do" name="to-do">To Do</a></h1>
<p>In this section we want to give you an overview about our
plans for the development of MDP:</p>
<ul class="simple">
<li>Add more data processing algorithms.</li>
<li>Advanced usage of the hinet package will be possible only in presence of
an easy and intuitive GUI :)</li>
<li>Wait for a good guy who wants to contribute a <tt class="docutils literal"><span class="pre">CovarianceMatrix</span></tt> class that
uses some of the fancy sum algorithms to avoid round off errors when
adding many numbers.</li>
</ul>
</div>
<div class="section">
<h1><a class="toc-backref" href="#id26" id="contributors" name="contributors">Contributors</a></h1>
<p>In this final section we want to thank all users who have contributed
code to the MDP project. Strictly in alphabetical order:</p>
<ul class="simple">
<li><a class="reference" href="http://www.gbeckers.nl/">Gabriel Beckers</a></li>
<li><a class="reference" href="http://www.bccn-berlin.de/People/farkhooi">Farzad Farkhooi</a></li>
<li>Mathias Franzius</li>
<li>Susanne Lezius</li>
<li><a class="reference" href="http://userpage.fu-berlin.de/~schmuker/">Michael Schmuker</a></li>
</ul>
</div>
</div>
<div class="footer">
<hr class="footer" />
Generated on: 2008-04-03 10:39 UTC.
Generated by <a class="reference" href="http://docutils.sourceforge.net/">Docutils</a> from <a class="reference" href="http://docutils.sourceforge.net/rst.html">reStructuredText</a> source.

</div>
</div>

<div id="footer">

<hr />
<a href="http://sourceforge.net"> <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=116959&amp;type=1" width="88" height="31" border="0" alt="SourceForge.net Logo" /></a>

<a href="http://validator.w3.org/check?uri=referer;verbose=1"><img border="0" src="valid-html401.png" alt="Valid HTML 4.01!" height="31" width="88" /></a>

<script language="javascript" type="text/javascript">
<!--
document.write("<p />Last modified: "+document.lastModified+"")
-->
</script>

</div>

</div>

</body>
</html>

