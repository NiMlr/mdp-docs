<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.6: http://docutils.sourceforge.net/" />
<title>Tutorial</title>
<meta name="author" content="Pietro Berkes, Niko Wilbert, Henning Sprekeler, and Tiziano Zito" />
<meta name="copyright" content="This document has been placed in the public domain." />
<meta content="Modular toolkit for Data Processing (MDP) Tutorial" name="description" />
<meta content="data processing, toolkit, scipy, python, SFA, ICA, PCA" name="keywords" />
<style type="text/css">

body {
        font-family:verdana;
	generic-family:sans-serif;
	text-align:justify;
	margin-bottom:50px;
	}

a:visited, a:link { 
	   text-decoration:none;
}

a:hover {
	color: black;
}

/* tables */
table { border-style:none; }
td { vertical-align:top; }
.screentable { text-align:center; }
.td_header { vertical-align:middle;
	     padding-left:15px;
	}

/* lists */

UL { padding-left:1em; 
list-style-image: url('blkpearl.png');
   }

UL LI {
	margin:0.5em 0.5em 0.5em 0.5em;
}

OL LI {
	margin:0.5em 0.5em 0.5em 0.5em;
}

DT { font-weight:bold; }

H1 {
     font-family:times;
     generic-family:serif;
}


#news-box {
  margin-top: 28px;
  /*margin-right: 35%;*/
  padding: .5em; 
  /*font-size:smaller;*/
}

.highlight {
	font-weight:bold;
}

.smaller_font {
	font-size:smaller;
}

code {
	text-align:left;
}

pycode {
	text-align:left;
	margin-left:2em;
	font-weight:bold;
}

pre.literal-block {
  font-family:courier;
  generic-family:monospace;
  overflow: auto;
  padding: .5em; 
  margin: 1em;
}

.soft {
	font-weight:bold;
}

#header {
	text-align:center;
	padding:5px;
	text-decoration:none;
	font-size:250%;
	font-weight:bold;
}

IMG.whiteb  { 	
	border-width:2px;
	border-color:white;
	border-style:solid;
 }


#sidebar{ 
clear: left;
float: left;
width: 10em;
margin: 10px 10px 10px 0;
padding: 0;
}

#sidebar ul {	
list-style: none;
width: auto;
margin: 18px 0 20px 0;
padding: 0;
text-align:center; 
}	

#sidebar li {
margin-top:4px;
margin-bottom: 4px;
}

#sidebar li a {
font-weight: bold;
text-decoration: none;
display: block;
padding: 4px 0px 4px 0px;
}
	
#container {
	padding-right:30px;
	padding-left:30px;
	margin-right:auto;
	margin-left:auto;
	max-width:800px; 
}

#content {
    margin-left:11em;
}

#footer {
}

/*------------ docutils ---------------*/
.first {
  margin-top: 0 ! important }

.last {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family:verdana;
  generic-family:sans-serif; }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title {
  color: red ;
  font-weight: bold ;
  font-family:verdana;
  generic-family: sans-serif; }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }


div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font-family:times;
  generic-family:serif;
  font-size: 100% }

pre.line-block {
  font-family:times;
  generic-family:serif;
  font-size: 100% }

span.classifier {
  font-family:verdana;
  generic-family:sans-serif;
  font-style: oblique }

span.classifier-delimiter {
  font-family:verdana;
  generic-family:sans-serif;
  font-weight: bold }

span.interpreted {
  font-family:verdana;
  generic-family:sans-serif; }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

table.citation {
  border-left: solid thin gray }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid thin black }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

tt.docutils { 
  font-weight: bold ;
}

ul.auto-toc {
  list-style-type: none }


/* COLORS DEFINITION */
body { background: white; }

code, pycode, #header, #sidebar li a {color:#A11200;}

a:visited, a:link {color:#A11200;} 

#header, a:hover, pre.literal-block, .img_header a:hover, #sidebar li a {background: #FFD242;}

#news-box, #header, pre.literal-block, #sidebar li a , div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {border: 1px solid #A11200;}

#sidebar li a:hover { color:black; }

</style>
</head>
<body><div id="container">
<div id="header">

<table width="100%">
<tr>
<td align="left" class="td_header">Modular toolkit for<br>Data Processing</td>
<td align="right" class="img_header">
<a href="logo_animation.html">
<img src="logo.png" alt="MDP logo" title="click to see the animated logo!" class="whiteb"/></a></td></tr>
</table>
</div>

<div id="sidebar">
<ul>
<li><a href="index.html">Home</a>
<li><a href="tutorial.html">Tutorial</a>
<li><a href="index.html#DOWINS">Download</a>
<li><a href="tutorial.html#node-list">Node list</a>
<li><a href="docs/api/index.html">API</a>
<li><a href="http://sourceforge.net/mail/?group_id=116959">Mailing list</a>
<li><a href="symeig.html">symeig</a>
</ul>
</div>

<div id="content">

<div class="document" id="tutorial">
<h1 class="title">Tutorial</h1>
<table class="docinfo" frame="void" rules="none">
<col class="docinfo-name" />
<col class="docinfo-content" />
<tbody valign="top">
<tr><th class="docinfo-name">Author:</th>
<td>Pietro Berkes, Niko Wilbert, Henning Sprekeler, and Tiziano Zito</td></tr>
<tr class="field"><th class="docinfo-name">Homepage:</th><td class="field-body"><a class="reference external" href="http://mdp-toolkit.sourceforge.net">http://mdp-toolkit.sourceforge.net</a></td>
</tr>
<tr><th class="docinfo-name">Copyright:</th>
<td>This document has been placed in the public domain.</td></tr>
<tr><th class="docinfo-name">Version:</th>
<td>2.6</td></tr>
</tbody>
</table>
This document is also available as <a href="http://prdownloads.sourceforge.net/mdp-toolkit/MDP2_6_tutorial.pdf?download">pdf file</a> (830 KB).<p>This is a guide to basic and some more advanced features of
the MDP library. Besides the present tutorial, you can learn
more about MDP by using the standard Python tools.
All MDP nodes have doc-strings, the public
attributes and methods have telling names: All information about a
node can be obtained using  the <tt class="docutils literal">help</tt> and <tt class="docutils literal">dir</tt> functions within
the Python interpreter. In addition to that, an automatically generated
<a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/index.html">API</a> is
available.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p>Code snippets throughout the script will be denoted by:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; print &quot;Hello world!&quot;
Hello world!
</pre>
<p>To run the following code examples don't forget to import mdp
in your Python session with:</p>
<pre class="literal-block">
&gt;&gt;&gt; import mdp
</pre>
<p class="last">You'll find all the code of this tutorial within the <tt class="docutils literal">demo</tt> directory
in the MDP installation path.</p>
</div>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id4">Introduction</a></li>
<li><a class="reference internal" href="#quick-start" id="id5">Quick Start</a></li>
<li><a class="reference internal" href="#nodes" id="id6">Nodes</a><ul>
<li><a class="reference internal" href="#node-instantiation" id="id7">Node Instantiation</a></li>
<li><a class="reference internal" href="#node-training" id="id8">Node Training</a></li>
<li><a class="reference internal" href="#node-execution" id="id9">Node Execution</a></li>
<li><a class="reference internal" href="#node-inversion" id="id10">Node Inversion</a></li>
<li><a class="reference internal" href="#writing-your-own-nodes-subclassing-node" id="id11">Writing your own nodes: subclassing Node</a></li>
</ul>
</li>
<li><a class="reference internal" href="#flows" id="id12">Flows</a><ul>
<li><a class="reference internal" href="#flow-instantiation-training-and-execution" id="id13">Flow instantiation, training and execution</a></li>
<li><a class="reference internal" href="#flow-inversion" id="id14">Flow inversion</a></li>
<li><a class="reference internal" href="#flows-are-container-type-objects" id="id15">Flows are container type objects</a></li>
<li><a class="reference internal" href="#crash-recovery" id="id16">Crash recovery</a></li>
</ul>
</li>
<li><a class="reference internal" href="#iterables" id="id17">Iterables</a></li>
<li><a class="reference internal" href="#checkpoints" id="id18">Checkpoints</a></li>
<li><a class="reference internal" href="#node-extensions" id="id19">Node Extensions</a><ul>
<li><a class="reference internal" href="#using-extensions" id="id20">Using Extensions</a></li>
<li><a class="reference internal" href="#writing-extension-nodes" id="id21">Writing Extension Nodes</a></li>
<li><a class="reference internal" href="#creating-extensions" id="id22">Creating Extensions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hierarchical-networks" id="id23">Hierarchical Networks</a><ul>
<li><a class="reference internal" href="#building-blocks" id="id24">Building blocks</a></li>
<li><a class="reference internal" href="#html-representation" id="id25">HTML representation</a></li>
<li><a class="reference internal" href="#example-application-2-d-image-data" id="id26">Example application (2-D image data)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#parallelization" id="id27">Parallelization</a><ul>
<li><a class="reference internal" href="#basic-examples" id="id28">Basic Examples</a></li>
<li><a class="reference internal" href="#scheduler" id="id29">Scheduler</a></li>
<li><a class="reference internal" href="#parallel-nodes" id="id30">Parallel Nodes</a></li>
<li><a class="reference internal" href="#parallel-flows" id="id31">Parallel Flows</a></li>
</ul>
</li>
<li><a class="reference internal" href="#real-life-examples" id="id32">Real life examples</a><ul>
<li><a class="reference internal" href="#logistic-maps" id="id33">Logistic maps</a></li>
<li><a class="reference internal" href="#growing-neural-gas" id="id34">Growing neural gas</a></li>
<li><a class="reference internal" href="#locally-linear-embedding" id="id35">Locally linear embedding</a></li>
</ul>
</li>
<li><a class="reference internal" href="#node-list" id="id36">Node List</a></li>
<li><a class="reference internal" href="#additional-utilities" id="id37">Additional utilities</a><ul>
<li><a class="reference internal" href="#html-slideshows" id="id38">HTML Slideshows</a></li>
<li><a class="reference internal" href="#graph-module" id="id39">Graph module</a></li>
</ul>
</li>
<li><a class="reference internal" href="#bimdp" id="id40">BiMDP</a><ul>
<li><a class="reference internal" href="#targets-id-s-and-messages" id="id41">Targets, id's and Messages</a></li>
<li><a class="reference internal" href="#biflow" id="id42">BiFlow</a></li>
<li><a class="reference internal" href="#binode" id="id43">BiNode</a></li>
<li><a class="reference internal" href="#inspection" id="id44">Inspection</a></li>
<li><a class="reference internal" href="#extending-binode-and-message-handling" id="id45">Extending BiNode and Message Handling</a></li>
<li><a class="reference internal" href="#hinet-in-bimdp" id="id46">HiNet in BiMDP</a></li>
<li><a class="reference internal" href="#parallel-in-bimdp" id="id47">Parallel in BiMDP</a></li>
</ul>
</li>
<li><a class="reference internal" href="#future-development" id="id48">Future Development</a></li>
<li><a class="reference internal" href="#contributors" id="id49">Contributors</a></li>
</ul>
</div>
<!-- This data file has been placed in the public domain. -->
<!-- Derived from the Unicode character mappings available from
<http://www.w3.org/2003/entities/xml/>.
Processed by unicode2rstsubs.py, part of Docutils:
<http://docutils.sourceforge.net>. -->
<div class="section" id="introduction">
<h1><a class="toc-backref" href="#id4">Introduction</a></h1>
<p>The use of the Python programming language in computational
neuroscience has been growing steadily during the past few years. The
maturation of two important open source projects, the scientific
libraries <a class="reference external" href="http://numpy.scipy.org">NumPy</a> and
<a class="reference external" href="http://www.scipy.org">SciPy</a>, gives access to a large
collection of scientific functions that rivals in size and speed with
well known commercial alternatives like The MathWorks™
<a class="reference external" href="http://www.mathworks.com/products/matlab">Matlab</a>©.
Furthermore, the flexible and dynamic nature of Python offers the
scientific programmer the opportunity to quickly develop efficient and
structured software while maximizing prototyping and reusability
capabilities.</p>
<p>The <a class="reference external" href="http://mdp-toolkit.sourceforge.net">Modular toolkit for Data Processing (MDP)</a> package contributes to this
growing community a library of widely used data processing algorithms,
and the possibility to combine them according to a pipeline analogy to
build more complex data processing software.</p>
<p>MDP has been designed to be used as-is and as a framework for
scientific data processing development.</p>
<p>From the user's perspective, MDP consists of a collection of
supervised and unsupervised learning algorithms, and other data
processing units (<em>nodes</em>) that can be combined into data processing
sequences (<em>flows</em>) and more complex feed-forward network
architectures. Given a set of input data, MDP takes care of
successively training or executing all nodes in the network. This
allows the user to specify complex algorithms as a series of simpler
data processing steps in a natural way.</p>
<p>The base of available algorithms is steadily increasing and includes,
to name but the most common, Principal Component Analysis (PCA and
NIPALS), several Independent Component Analysis algorithms (CuBICA,
FastICA, TDSEP, JADE, and XSFA), Slow Feature Analysis, Gaussian
Classifiers, Restricted Boltzmann Machine, and Locally Linear Embedding
(see the <a class="reference internal" href="#node-list">Node List</a> section for a more exhaustive list and
references).</p>
<p>Particular care has been taken to make computations efficient in terms
of speed and memory.  To reduce memory requirements, it is possible to
perform learning using batches of data, and to define the internal
parameters of the nodes to be single precision, which makes the usage of
very large data sets possible.  Moreover, the <tt class="docutils literal">parallel</tt> subpackage
offers a parallel implementation of the basic nodes and flows.</p>
<p>From the developer's perspective, MDP is a framework that makes the
implementation of new supervised and unsupervised learning algorithms
easy and straightforward.  The basic class, <tt class="docutils literal">Node</tt>, takes care of
tedious tasks like numerical type and dimensionality checking, leaving
the developer free to concentrate on the implementation of the
learning and execution phases. Because of the common interface, the
node then automatically integrates with the rest of the library and
can be used in a network together with other nodes. A node can have
multiple training phases and even an undetermined number of phases.
This allows the implementation of algorithms that need to collect some
statistics on the whole input before proceeding with the actual
training, and others that need to iterate over a training phase until
a convergence criterion is satisfied. The ability to train each phase
using chunks of input data is maintained if the chunks are given as an
iterable. Moreover, crash recovery is optionally available: in case of
failure, the current state of the flow is saved for later inspection.</p>
<p>MDP is distributed under the open source LGPL license. It has been
written in the context of theoretical research in neuroscience, but it
has been designed to be helpful in any context where trainable data
processing algorithms are used. Its simplicity on the user's side, the
variety of readily available algorithms, and the reusability of the
implemented nodes make it also a useful educational tool.</p>
<p>With over 10,000 downloads since its first public release in 2004, MDP
has become a widely used Python scientific software. It has minimal
dependencies, requiring only the NumPy numerical extension, is
completely platform-independent, and is available as a
<a class="reference external" href="http://packages.debian.org/python-mdp">package</a>
in the GNU/Linux
<a class="reference external" href="http://www.debian.org">Debian</a> distribution and the
<a class="reference external" href="http://www.pythonxy.com">Python(x,y)</a> scientific Python
distribution.</p>
<p>As the number of its users and contributors is increasing, MDP appears
to be a good candidate for becoming a community-driven common
repository of user-supplied, freely available, Python implemented data
processing algorithms.</p>
</div>
<div class="section" id="quick-start">
<h1><a class="toc-backref" href="#id5">Quick Start</a></h1>
<p>Using MDP is as easy as:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; import mdp
&gt;&gt;&gt; # perform pca on some data x
...
&gt;&gt;&gt; y = mdp.pca(x)
&gt;&gt;&gt; # perform ica on some data x using single precision
...
&gt;&gt;&gt; y = mdp.fastica(x, dtype='float32')
</pre>
<p>MDP requires the numerical Python extensions <a class="reference external" href="http://numpy.scipy.org">NumPy</a> or <a class="reference external" href="http://www.scipy.org">SciPy</a>. At
import time MDP will select <tt class="docutils literal">scipy</tt> if available, otherwise
<tt class="docutils literal">numpy</tt> will be loaded. You can force the use of a numerical
extension by setting the environment variable <tt class="docutils literal">MDPNUMX=numpy</tt> or
<tt class="docutils literal">MDPNUMX=scipy</tt>.</p>
<div class="admonition-an-important-remark admonition">
<p class="first admonition-title">An important remark</p>
<p class="last">Input array data is typically assumed to be two-dimensional and
ordered such that observations of the same variable are stored on
rows and different variables are stored on columns.</p>
</div>
</div>
<div class="section" id="nodes">
<h1><a class="toc-backref" href="#id6">Nodes</a></h1>
<p>A <em>node</em> is the basic building block of an MDP application.  It
represents a data processing element, like for example a learning
algorithm, a data filter, or a visualization step (see the <a class="reference internal" href="#node-list">Node
List</a> section for an exhaustive list and references).</p>
<p>Each node can have one or more training phases, during which the
internal structures are learned from training data (e.g. the weights
of a neural network are adapted or the covariance matrix is estimated)
and an execution phase, where new data can be processed forwards (by
processing the data through the node) or backwards (by applying the
inverse of the transformation computed by the node if defined).</p>
<p>Nodes have been designed to be applied to arbitrarily long sets of data:
if the underlying algorithms supports it, the internal structures can
be updated incrementally by sending multiple batches of data (this is
equivalent to online learning if the chunks consists of single
observations, or to batch learning if the whole data is sent in a
single chunk). It is thus possible to perform computations on amounts
of data that would not fit into memory or to generate data on-the-fly.</p>
<p>A <tt class="docutils literal">Node</tt> also defines some utility methods, like for example
<tt class="docutils literal">copy</tt> and <tt class="docutils literal">save</tt>, that return an exact copy of a node and save it
in a file, respectively. Additional methods may be present, depending on the
algorithm.</p>
<div class="section" id="node-instantiation">
<h2><a class="toc-backref" href="#id7">Node Instantiation</a></h2>
<p>Nodes can be obtained by creating an instance of the <tt class="docutils literal">Node</tt> class.</p>
<p>Each node is characterized by an input dimension (i.e., the
dimensionality of the input vectors), an output dimension, and a
<tt class="docutils literal">dtype</tt>, which determines the numerical type of the internal
structures and of the output signal. By default, these attributes are
inherited from the input data if left unspecified. The constructor of
each node class can require other task-specific arguments. The full
documentation is always available in the doc-string of the node's
class.</p>
<p>Some examples of node instantiation:</p>
<ul>
<li><p class="first">Create a node that performs Principal Component Analysis (PCA)
whose input dimension and <tt class="docutils literal">dtype</tt>
are inherited from the input data during training. Output dimensions
default to input dimensions.</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1 = mdp.nodes.PCANode()
&gt;&gt;&gt; pcanode1
PCANode(input_dim=None, output_dim=None, dtype=None)
</pre>
</li>
<li><p class="first">Setting <tt class="docutils literal">output_dim = 10</tt> means that the node will keep only the
first 10 principal components of the input.</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode2 = mdp.nodes.PCANode(output_dim = 10)
&gt;&gt;&gt; pcanode2
PCANode(input_dim=None, output_dim=10, dtype=None)
</pre>
<p>The output dimensionality can also be specified in terms of the explained
variance. If we want to keep the number of principal components which can
account for 80% of the input variance, we set:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode3 = mdp.nodes.PCANode(output_dim = 0.8)
&gt;&gt;&gt; pcanode3.desired_variance
0.80000000000000004
</pre>
</li>
<li><p class="first">If <tt class="docutils literal">dtype</tt> is set to <tt class="docutils literal">float32</tt> (32-bit float), the input
data is cast to single precision when received and the internal
structures are also stored as <tt class="docutils literal">float32</tt>. <tt class="docutils literal">dtype</tt> influences the
memory space necessary for a node and the precision with which the
computations are performed.</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode4 = mdp.nodes.PCANode(dtype = 'float32')
&gt;&gt;&gt; pcanode4
PCANode(input_dim=None, output_dim=None, dtype='float32')
</pre>
<p>You can obtain a list of the numerical types supported by a node
lookng at its <tt class="docutils literal">supported_dtypes</tt> property:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode4.supported_dtypes
[dtype('float32'), dtype('float64')]
</pre>
<p>This attribute is a list of <tt class="docutils literal">numpy.dtype</tt> objects.</p>
</li>
<li><p class="first">A <tt class="docutils literal">PolynomialExpansionNode</tt> expands its input in the space
of polynomials of a given degree by computing all monomials up
to the specified degree. Its constructor needs as first argument
the degree of the polynomials space (3 in this case).</p>
<pre class="literal-block">
&gt;&gt;&gt; expnode = mdp.nodes.PolynomialExpansionNode(3)
</pre>
</li>
</ul>
</div>
<div class="section" id="node-training">
<h2><a class="toc-backref" href="#id8">Node Training</a></h2>
<p>Some nodes need to be trained to perform their task. For example, the
Principal Component Analysis (PCA) algorithm requires the computation
of the mean and covariance matrix of a set of training data from which
the principal eigenvectors of the data distribution are estimated.</p>
<p>This can be done during a training phases by calling the <tt class="docutils literal">train</tt>
method.  MDP supports both supervised and unsupervised training, and
algorithms with multiple training phases.</p>
<p>Some examples of node training:</p>
<ul>
<li><p class="first">Create some random data to train the node</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 25))  # 25 variables, 100 observations
</pre>
</li>
<li><p class="first">Analyzes the batch of data <tt class="docutils literal">x</tt> and update the estimation of
mean and covariance matrix:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1.train(x)
</pre>
<p>At this point the input dimension and the <tt class="docutils literal">dtype</tt> have been
inherited from <tt class="docutils literal">x</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1
PCANode(input_dim=25, output_dim=None, dtype='float64')
</pre>
</li>
<li><p class="first">We can train our node with more than one chunk of data. This
is especially useful when the input data is too long to
be stored in memory or when it has to be created on-the-fly.
(See also the <a class="reference internal" href="#iterables">Iterables</a> section):</p>
<pre class="literal-block">
&gt;&gt;&gt; for i in range(100):
...     x = mdp.numx_rand.random((100, 25))
...     pcanode1.train(x)
&gt;&gt;&gt;
</pre>
</li>
<li><p class="first">Some nodes don't need to or cannot be trained:</p>
<pre class="literal-block">
&gt;&gt;&gt; expnode.is_trainable()
False
</pre>
<p>Trying to train them anyway would raise
an <tt class="docutils literal">IsNotTrainableException</tt>.</p>
</li>
<li><p class="first">The training phase ends when the <tt class="docutils literal">stop_training</tt>, <tt class="docutils literal">execute</tt>,
<tt class="docutils literal">inverse</tt>, and possibly some other node-specific methods are called.
For example we can finalyze the PCA algorithm by computing and selecting
the principal eigenvectors</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1.stop_training()
</pre>
</li>
<li><p class="first">If the <tt class="docutils literal">PCANode</tt> was declared to have a number of output components
dependent on the input variance to be explained, we can check after
training the number of output components and the actually explained variance:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode3.train(x)
&gt;&gt;&gt; pcanode3.stop_training()
&gt;&gt;&gt; pcanode3.output_dim
16
&gt;&gt;&gt; pcanode3.explained_variance
0.85261144755506446
</pre>
<p>It is now possible to access the trained internal data. In general,
a list of the interesting internal attributes can be found in the
class documentation.</p>
<pre class="literal-block">
&gt;&gt;&gt; avg = pcanode1.avg            # mean of the input data
&gt;&gt;&gt; v = pcanode1.get_projmatrix() # projection matrix
</pre>
</li>
<li><p class="first">Some nodes, namely the one corresponding to supervised algorithms, e.g.
Fisher Discriminant Analysis (FDA), may need some labels or other
supervised signals to be passed
during training. Detailed information about the signature of the
<tt class="docutils literal">train</tt> method can be read in its doc-string.</p>
<pre class="literal-block">
&gt;&gt;&gt; fdanode = mdp.nodes.FDANode()
&gt;&gt;&gt; for label in ['a', 'b', 'c']:
...     x = mdp.numx_rand.random((100, 25))
...     fdanode.train(x, label)
&gt;&gt;&gt;
</pre>
</li>
<li><p class="first">A node could also require multiple training phases. For example, the
training of <tt class="docutils literal">fdanode</tt> is not complete yet, since it has two
training phases: The first one computing the mean of the data
conditioned on the labels, and the second one computing the overall
and within-class covariance matrices and solving the FDA
problem. The first phase must be stopped and the second one trained:</p>
<pre class="literal-block">
&gt;&gt;&gt; fdanode.stop_training()
&gt;&gt;&gt; for label in ['a', 'b', 'c']:
...     x = mdp.numx_rand.random((100, 25))
...     fdanode.train(x, label)
&gt;&gt;&gt;
</pre>
<p>The easiest way to train multiple phase nodes is using flows,
which automatically handle multiple phases (see the <a class="reference internal" href="#flows">Flows</a> section).</p>
</li>
</ul>
</div>
<div class="section" id="node-execution">
<h2><a class="toc-backref" href="#id9">Node Execution</a></h2>
<p>Once the training is finished, it is possible to execute the node:</p>
<ul>
<li><p class="first">The input data is projected on the principal components learned
in the training phase:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 25))
&gt;&gt;&gt; y_pca = pcanode1.execute(x)
</pre>
</li>
<li><p class="first">Calling a node instance is equivalent to executing it:</p>
<pre class="literal-block">
&gt;&gt;&gt; y_pca = pcanode1(x)
</pre>
</li>
<li><p class="first">The input data is expanded in the space of polynomials of
degree 3:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 5))
&gt;&gt;&gt; y_exp = expnode(x)
</pre>
</li>
<li><p class="first">The input data is projected to the directions learned by FDA:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx_rand.random((100, 25))
&gt;&gt;&gt; y_fda = fdanode(x)
</pre>
</li>
<li><p class="first">Some nodes may allow for optional arguments in the <tt class="docutils literal">execute</tt> method.
As always the complete information can be found in the doc-string.</p>
</li>
</ul>
</div>
<div class="section" id="node-inversion">
<h2><a class="toc-backref" href="#id10">Node Inversion</a></h2>
<p>If the operation computed by the node is invertible, the node can also
be executed <em>backwards</em>, thus computing the inverse transformation:</p>
<ul>
<li><p class="first">In the case of PCA, for example, this corresponds to projecting a
vector in the principal components space back to the original data
space:</p>
<pre class="literal-block">
&gt;&gt;&gt; pcanode1.is_invertible()
True
&gt;&gt;&gt; x = pcanode1.inverse(y_pca)
</pre>
</li>
<li><p class="first">The expansion node in not invertible:</p>
<pre class="literal-block">
&gt;&gt;&gt; expnode.is_invertible()
False
</pre>
<p>Trying to compute the inverse would raise an <tt class="docutils literal">IsNotInvertibleException</tt>.</p>
</li>
</ul>
</div>
<div class="section" id="writing-your-own-nodes-subclassing-node">
<h2><a class="toc-backref" href="#id11">Writing your own nodes: subclassing Node</a></h2>
<p>MDP tries to make it easy to write new nodes that interface with the
existing data processing elements.</p>
<p>The <tt class="docutils literal">Node</tt> class is designed to make the implementation of new
algorithms easy and intuitive. This base class takes care of setting
input and output dimension and casting the data to match the numerical
type (e.g. float or double) of the internal variables, and offers
utility methods that can be used by the developer.</p>
<p>To expand the MDP library of implemented nodes with user-made nodes,
it is sufficient to subclass <tt class="docutils literal">Node</tt>, overriding some of
the methods according to the algorithm one wants to implement,
typically the <tt class="docutils literal">_train</tt>, <tt class="docutils literal">_stop_training</tt>, and <tt class="docutils literal">_execute</tt>
methods.</p>
<p>In its namespace MDP offers references to the main modules <tt class="docutils literal">numpy</tt>
or <tt class="docutils literal">scipy</tt>, and the subpackages <tt class="docutils literal">linalg</tt>, <tt class="docutils literal">random</tt>, and <tt class="docutils literal">fft</tt>
as <tt class="docutils literal">mdp.numx</tt>, <tt class="docutils literal">mdp.numx_linalg</tt>, <tt class="docutils literal">mdp.numx_rand</tt>, and
<tt class="docutils literal">mdp.numx_fft</tt>. This is done to possibly support additional
numerical extensions in the future. For this reason it is recommended
to refer to the <tt class="docutils literal">numpy</tt> or <tt class="docutils literal">scipy</tt> numerical extensions through
the MDP aliases <tt class="docutils literal">mdp.numx</tt>, <tt class="docutils literal">mdp.numx_linalg</tt>, <tt class="docutils literal">mdp.numx_fft</tt>,
and <tt class="docutils literal">mdp.numx_rand</tt> when writing <tt class="docutils literal">Node</tt> subclasses. This shall
ensure that your nodes can be used without modifications should MDP
support alternative numerical extensions in the future.</p>
<p>We'll illustrate all this with some toy examples.</p>
<ul>
<li><p class="first">We start by defining a node that multiplies its input by 2.</p>
<p>Define the class as a subclass of <tt class="docutils literal">Node</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; class TimesTwoNode(mdp.Node):
</pre>
<p>This node cannot be trained. To specify this, one has to overwrite
the <tt class="docutils literal">is_trainable</tt> method to return False:</p>
<pre class="literal-block">
...     def is_trainable(self):
...         return False
</pre>
<p>Execute only needs to multiply <tt class="docutils literal">x</tt> by 2:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return 2*x
</pre>
<p>Note that the <tt class="docutils literal">execute</tt> method, which should never be overwritten
and which is inherited from the <tt class="docutils literal">Node</tt> parent class, will perform
some tests, for example to make sure that <tt class="docutils literal">x</tt> has the right rank,
dimensionality and casts it to have the right <tt class="docutils literal">dtype</tt>.  After that
the user-supplied <tt class="docutils literal">_execute</tt> method is called.  Each subclass has
to handle the <tt class="docutils literal">dtype</tt> defined by the user or inherited by the
input data, and make sure that internal structures are stored
consistently. To help with this the <tt class="docutils literal">Node</tt> base class has a method
called <tt class="docutils literal">_refcast(array)</tt> that casts the input <tt class="docutils literal">array</tt> only when its
<tt class="docutils literal">dtype</tt> is different from the <tt class="docutils literal">Node</tt> instance's <tt class="docutils literal">dtype</tt>.</p>
<p>The inverse of the multiplication by 2 is of course the division by 2:</p>
<pre class="literal-block">
...     def _inverse(self, y):
...         return y/2
...
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = TimesTwoNode(dtype = 'int32')
&gt;&gt;&gt; x = mdp.numx.array([[1.0, 2.0, 3.0]])
&gt;&gt;&gt; y = node(x)
&gt;&gt;&gt; print x, '* 2 =  ', y
[ [ 1.  2.  3.]] * 2 =   [ [2 4 6]]
&gt;&gt;&gt; print y, '/ 2 =', node.inverse(y)
[ [2 4 6]] / 2 = [ [1 2 3]]
</pre>
</li>
<li><p class="first">We then define a node that raises the input to the power specified
in the initializer:</p>
<pre class="literal-block">
&gt;&gt;&gt; class PowerNode(mdp.Node):
</pre>
<p>We redefine the init method to take the power as first argument.
In general one should always give the possibility to set the <tt class="docutils literal">dtype</tt>
and the input dimensions. The default value is <tt class="docutils literal">None</tt>, which means that
the exact value is going to be inherited from the input data:</p>
<pre class="literal-block">
...     def __init__(self, power, input_dim=None, dtype=None):
</pre>
<p>Initialize the parent class:</p>
<pre class="literal-block">
...         super(PowerNode, self).__init__(input_dim=input_dim, dtype=dtype)
</pre>
<p>Store the power:</p>
<pre class="literal-block">
...         self.power = power
</pre>
<p><tt class="docutils literal">PowerNode</tt> is not trainable...</p>
<pre class="literal-block">
...     def is_trainable(self):
...         return False
</pre>
<p>... nor invertible:</p>
<pre class="literal-block">
...     def is_invertible(self):
...         return False
</pre>
<p>It is possible to overwrite the function <tt class="docutils literal">_get_supported_dtypes</tt>
to return a list of <tt class="docutils literal">dtype</tt> supported by the node:</p>
<pre class="literal-block">
...     def _get_supported_dtypes(self):
...         return ['float32', 'float64']
</pre>
<p>The supported types can be specified in any format allowed by the
<tt class="docutils literal">numpy.dtype</tt> constructor. The interface method <tt class="docutils literal">get_supported_dtypes</tt>
converts them and sets the property <tt class="docutils literal">supported_dtypes</tt>, which is
a list of <tt class="docutils literal">numpy.dtype</tt> objects.</p>
<p>The <tt class="docutils literal">_execute</tt> method:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return self._refcast(x**self.power)
...
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = PowerNode(3)
&gt;&gt;&gt; x = mdp.numx.array([[1.0, 2.0, 3.0]])
&gt;&gt;&gt; y = node(x)
&gt;&gt;&gt; print x, '**', node.power, '=', node(x)
[ [ 1.  2.  3.]] ** 3 = [ [  1.   8.  27.]]
</pre>
</li>
<li><p class="first">We now define a node that needs to be trained. The <tt class="docutils literal">MeanFreeNode</tt>
computes the mean of its training data and subtracts it from the input
during execution:</p>
<pre class="literal-block">
&gt;&gt;&gt; class MeanFreeNode(mdp.Node):
...     def __init__(self, input_dim=None, dtype=None):
...         super(MeanFreeNode, self).__init__(input_dim=input_dim,
...                                            dtype=dtype)
</pre>
<p>We store the mean of the input data in an attribute. We initialize it
to <tt class="docutils literal">None</tt> since we still don't know how large is an input vector:</p>
<pre class="literal-block">
...         self.avg = None
</pre>
<p>Same for the number of training points:</p>
<pre class="literal-block">
...         self.tlen = 0
</pre>
<p>The subclass only needs to overwrite the <tt class="docutils literal">_train</tt> method, which
will be called by the parent <tt class="docutils literal">train</tt> after some testing and casting has
been done:</p>
<pre class="literal-block">
...     def _train(self, x):
...         # Initialize the mean vector with the right
...         # size and dtype if necessary:
...         if self.avg is None:
...             self.avg = mdp.numx.zeros(self.input_dim,
...                                       dtype=self.dtype)
</pre>
<p>Update the mean with the sum of the new data:</p>
<pre class="literal-block">
...         self.avg += mdp.numx.sum(x, axis=0)
</pre>
<p>Count the number of points processed:</p>
<pre class="literal-block">
...         self.tlen += x.shape[0]
</pre>
<p>Note that the <tt class="docutils literal">train</tt> method can have further arguments, which might be
useful to implement algorithms that require supervised learning.
For example, if you want to define a node that performs some form
of classification you can define a <tt class="docutils literal">_train(self, data, labels)</tt>
method. The parent <tt class="docutils literal">train</tt> checks <tt class="docutils literal">data</tt> and takes care to pass
the <tt class="docutils literal">labels</tt> on (cf. for example <tt class="docutils literal">mdp.nodes.FDANode</tt>).</p>
<p>The <tt class="docutils literal">_stop_training</tt> function is called by the parent <tt class="docutils literal">stop_training</tt>
method when the training phase is over. We divide the sum of the training
data by the number of training vectors to obtain the mean:</p>
<pre class="literal-block">
...     def _stop_training(self):
...         self.avg /= self.tlen
...         if self.output_dim is None:
...             self.output_dim = self.input_dim
</pre>
<p>Note that we <tt class="docutils literal">input_dim</tt> are set automatically by the <tt class="docutils literal">train</tt> method,
and we want to ensure that the node has <tt class="docutils literal">output_dim</tt> set after training.
For nodes that do not need training, the setting is performed automatically
upon execution. The <tt class="docutils literal">_execute</tt> and <tt class="docutils literal">_inverse</tt> methods:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return x - self.avg
...     def _inverse(self, y):
...         return y + self.avg
...
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = MeanFreeNode()
&gt;&gt;&gt; x = mdp.numx_rand.random((10,4))
&gt;&gt;&gt; node.train(x)
&gt;&gt;&gt; y = node(x)
&gt;&gt;&gt; print 'Mean of y (should be zero): ', mdp.numx.mean(y, 0)
Mean of y (should be zero):  [  0.00000000e+00   2.22044605e-17
-2.22044605e-17   1.11022302e-17]
</pre>
</li>
<li><p class="first">It is also possible to define nodes with multiple training phases.
In such a case, calling the <tt class="docutils literal">train</tt> and <tt class="docutils literal">stop_training</tt> functions
multiple times is going to execute successive training phases
(this kind of node is much easier to train using <a class="reference internal" href="#flows">Flows</a>).
Here we'll define a node that returns a meanfree, unit variance signal.
We define two training phases: first we compute the mean of the
signal and next we sum the squared, meanfree input to compute
the standard deviation  (of course it is possible to solve this
problem in one single step - remember this is just a toy example).</p>
<pre class="literal-block">
&gt;&gt;&gt; class UnitVarianceNode(mdp.Node):
...     def __init__(self, input_dim=None, dtype=None):
...         super(UnitVarianceNode, self).__init__(input_dim=input_dim,
...                                                dtype=dtype)
...         self.avg = None # average
...         self.std = None # standard deviation
...         self.tlen = 0
</pre>
<p>The training sequence is defined by the user-supplied method
<tt class="docutils literal">_get_train_seq</tt>, that returns a list of tuples, one for each
training phase. The tuples contain references to the training
and stop-training methods of each of them. The default output
of this method is <tt class="docutils literal">[(_train, _stop_training)]</tt>, which explains
the standard behavior illustrated above. We overwrite the method to
return the list of our training/stop_training methods:</p>
<pre class="literal-block">
...     def _get_train_seq(self):
...         return [(self._train_mean, self._stop_mean),
...                 (self._train_std, self._stop_std)]
</pre>
<p>Next we define the training methods. The first phase is identical
to the one in the previous example:</p>
<pre class="literal-block">
...     def _train_mean(self, x):
...         if self.avg is None:
...             self.avg = mdp.numx.zeros(self.input_dim,
...                                       dtype=self.dtype)
...         self.avg += mdp.numx.sum(x, 0)
...         self.tlen += x.shape[0]
...     def _stop_mean(self):
...         self.avg /= self.tlen
</pre>
<p>The second one is only marginally different and does not require many
explanations:</p>
<pre class="literal-block">
...     def _train_std(self, x):
...         if self.std is None:
...             self.tlen = 0
...             self.std = mdp.numx.zeros(self.input_dim,
...                                       dtype=self.dtype)
...         self.std += mdp.numx.sum((x - self.avg)**2., 0)
...         self.tlen += x.shape[0]
...     def _stop_std(self):
...         # compute the standard deviation
...         self.std = mdp.numx.sqrt(self.std/(self.tlen-1))
</pre>
<p>The <tt class="docutils literal">_execute</tt> and <tt class="docutils literal">_inverse</tt> methods are not surprising, either:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return (x - self.avg)/self.std
...     def _inverse(self, y):
...         return y*self.std + self.avg
&gt;&gt;&gt;
</pre>
<p>Test the new node:</p>
<pre class="literal-block">
&gt;&gt;&gt; node = UnitVarianceNode()
&gt;&gt;&gt; x = mdp.numx_rand.random((10,4))
&gt;&gt;&gt; # loop over phases
... for phase in range(2):
...     node.train(x)
...     node.stop_training()
...
...
&gt;&gt;&gt; # execute
... y = node(x)
&gt;&gt;&gt; print 'Standard deviation of y (should be one): ', mdp.numx.std(y, axis=0)
Standard deviation of y (should be one):  [ 1.  1.  1.  1.]
</pre>
</li>
<li><p class="first">In our last example we'll define a node that returns two copies of its input.
The output is going to have twice as many dimensions.</p>
<pre class="literal-block">
&gt;&gt;&gt; class TwiceNode(mdp.Node):
...     def is_trainable(self): return False
...     def is_invertible(self): return False
</pre>
<p>When <tt class="docutils literal">Node</tt> inherits the input dimension, output dimension, and <tt class="docutils literal">dtype</tt>
from the input data, it calls the methods <tt class="docutils literal">set_input_dim</tt>,
<tt class="docutils literal">set_output_dim</tt>, and <tt class="docutils literal">set_dtype</tt>. Those are the setters for
<tt class="docutils literal">input_dim</tt>, <tt class="docutils literal">output_dim</tt> and <tt class="docutils literal">dtype</tt>, which are Python
<a class="reference external" href="http://www.python.org/2.2/descrintro.html">properties</a>.
If a subclass needs to change the default behavior, the internal methods
<tt class="docutils literal">_set_input_dim</tt>, <tt class="docutils literal">_set_output_dim</tt> and <tt class="docutils literal">_set_dtype</tt> can
be overwritten. The property setter will call the internal method after
some basic testing and internal settings. The private methods
<tt class="docutils literal">_set_input_dim</tt>, <tt class="docutils literal">_set_output_dim</tt> and <tt class="docutils literal">_set_dtype</tt> are responsible
for setting the private attributes <tt class="docutils literal">_input_dim</tt>, <tt class="docutils literal">_output_dim</tt>,
and <tt class="docutils literal">_dtype</tt> that contain the actual value.</p>
<p>Here we overwrite
<tt class="docutils literal">_set_input_dim</tt> to automatically set the output dimension to be twice the
input one, and <tt class="docutils literal">_set_output_dim</tt> to raise an exception, since
the output dimension should not be set explicitly.</p>
<pre class="literal-block">
...     def _set_input_dim(self, n):
...         self._input_dim = n
...         self._output_dim = 2*n
...     def _set_output_dim(self, n):
...         raise mdp.NodeException, &quot;Output dim can not be set explicitly!&quot;
</pre>
<p>The <tt class="docutils literal">_execute</tt> method:</p>
<pre class="literal-block">
...     def _execute(self, x):
...         return mdp.numx.concatenate((x, x), 1)
...
&gt;&gt;&gt;
</pre>
<p>Test the new node</p>
<pre class="literal-block">
&gt;&gt;&gt; node = TwiceNode()
&gt;&gt;&gt; x = mdp.numx.zeros((5,2))
&gt;&gt;&gt; x
array([[0, 0],
       [0, 0],
       [0, 0],
       [0, 0],
       [0, 0]])
&gt;&gt;&gt; node.execute(x)
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]])
</pre>
</li>
</ul>
</div>
</div>
<div class="section" id="flows">
<h1><a class="toc-backref" href="#id12">Flows</a></h1>
<p>A <em>flow</em> is a sequence of nodes that are trained and executed
together to form a more complex algorithm.  Input data is sent to the
first node and is successively processed by the subsequent nodes along
the sequence.</p>
<p>Using a flow as opposed to handling manually a set of nodes has a
clear advantage: The general flow implementation automatizes the
training (including supervised training and multiple training phases),
execution, and inverse execution (if defined) of the whole sequence.</p>
<p>Crash recovery is optionally available: in case of failure the current
state of the flow is saved for later inspection. A subclass of the
basic flow class (<tt class="docutils literal">CheckpointFlow</tt>) allows user-supplied checkpoint
functions to be executed at the end of each phase, for example to save
the internal structures of a node for later analysis.
Flow objects are Python containers. Most of the builtin <tt class="docutils literal">list</tt>
methods are available. A <tt class="docutils literal">Flow</tt> can be saved or copied using the
corresponding <tt class="docutils literal">save</tt> and <tt class="docutils literal">copy</tt> methods.</p>
<div class="section" id="flow-instantiation-training-and-execution">
<h2><a class="toc-backref" href="#id13">Flow instantiation, training and execution</a></h2>
<p>For example, suppose we need to analyze a very
high-dimensional input signal using Independent Component Analysis
(ICA). To reduce the computational load, we would like to reduce the
input dimensionality of the data using PCA. Moreover, we would like to
find the data that produces local maxima in the output of the ICA
components on a new test set (this information could be used
for instance to characterize the ICA filters).</p>
<p>We start by generating some input signal at random (which makes the
example useless, but it's just for illustration...).  Generate 1000
observations of 20 independent source signals:</p>
<pre class="literal-block">
&gt;&gt;&gt; inp = mdp.numx_rand.random((1000, 20))
</pre>
<p>Rescale x to have zero mean and unit variance:</p>
<pre class="literal-block">
&gt;&gt;&gt; inp = (inp - mdp.numx.mean(inp, 0))/mdp.numx.std(inp, 0)
</pre>
<p>We reduce the variance of the last 15 components, so that they are
going to be eliminated by PCA:</p>
<pre class="literal-block">
&gt;&gt;&gt; inp[:,5:] /= 10.0
</pre>
<p>Mix the input signals linearly:</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.utils.mult(inp,mdp.numx_rand.random((20, 20)))
</pre>
<p><tt class="docutils literal">x</tt> is now the training data for our simulation. In the same way
we also create a test set <tt class="docutils literal">x_test</tt>.</p>
<pre class="literal-block">
&gt;&gt;&gt; inp_test = mdp.numx_rand.random((1000, 20))
&gt;&gt;&gt; inp_test = (inp_test - mdp.numx.mean(inp_test, 0))/mdp.numx.std(inp_test, 0)
&gt;&gt;&gt; inp_test[:,5:] /= 10.0
&gt;&gt;&gt; x_test = mdp.utils.mult(inp_test, mdp.numx_rand.random((20, 20)))
</pre>
<ul>
<li><p class="first">We could now perform our analysis using only nodes, that's the
lengthy way...</p>
<p>1. Perform PCA:</p>
<pre class="literal-block">
&gt;&gt;&gt; pca = mdp.nodes.PCANode(output_dim=5)
&gt;&gt;&gt; pca.train(x)
&gt;&gt;&gt; out1 = pca(x)
</pre>
<p>2. Perform ICA using CuBICA algorithm:</p>
<pre class="literal-block">
&gt;&gt;&gt; ica = mdp.nodes.CuBICANode()
&gt;&gt;&gt; ica.train(out1)
&gt;&gt;&gt; out2 = ica(out1)
</pre>
<p>3. Find the three largest local maxima in the output of the ICA node
when applied to the test data, using a <tt class="docutils literal">HitParadeNode</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; out1_test = pca(x_test)
&gt;&gt;&gt; out2_test = ica(out1_test)
&gt;&gt;&gt; hitnode = mdp.nodes.HitParadeNode(3)
&gt;&gt;&gt; hitnode.train(out2_test)
&gt;&gt;&gt; maxima, indices = hitnode.get_maxima()
</pre>
</li>
<li><p class="first">... or we could use flows, which is the best way:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([mdp.nodes.PCANode(output_dim=5), mdp.nodes.CuBICANode()])
</pre>
<p>Note that flows can be built simply by concatenating nodes:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.nodes.PCANode(output_dim=5) + mdp.nodes.CuBICANode()
</pre>
<p>Train the resulting flow:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.train(x)
</pre>
<p>Now the training phase of PCA and ICA are completed. Next we append
a <tt class="docutils literal">HitParadeNode</tt> which we want to train on the test data:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.append(mdp.nodes.HitParadeNode(3))
</pre>
<p>As before, new nodes can be appended to an existing flow by adding
them ot it:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow += mdp.nodes.HitParadeNode(3)
</pre>
<p>Train the <tt class="docutils literal">HitParadeNode</tt> on the test data:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.train(x_test)
&gt;&gt;&gt; maxima, indices = flow[2].get_maxima()
</pre>
<p>A single call to the <tt class="docutils literal">flow</tt>'s <tt class="docutils literal">train</tt> method will automatically
take care of training nodes with multiple training phases, if such
nodes are present.</p>
<p>Just to check that everything works properly, we
can calculate covariance between the generated sources and the output
(should be approximately 1):</p>
<pre class="literal-block">
&gt;&gt;&gt; out = flow.execute(x)
&gt;&gt;&gt; cov = mdp.numx.amax(abs(mdp.utils.cov2(inp[:,:5], out)), axis=1)
&gt;&gt;&gt; print cov
[ 0.98992083  0.99244511  0.99227319  0.99663185  0.9871812 ]
</pre>
<p>The <tt class="docutils literal">HitParadeNode</tt> is an analysis node and as such does not
interfere with the data flow.</p>
<p>Note that flows can be executed by calling the <tt class="docutils literal">Flow</tt> instance
directly:</p>
<pre class="literal-block">
&gt;&gt;&gt; out = flow(x)
</pre>
</li>
</ul>
</div>
<div class="section" id="flow-inversion">
<h2><a class="toc-backref" href="#id14">Flow inversion</a></h2>
<p>Flows can be inverted by calling their <tt class="docutils literal">inverse</tt> method.
In the case where the flow contains non-invertible nodes,
trying to invert it would raise an exception.
In this case, however, all nodes are invertible.
We can reconstruct the mix by inverting the flow:</p>
<pre class="literal-block">
&gt;&gt;&gt; rec = flow.inverse(out)
</pre>
<p>Calculate covariance between input mix and reconstructed mix:
(should be approximately 1)</p>
<pre class="literal-block">
&gt;&gt;&gt; cov = mdp.numx.amax(abs(mdp.utils.cov2(x/mdp.numx.std(x,axis=0),
...                                        rec/mdp.numx.std(rec,axis=0))))
&gt;&gt;&gt; print cov
[ 0.99839606  0.99744461  0.99616208  0.99772863  0.99690947
  0.99864056  0.99734378  0.98722502  0.98118101  0.99407939
  0.99683096  0.99756988  0.99664384  0.99723419  0.9985529
  0.99829763  0.9982712   0.99721741  0.99682906  0.98858858]
</pre>
</div>
<div class="section" id="flows-are-container-type-objects">
<h2><a class="toc-backref" href="#id15">Flows are container type objects</a></h2>
<p><tt class="docutils literal">Flow</tt> objects are defined as Python containers, and thus are endowed with
most of the methods of Python lists.</p>
<p>You can loop through a <tt class="docutils literal">Flow</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; for node in flow:
...     print repr(node)
...
PCANode(input_dim=20, output_dim=5, dtype='float64')
CuBICANode(input_dim=5, output_dim=5, dtype='float64')
HitParadeNode(input_dim=5, output_dim=5, dtype='float64')
HitParadeNode(input_dim=5, output_dim=5, dtype='float64')
&gt;&gt;&gt;
</pre>
<p>You can get slices, <tt class="docutils literal">pop</tt>, <tt class="docutils literal">insert</tt>, and <tt class="docutils literal">append</tt> nodes:</p>
<pre class="literal-block">
&gt;&gt;&gt; len(flow)
4
&gt;&gt;&gt; print flow[::2]
[PCANode, HitParadeNode]
&gt;&gt;&gt; nodetoberemoved = flow.pop(-1)
&gt;&gt;&gt; nodetoberemoved
HitParadeNode(input_dim=5, output_dim=5, dtype='float64')
&gt;&gt;&gt; len(flow)
3
</pre>
<p>Finally, you can concatenate flows:</p>
<pre class="literal-block">
&gt;&gt;&gt; dummyflow = flow[1:].copy()
&gt;&gt;&gt; longflow = flow + dummyflow
&gt;&gt;&gt; len(longflow)
4
</pre>
<p>The returned flow must always be consistent, i.e. input and
output dimensions of successive nodes always have to match. If
you try to create an inconsistent flow you'll get an exception.</p>
</div>
<div class="section" id="crash-recovery">
<h2><a class="toc-backref" href="#id16">Crash recovery</a></h2>
<p>If a node in a flow fails, you'll get a traceback that tells you which
node has failed. You can also switch the crash recovery capability on. If
something goes wrong you'll end up with a pickle dump of the flow, that
can be later inspected.</p>
<p>To see how it works let's define a bogus node that always throws an
<tt class="docutils literal">Exception</tt> and put it into a flow:</p>
<pre class="literal-block">
&gt;&gt;&gt; class BogusExceptNode(mdp.Node):
...    def train(self,x):
...        self.bogus_attr = 1
...        raise Exception, &quot;Bogus Exception&quot;
...    def execute(self,x):
...        raise Exception, &quot;Bogus Exception&quot;
...
&gt;&gt;&gt; flow = mdp.Flow([BogusExceptNode()])
</pre>
<p>Switch on crash recovery:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.set_crash_recovery(1)
</pre>
<p>Attempt to train the flow:</p>
<blockquote>
<!-- ignore --></blockquote>
<pre class="literal-block">
&gt;&gt;&gt; flow.train(x)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in ?
  [...]
mdp.linear_flows.FlowExceptionCR:
----------------------------------------
! Exception in node #0 (BogusExceptNode):
Node Traceback:
Traceback (most recent call last):
  [...]
Exception: Bogus Exception
----------------------------------------
A crash dump is available on: &quot;/tmp/MDPcrash_LmISO_.pic&quot;
</pre>
<p>You can give a file name to tell the flow where to save the dump:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.set_crash_recovery('/home/myself/mydumps/MDPdump.pic')
</pre>
</div>
</div>
<div class="section" id="iterables">
<h1><a class="toc-backref" href="#id17">Iterables</a></h1>
<p>Python allows user-defined classes to support iteration,
as described in the <a class="reference external" href="http://docs.python.org/library/stdtypes.html#iterator-types">Python docs</a>. A class is a
so called iterable if it defines a method <tt class="docutils literal">__iter__</tt> that returns an
iterator instance. An iterable is typically some kind of container or
collection (e.g. <tt class="docutils literal">list</tt> and <tt class="docutils literal">tuple</tt> are iterables).</p>
<p>The iterator instance must have a <tt class="docutils literal">next</tt> method that returns the next
element in the iteration. In Python an iterable also has to have an
<tt class="docutils literal">__iter__</tt> method itself that returns <tt class="docutils literal">self</tt> instead of a new iterator.
It is important to understand that an iterator only manages a single iteration.
After this iteration it is spend and cannot be used for a second iteration
(it cannot be restarted). An iterable on the other hand can create as many
iterators as needed and therefore supports multiple iterations. Even though
both iterables and iterators have an <tt class="docutils literal">__iter__</tt> method they are
semantically very different (duck-typing can be misleading in this case).</p>
<p>In the context of MDP this means that an iterator can only be used for a
single training phase, while iterables also support multiple training phases.
So if you use a node with multiple training phases and train it in a flow
make sure that you provide an iterable for this node (otherwise an exception
will be raised). For nodes with a single training phase you can use
either an iterable or an iterator.</p>
<p>A convenient implementation of the iterator protocol is provided
by generators:
see <a class="reference external" href="http://linuxgazette.net/100/pramode.html">this article</a> for an
introduction, and the
<a class="reference external" href="http://www.python.org/peps/pep-0255.html">official PEP</a> for a
complete description.</p>
<p>Let us define two bogus node classes to be used as examples of nodes:</p>
<pre class="literal-block">
&gt;&gt;&gt; class BogusNode(mdp.Node):
...     &quot;&quot;&quot;This node does nothing.&quot;&quot;&quot;
...     def _train(self, x):
...         pass
...
&gt;&gt;&gt; class BogusNode2(mdp.Node):
...     &quot;&quot;&quot;This node does nothing. But it's not trainable nor invertible.
...     &quot;&quot;&quot;
...     def is_trainable(self): return False
...     def is_invertible(self): return False
...
&gt;&gt;&gt;
</pre>
<p>This generator generates <tt class="docutils literal">blocks</tt> input blocks to be used as training set.
In this example one block is a 2-dimensional time series. The first variable
is [2,4,6,....,1000] and the second one [0,1,3,5,...,999].
All blocks are equal, this of course would not be the case in a real-life
example.</p>
<p>In this example we use a progress bar to get progress information.</p>
<pre class="literal-block">
&gt;&gt;&gt; def gen_data(blocks):
...     for i in mdp.utils.progressinfo(xrange(blocks)):
...         block_x = mdp.numx.atleast_2d(mdp.numx.arange(2,1001,2))
...         block_y = mdp.numx.atleast_2d(mdp.numx.arange(1,1001,2))
...         # put variables on columns and observations on rows
...         block = mdp.numx.transpose(mdp.numx.concatenate([block_x,block_y]))
...         yield block
...
&gt;&gt;&gt;
</pre>
<p>The <tt class="docutils literal">progressinfo</tt> function is a fully configurable text-mode
progress info box tailored to the command-line die-hards. Have a look
at its doc-string and prepare to be amazed!</p>
<p>Let's define a bogus flow consisting of 2 <tt class="docutils literal">BogusNode</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode(),BogusNode()], verbose=1)
</pre>
<p>Train the first node with 5000 blocks and the second node with 3000 blocks.
Note that the only allowed argument to <tt class="docutils literal">train</tt> is a sequence (list or
tuple) of iterables or iterators. In case you don't want or need to use
incremental learning and want to do a one-shot training, you can use as
argument to <tt class="docutils literal">train</tt> a single array of data:</p>
<p><strong>block-mode training</strong></p>
<blockquote>
<pre class="literal-block">
&gt;&gt;&gt; flow.train([gen_data(5000),gen_data(3000)])
Training node #0 (BogusNode)
[===================================100%==================================&gt;]

Training finished
Training node #1 (BogusNode)
[===================================100%==================================&gt;]

Training finished
Close the training phase of the last node
</pre>
</blockquote>
<p><strong>one-shot training</strong> using one single set of data for both nodes</p>
<blockquote>
<pre class="literal-block">
&gt;&gt;&gt; flow = BogusNode() + BogusNode()
&gt;&gt;&gt; block_x = mdp.numx.atleast_2d(mdp.numx.arange(2,1001,2))
&gt;&gt;&gt; block_y = mdp.numx.atleast_2d(mdp.numx.arange(1,1001,2))
&gt;&gt;&gt; single_block = mdp.numx.transpose(mdp.numx.concatenate([block_x,block_y]))
&gt;&gt;&gt; flow.train(single_block)
</pre>
</blockquote>
<p>If your flow contains non-trainable nodes, you must specify a <tt class="docutils literal">None</tt>
for the non-trainable nodes:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode2(),BogusNode()], verbose=1)
&gt;&gt;&gt; flow.train([None, gen_data(5000)])
Training node #0 (BogusNode2)
Training finished
Training node #1 (BogusNode)
[===================================100%==================================&gt;]

Training finished
Close the training phase of the last node
</pre>
<p>You can use the one-shot training:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode2(),BogusNode()], verbose=1)
&gt;&gt;&gt; flow.train(single_block)
Training node #0 (BogusNode2)
Training finished
Training node #1 (BogusNode)
Training finished
Close the training phase of the last node
</pre>
<p>Iterators can always be safely used for execution and inversion, since only a
single iteration is needed:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.Flow([BogusNode(),BogusNode()], verbose=1)
&gt;&gt;&gt; flow.train([gen_data(1), gen_data(1)])
Training node #0 (BogusNode)
Training finished
Training node #1 (BosgusNode)
[===================================100%==================================&gt;]

Training finished
Close the training phase of the last node
&gt;&gt;&gt; output = flow.execute(gen_data(1000))
[===================================100%==================================&gt;]
&gt;&gt;&gt; output = flow.inverse(gen_data(1000))
[===================================100%==================================&gt;]
</pre>
<p>Execution and inversion can be done in one-shot mode also. Note that
since training is finished you are not going to get a warning</p>
<pre class="literal-block">
&gt;&gt;&gt; output = flow(single_block)
&gt;&gt;&gt; output = flow.inverse(single_block)
</pre>
<p>If a node requires multiple training phases (e.g.,
<tt class="docutils literal">GaussianClassifierNode</tt>), <tt class="docutils literal">Flow</tt> automatically takes care of using the
iterable multiple times. In this case generators (and iterators) are not
allowed, since they are spend after yielding the last data block.</p>
<p>However, it is fairly easy to wrap a generator in a simple iterable if you need to:</p>
<pre class="literal-block">
&gt;&gt;&gt; class SimpleIterable(object):
...     def __init__(self, blocks):
...         self.blocks = blocks
...     def __iter__(self):
...         # this is a generator
...         for i in range(self.blocks):
...             yield generate_some_data()
&gt;&gt;&gt;
</pre>
<p>Note that if you use random numbers within the generator, you usually
would like to reset the random number generator to produce the
same sequence every time:</p>
<pre class="literal-block">
&gt;&gt;&gt; class RandomIterable(object):
...     def __init__(self):
...         self.state = None
...     def __iter__(self):
...         if self.state is None:
...             self.state = mdp.numx_rand.get_state()
...         else:
...             mdp.numx_rand.set_state(self.state)
...         for i in range(2):
...             yield mdp.numx_rand.random((1,4))
&gt;&gt;&gt; iterable = RandomIterable()
&gt;&gt;&gt; for x in iterable:
...     print x
...
[[ 0.99586495  0.53463386  0.6306412   0.09679571]]
[[ 0.51117469  0.46647448  0.95089738  0.94837122]]
&gt;&gt;&gt; for x in iterable:
...     print x
...
[[ 0.99586495  0.53463386  0.6306412   0.09679571]]
[[ 0.51117469  0.46647448  0.95089738  0.94837122]]
</pre>
</div>
<div class="section" id="checkpoints">
<h1><a class="toc-backref" href="#id18">Checkpoints</a></h1>
<p>It can sometimes be useful to execute arbitrary functions at the end
of the training or execution phase, for example to save the internal
structures of a node for later analysis. This can easily be done
by defining a <tt class="docutils literal">CheckpointFlow</tt>. As an example imagine the following
situation: you want to perform Principal Component Analysis (PCA) on
your data to reduce the dimensionality. After this you want to expand
the signals into a nonlinear space and then perform Slow Feature
Analysis to extract slowly varying signals. As the expansion will increase
the number of components, you don't want to run out of memory, but at the same
time you want to keep as much information as possible after the dimensionality
reduction. You could do that by specifying the percentage of
the total input variance that has to be conserved in the dimensionality
reduction. As the number of output components of the PCA node now can become
as large as the that of the input components, you want to check, after training the
PCA node, that this number is below a certain threshold. If this is not
the case you want to abort the execution and maybe start again requesting
less variance to be kept.</p>
<p>Let start defining a generator to be used through the whole example:</p>
<pre class="literal-block">
&gt;&gt;&gt; def gen_data(blocks,dims):
...     mat = mdp.numx_rand.random((dims,dims))-0.5
...     for i in xrange(blocks):
...         # put variables on columns and observations on rows
...         block = mdp.utils.mult(mdp.numx_rand.random((1000,dims)), mat)
...         yield block
...
&gt;&gt;&gt;
</pre>
<p>Define a <tt class="docutils literal">PCANode</tt> which reduces dimensionality of the input,
a <tt class="docutils literal">PolynomialExpansionNode</tt> to expand the signals in the space
of polynomials of degree 2 and a <tt class="docutils literal">SFANode</tt> to perform SFA:</p>
<pre class="literal-block">
&gt;&gt;&gt; pca = mdp.nodes.PCANode(output_dim=0.9)
&gt;&gt;&gt; exp = mdp.nodes.PolynomialExpansionNode(2)
&gt;&gt;&gt; sfa = mdp.nodes.SFANode()
</pre>
<p>As you see we have set the output dimension of the <tt class="docutils literal">PCANode</tt> to be <tt class="docutils literal">0.9</tt>.
This means that we want to keep at least 90% of the variance of the original signal.
We define a <tt class="docutils literal">PCADimensionExceededException</tt> that has to be thrown when
the number of output components exceeds a certain threshold:</p>
<pre class="literal-block">
&gt;&gt;&gt; class PCADimensionExceededException(Exception):
...     &quot;&quot;&quot;Exception base class for PCA exceeded dimensions case.&quot;&quot;&quot;
...     pass
...
&gt;&gt;&gt;
</pre>
<p>Then, write a <tt class="docutils literal">CheckpointFunction</tt> that checks the number of output
dimensions of the <tt class="docutils literal">PCANode</tt> and aborts if this number is larger than <tt class="docutils literal">max_dim</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; class CheckPCA(mdp.CheckpointFunction):
...     def __init__(self,max_dim):
...         self.max_dim = max_dim
...     def __call__(self,node):
...         node.stop_training()
...         act_dim = node.get_output_dim()
...         if act_dim &gt; self.max_dim:
...             errstr = 'PCA output dimensions exceeded maximum '+\
...                      '(%d &gt; %d)'%(act_dim,self.max_dim)
...             raise PCADimensionExceededException, errstr
...         else:
...             print 'PCA output dimensions = %d'%(act_dim)
...
&gt;&gt;&gt;
</pre>
<p>Define the CheckpointFlow:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = mdp.CheckpointFlow([pca, exp, sfa])
</pre>
<p>To train it we have to supply 3 generators and 3 checkpoint functions:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; flow.train([gen_data(10, 50), None, gen_data(10, 50)],
...            [CheckPCA(10), None, None])
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 2, in ?
  [...]
__main__.PCADimensionExceededException: PCA output dimensions exceeded maximum (25 &gt; 10)
</pre>
<p>The training fails with a <tt class="docutils literal">PCADimensionExceededException</tt>.
If we only had 12 input dimensions instead of 50 we would have passed
the checkpoint:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow[0] = mdp.nodes.PCANode(output_dim=0.9)
&gt;&gt;&gt; flow.train([gen_data(10, 12), None, gen_data(10, 12)],
...            [CheckPCA(10), None, None])
PCA output dimensions = 6
</pre>
<p>We could use the built-in <tt class="docutils literal">CheckpoinSaveFunction</tt> to save the <tt class="docutils literal">SFANode</tt>
and analyze the results later :</p>
<pre class="literal-block">
&gt;&gt;&gt; pca = mdp.nodes.PCANode(output_dim=0.9)
&gt;&gt;&gt; exp = mdp.nodes.PolynomialExpansionNode(2)
&gt;&gt;&gt; sfa = mdp.nodes.SFANode()
&gt;&gt;&gt; flow = mdp.CheckpointFlow([pca, exp, sfa])
&gt;&gt;&gt; flow.train([gen_data(10, 12), None, gen_data(10, 12)],
...            [CheckPCA(10),
...             None,
...             mdp.CheckpointSaveFunction('dummy.pic',
...                                        stop_training = 1,
...                                        protocol = 0)])
...
PCA output dimensions = 7
</pre>
<p>We can now reload and analyze the <tt class="docutils literal">SFANode</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; fl = file('dummy.pic')
&gt;&gt;&gt; import cPickle
&gt;&gt;&gt; sfa_reloaded = cPickle.load(fl)
&gt;&gt;&gt; sfa_reloaded
SFANode(input_dim=35, output_dim=35, dtype='d')
</pre>
<p>Don't forget to clean the rubbish:</p>
<pre class="literal-block">
&gt;&gt;&gt; fl.close()
&gt;&gt;&gt; import os
&gt;&gt;&gt; os.remove('dummy.pic')
</pre>
</div>
<div class="section" id="node-extensions">
<h1><a class="toc-backref" href="#id19">Node Extensions</a></h1>
<p>First note that dealing with the node extension mechanism should be considered
advanced usage, so you can skip this section.</p>
<p>The node extension mechanism makes it possible to dynamically add methods or
class attributes for specific features to node classes (e.g. for
parallelization the nodes need a <tt class="docutils literal">_fork</tt> and <tt class="docutils literal">_join</tt> method). Note that
methods are just a special case of class attributes, the extension mechanism
treats them like any other class attributes.
It is also possible for users to define new extensions
to introduce new functionality for MDP nodes without having to directly modify
any MDP code. The node extension mechanism basically enables some
form of <em>Aspect-oriented programming</em> (AOP) to deal with <em>cross-cutting
concerns</em> (i.e., you want to add a new aspect to node classes which are
spread all over MDP and possibly your own code). In the AOP terminology any
new methods you introduce contain <em>advice</em> and the <em>pointcut</em> is effectively
defined by the calling of these methods.</p>
<p>Without the extension mechanism the adding of new aspects to nodes would
be done through inheritance, deriving new node classes that implement
the aspect for the parent node class. This is fine unless one wants to use
multiple aspects, requiring multiple inheritance for every combination of
aspects one wants to use. Therefore this approach does not scale well with
the number of aspects.</p>
<p>The node extension mechanism does not directly depend on inheritance,
instead it adds the methods or class attributes to the node classes
dynamically at runtime (like <em>method injection</em>). This makes it possible
to activate extensions just when they are needed, reducing the risk of
interference between different extensions. One can also use multiple
extensions at the same time, as long as there is no interference, i.e.,
both extensions do not use any attributes with the same name.</p>
<p>The node extension mechanism uses a special Metaclass, which allows it to
define the node extensions as classes derived from nodes (bascially just what
one would do without the extension mechanism).
This keeps the code readable and avoids some problems when using automatic
code checkers (like the background pylint checks in the
Eclipse IDE with PyDev).</p>
<p>In MDP the node extension mechanism is currently used by the <tt class="docutils literal">parallel</tt>
package and for the the HTML representation in the <tt class="docutils literal">hinet</tt> package,
so the best way to learn more is to look there.
We also use these packages in the following examples.</p>
<div class="section" id="using-extensions">
<h2><a class="toc-backref" href="#id20">Using Extensions</a></h2>
<p>First of all you can get all the available node extensions by calling
the <tt class="docutils literal">get_extensions</tt> function, or to get just a list of their names use
<tt class="docutils literal"><span class="pre">get_extensions().keys()</span></tt>. Be careful not to modify the dict returned
by <tt class="docutils literal">get_extensions</tt>, since this will actually modify the registered
extensions. The currently activated extensions are returned
by <tt class="docutils literal">get_active_extensions</tt>. To activate an extension use
<tt class="docutils literal">activate_extension</tt>, e.g. to activate the parallel extension
use <tt class="docutils literal"><span class="pre">mdp.activate_extension(&quot;parallel&quot;)</span></tt>. Alternatively you can
use the function decorator <tt class="docutils literal"><span class="pre">&#64;with_extension(&quot;parallel&quot;)</span></tt>. In the future
we will also support the new <tt class="docutils literal">with</tt> statement in Python. Activating an
extension adds the available extensions attributes to the supported nodes.
An extension can be deactivated with <tt class="docutils literal">deactivate_extension</tt> (if you use the
function decorator this is done automatically at the end).</p>
</div>
<div class="section" id="writing-extension-nodes">
<h2><a class="toc-backref" href="#id21">Writing Extension Nodes</a></h2>
<p>Suppose you have written your own nodes and would like to make them compatible
with a particular extension (e.g. add the required methods).
The first way to do this is by using multiple inheritance to derive from
the base class of this extension and your custom node class. For example
the parallel extension of the SFA node is defined in a class
<tt class="docutils literal">ParallelSFANode(ParallelExtensionNode, mdp.nodes.SFANode)</tt>. Here
<tt class="docutils literal">ParallelExtensionNode</tt> is the base class of the extension. Then you define
the required methods or attributes just like in a normal class.
If you want you could even use the new <tt class="docutils literal">ParallelSFANode</tt> class like a
normal class, ignoring the extension mechanism.
Note that your extension node is automatically registered in the
extension mechanism (through a little metaclass magic).</p>
<p>For methods you can alternatively use the <tt class="docutils literal">extension_method</tt> function
decorator. You define the extension method like a normal function, but add
the function decorator on top, for example:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; &#64;mdp.extension_method(&quot;html&quot;, mdp.hinet.Rectangular2dSwitchboard)
... def _html_representation(self):
...     pass
...
&gt;&gt;&gt;
</pre>
<p>The first decorator argument is the name of the extension, the second is the
class you want to extend. You can also specify the method name as a third
argument, then the name of the function is ignored (this allows you to get
rid of warnings about multiple functions with the same name).</p>
</div>
<div class="section" id="creating-extensions">
<h2><a class="toc-backref" href="#id22">Creating Extensions</a></h2>
<p>To create a new node extension you just have to create a new extension base
class. For example the HTML representation extension in <tt class="docutils literal">mdp.hinet</tt>
is created with</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; class  HTMLExtensionNode(mdp.ExtensionNode, mdp.Node):
...     &quot;&quot;&quot;Extension node for HTML representations of individual nodes.&quot;&quot;&quot;
...     extension_name = &quot;html&quot;
...     def html_representation(self):
...         pass
...     def _html_representation(self):
...         pass
...
&gt;&gt;&gt;
</pre>
<p>Note that you must derive from <tt class="docutils literal">ExtensionNode</tt>. If you also derive from
<tt class="docutils literal">mdp.Node</tt> then the methods (and attributes) in this class are the default implementation for the <tt class="docutils literal">mdp.Node</tt> class. So they will be used
by all nodes without a more specific implementation. If you do not derive from
<tt class="docutils literal">mdp.Node</tt> then there is no such default implementation. You can also derive
from a more specific node class if your extension only applies to these
specific nodes.</p>
<p>When you define a new extension then you must define the <tt class="docutils literal">extension_name</tt>
attribute. This magic attribute is used to register the new extension and you
can activate or deactivate the extension by using this name.</p>
<p>Note that extensions can override attributes and methods that are
defined in a node class. The original attributes can still be accessed
by prefixing the name with <tt class="docutils literal">_non_extension_</tt> (the prefix string is
also available as <tt class="docutils literal">mdp.ORIGINAL_ATTR_PREFIX</tt>). On the other hand one
extension is not allowed to override attributes that were defined by
another currently active extension.</p>
<p>The extension mechanism uses some
magic to make the behavior more intuitive with respect to inheritance.
Basically methods or attributes defined by extensions shadow those which
are not defined in the extension. Here is an example:</p>
<pre class="literal-block">
&gt;&gt;&gt; class TestExtensionNode(mdp.ExtensionNode):
...     extension_name = &quot;test&quot;
...     def _execute(self):
...         return 0
...
&gt;&gt;&gt; class TestNode(mdp.Node):
...     def _execute(self):
...         return 1
...
&gt;&gt;&gt; class ExtendedTestNode(TestExtensionNode, TestNode):
...     pass
...
&gt;&gt;&gt;
</pre>
<p>After this extension is activated any calls of <tt class="docutils literal">_execute</tt> in instances
of <tt class="docutils literal">TestNode</tt> will return 0 instead of 1. The <tt class="docutils literal">_execute</tt> from the
extension base-class shadows the method from <tt class="docutils literal">TestNode</tt>. This makes it
easier to share behavior for different classes. Without this magic one
would have to explicitly override <tt class="docutils literal">_execute</tt> in <tt class="docutils literal">ExtendedTestNode</tt>
(or derive the extension base-class from <tt class="docutils literal">Node</tt>, but that would give
this behavior to all node classes). Note that there is a <tt class="docutils literal">verbose</tt>
argument in <tt class="docutils literal">activate_extension</tt> which can help with debugging.</p>
</div>
</div>
<div class="section" id="hierarchical-networks">
<h1><a class="toc-backref" href="#id23">Hierarchical Networks</a></h1>
<p>In case the desired data processing application cannot be defined as a
sequence of nodes, the <tt class="docutils literal">hinet</tt> subpackage makes it possible to
construct arbitrary feed-forward architectures, and in particular
hierarchical networks.</p>
<div class="section" id="building-blocks">
<h2><a class="toc-backref" href="#id24">Building blocks</a></h2>
<p>The <tt class="docutils literal">hinet</tt> package contains three basic building blocks (which are all nodes
themselves) to construct hierarchical node networks: <tt class="docutils literal">Layer</tt>,
<tt class="docutils literal">FlowNode</tt>, <tt class="docutils literal">Switchboard</tt>.</p>
<ul>
<li><p class="first">The first building block is the <tt class="docutils literal">Layer</tt> node, which works like a
horizontal version of flow. It acts as a wrapper for a set of nodes
that are trained and executed in parallel. For example, we can
combine two nodes with 100 dimensional input to construct a layer
with a 200-dimensional input</p>
<pre class="literal-block">
&gt;&gt;&gt; node1 = mdp.nodes.PCANode(input_dim=100, output_dim=10)
&gt;&gt;&gt; node2 = mdp.nodes.SFANode(input_dim=100, output_dim=20)
&gt;&gt;&gt; layer = mdp.hinet.Layer([node1, node2])
&gt;&gt;&gt; layer
Layer(input_dim=200, output_dim=30, dtype=None)
</pre>
<p>The first half of the 200 dimensional input data is then
automatically assigned to <tt class="docutils literal">node1</tt> and the second half to
<tt class="docutils literal">node2</tt>. We can train and execute a <tt class="docutils literal">Layer</tt> just like any other
node. Note that the dimensions of the nodes must be already set when
the layer is constructed.</p>
</li>
<li><p class="first">In order to be able to build arbitrary feed-forward node structures,
<tt class="docutils literal">hinet</tt> provides a wrapper class for flows (i.e., vertical stacks
of nodes) called <tt class="docutils literal">FlowNode</tt>. For example, we can replace
<tt class="docutils literal">node1</tt> in the above example with a <tt class="docutils literal">FlowNode</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; node1_1 = mdp.nodes.PCANode(input_dim=100, output_dim=50)
&gt;&gt;&gt; node1_2 = mdp.nodes.SFANode(input_dim=50, output_dim=10)
&gt;&gt;&gt; node1_flow = mdp.Flow([node1_1, node1_2])
&gt;&gt;&gt; node1 = mdp.hinet.FlowNode(node1_flow)
&gt;&gt;&gt; layer = mdp.hinet.Layer([node1, node2])
&gt;&gt;&gt; layer
Layer(input_dim=200, output_dim=30, dtype=None)
</pre>
<p>in this example <tt class="docutils literal">node1</tt> has two training phases (one for each
internal node). Therefore <tt class="docutils literal">layer</tt> now has two training phases as well and
behaves like any other node with two training phases.
By combining and nesting <tt class="docutils literal">FlowNode</tt> and <tt class="docutils literal">Layer</tt>, it is thus possible
to build complex node structures.</p>
</li>
<li><p class="first">When implementing networks one might have to route
different parts of the data to different nodes in a layer in complex
ways. This is done by the <tt class="docutils literal">Switchboard</tt> node, which can handle such
the routing. A <tt class="docutils literal">Switchboard</tt> is initialized with a 1-D Array with
one entry for each output connection, containing the corresponding
index of the input connection that it receives its input from, e.g.:</p>
<pre class="literal-block">
&gt;&gt;&gt; switchboard = mdp.hinet.Switchboard(input_dim=6, connections=[0,1,2,3,4,3,4,5])
&gt;&gt;&gt; switchboard
Switchboard(input_dim=3, output_dim=2, dtype=None)
&gt;&gt;&gt; x = mdp.numx.array([[2,4,6,8,10,12]])
&gt;&gt;&gt; switchboard.execute(x)
array([[ 2,  4,  6,  8, 10,  8, 10, 12]])
</pre>
<p>The switchboard can then be followed by a layer that
splits the routed input to the appropriate nodes, as
illustrated in following picture:</p>
<img alt="switchboard example" src="hinet_switchboard.png" style="width: 400px;" />
<p>By combining layers with switchboards one can realize any
feed-forward network topology.  Defining the switchboard routing
manually can be quite tedious. One way to automatize this is by
defining switchboard subclasses for special routing situations. The
<tt class="docutils literal">Rectangular2dSwitchboard</tt> class is one such example and will be
briefly described in a later example.</p>
</li>
</ul>
</div>
<div class="section" id="html-representation">
<h2><a class="toc-backref" href="#id25">HTML representation</a></h2>
<p>Since hierarchical networks can be quite complicated, <tt class="docutils literal">hinet</tt>
includes the class <tt class="docutils literal">HiNetHTMLTranslator</tt> that translates
an MDP flow into a graphical visualization in an HTML file. We also provide
the helper function <tt class="docutils literal">show_flow</tt> which creates a complete HTML file with
the flow visualization in it and opens it in your standard browser.</p>
<pre class="literal-block">
&gt;&gt;&gt; mdp.hinet.show_flow(flow)
</pre>
<p>To integrate the HTML representation into your own custom HTML file
you can take a look at <tt class="docutils literal">show_flow</tt> to learn the usage of
<tt class="docutils literal">HiNetHTMLTranslator</tt>. You can also specify custom translations for
node types via the extension mechanism (e.g to define which parameters are
displayed). Note that <tt class="docutils literal">HiNetHTMLTranslator</tt> is derived from
<tt class="docutils literal">HiNetTranslator</tt> which is
the base class for general flow translations and is for example also used in
the <tt class="docutils literal">parallel</tt> package (to translate a flow into a parallel version).</p>
</div>
<div class="section" id="example-application-2-d-image-data">
<h2><a class="toc-backref" href="#id26">Example application (2-D image data)</a></h2>
<p>As promised we now present a more complicated example. We define the
lowest layer for some kind of image processing system. The input
data is assumed to consist of image sequences, with each image having
a size of 50 by 50 pixels. We take color images, so after converting
the images to one dimensional numpy arrays each pixel corresponds to
three numeric values in the array, which the values just next to each
other (one for each color channel).</p>
<p>The processing layer consists of many parallel units, which only see a
small image region with a size of 10 by 10 pixels. These so called
receptive fields cover the whole image and have an overlap of five
pixels. Note that the image data is represented as an 1-D
array. Therefore we need the <tt class="docutils literal">Rectangular2dSwitchboard</tt> class to
correctly route the data for each receptive field to the corresponding
unit in the following layer.  We also call the switchboard output for
a single receptive field an output channel and the three RGB values
for a single pixel form an input channel.  Each processing unit is a
flow consisting of an <tt class="docutils literal">SFANode</tt> (to somewhat reduce the
dimensionality) that is followed by an <tt class="docutils literal">SFA2Node</tt>. Since we assume
that the statistics are similar in each receptive filed we actually
use the same nodes for each receptive field. Therefore we use a
<tt class="docutils literal">CloneLayer</tt> instead of the standard <tt class="docutils literal">Layer</tt>. Here is the actual
code:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; switchboard = mdp.hinet.Rectangular2dSwitchboard(x_in_channels=50,
...                                                  y_in_channels=50,
...                                                  x_field_channels=10,
...                                                  y_field_channels=10,
...                                                  x_field_spacing=5,
...                                                  y_field_spacing=5,
...                                                  in_channel_dim=3)
&gt;&gt;&gt; sfa_dim = 48
&gt;&gt;&gt; sfa_node = mdp.nodes.SFANode(input_dim=switchboard.out_channel_dim,
...                              output_dim=sfa_dim)
&gt;&gt;&gt; sfa2_dim = 32
&gt;&gt;&gt; sfa2_node = mdp.nodes.SFA2Node(input_dim=sfa_dim,
...                                output_dim=sfa2_dim)
&gt;&gt;&gt; flownode = mdp.hinet.FlowNode(mdp.Flow([sfa_node, sfa2_node]))
&gt;&gt;&gt; sfa_layer = mdp.hinet.CloneLayer(flownode,
...                                  n_nodes=switchboard.output_channels)
&gt;&gt;&gt; flow = mdp.Flow([switchboard, sfa_layer])
</pre>
<p>The HTML representation of the the constructed flow looks like this in your
browser:</p>
<img alt="hinet HTML rendering" src="hinet_html.png" style="width: 450px;" />
<p>Now one can train this flow for example with image sequences from a movie.
After the training phase one can compute the image pattern that produces
the highest response in a given output coordinate
(use <tt class="docutils literal">mdp.utils.QuadraticForm</tt>). One such optimal image pattern may
look like this (only a grayscale version is shown):</p>
<img alt="optimal stimulus" src="hinet_opt_stim.png" />
<p>So the network units have developed some kind of primitive line
detector. More on this topic can be found in: Berkes, P. and Wiskott,
L., <cite>Slow feature analysis yields a rich repertoire of complex cell
properties</cite>.
<a class="reference external" href="http://journalofvision.org/5/6/9/">Journal of Vision, 5(6):579-602</a>.</p>
<p>One could also add more layers on top of this first layer to do more
complicated stuff. Note that the <tt class="docutils literal">in_channel_dim</tt> in the next
<tt class="docutils literal">Rectangular2dSwitchboard</tt> would be 32, since this is the output dimension
of one unit in the <tt class="docutils literal">CloneLayer</tt> (instead of 3 in the first switchboard,
corresponding to the three RGB colors).</p>
</div>
</div>
<div class="section" id="parallelization">
<h1><a class="toc-backref" href="#id27">Parallelization</a></h1>
<p>The <tt class="docutils literal">parallel</tt> package adds the ability to parallelize the training
and execution of MPD flows. This package is split into two decoupled parts:</p>
<ul class="simple">
<li>The first part consists of a parallel extension of the familiar MDP
structures of nodes and flows. The first basic building block is the
extension class <tt class="docutils literal">ParallelExtensionNode</tt> for nodes which can be trained
in a parallelized way. It adds the <tt class="docutils literal">fork</tt> and <tt class="docutils literal">join</tt> methods. When
providing a parallel extension for custom node classes you should provide
<tt class="docutils literal">_fork</tt> and <tt class="docutils literal">_join</tt>.
Secondly there is the <tt class="docutils literal">ParallelFlow</tt> class,
which internally splits the training or execution into tasks which can
then be processed in parallel.</li>
<li>The second part consists of the schedulers. A scheduler takes tasks
and processes them in a more or less parallel way (e.g. in multiple
Python processes). A scheduler deals with the more technical aspects
of the parallelization, but does not need to know anything about
nodes and flows.</li>
</ul>
<div class="section" id="basic-examples">
<h2><a class="toc-backref" href="#id28">Basic Examples</a></h2>
<p>In the following example we parallelize a simple <tt class="docutils literal">Flow</tt> consisting of
PCA and quadratic SFA, so that it makes use of two cores on a modern CPU:</p>
<pre class="literal-block">
&gt;&gt;&gt; node1 = mdp.nodes.PCANode(input_dim=100, output_dim=10)
&gt;&gt;&gt; node2 = mdp.nodes.SFA2Node(input_dim=10, output_dim=10)
&gt;&gt;&gt; parallel_flow = mdp.parallel.ParallelFlow([node1, node2])
&gt;&gt;&gt; n_data_chunks = 2
&gt;&gt;&gt; data_iterables = [[mdp.numx_rand.random((200, 100))
...                    for _ in range(n_data_chunks)]
...                    for _ in range(2)]
&gt;&gt;&gt; scheduler = mdp.parallel.ProcessScheduler(n_processes=2)
&gt;&gt;&gt; parallel_flow.train(data_iterables, scheduler=scheduler)
&gt;&gt;&gt; scheduler.shutdown()
</pre>
<p>So only two additional lines were needed to parallelize the training
of the flow. All one has to do is use a <tt class="docutils literal">ParallelFlow</tt> insttead of the normal
<tt class="docutils literal">Flow</tt> and provide a scheduler. Note that the
<tt class="docutils literal">shutdown</tt> method should be always called at the end to make sure
that the threads and processes used by the scheduler are cleaned up
properly. So one should better put the <tt class="docutils literal">shutdown</tt> call into a safer
try/finally statement:</p>
<pre class="literal-block">
&gt;&gt;&gt; try:
...     parallel_flow.train(data_iterables, scheduler=scheduler)
... finally:
...     scheduler.shutdown()
...
</pre>
</div>
<div class="section" id="scheduler">
<h2><a class="toc-backref" href="#id29">Scheduler</a></h2>
<p>A scheduler is an instance of one of the scheduler classes we
provide. They are all derived from the <tt class="docutils literal">Scheduler</tt> base class. Apart
from the base class we currently only provide  the <tt class="docutils literal">ProcessScheduler</tt>
which distributes the incoming tasks over multiple Python
processes (circumventing the global interpreter lock). There is also
experimental support for the
<a class="reference external" href="http://www.parallelpython.com">Parallel Python library</a>
in the <tt class="docutils literal">mdp.parallel.pp_support</tt> package.</p>
<p>The first important method of the scheduler class is
<tt class="docutils literal">add_task</tt>. This method takes two arguments: <tt class="docutils literal">data</tt> and
<tt class="docutils literal">task_callable</tt>, which can be a function or an object with a
<tt class="docutils literal">__call__</tt> method. The return value of the <tt class="docutils literal">task_callable</tt> is the
result of the task. If <tt class="docutils literal">task_callable</tt> is <tt class="docutils literal">None</tt> then the last
provided <tt class="docutils literal">task_callable</tt> will be used. This splitting into callable
and data makes it possible to implement caching of the
<tt class="docutils literal">task_callable</tt> in the scheduler and its workers (caching is turned on by
default in the <tt class="docutils literal">ProcessScheduler</tt>). To further influence caching you can also
derive from the <tt class="docutils literal">TaskCallable</tt> class, which has a <tt class="docutils literal">fork</tt> to generate new
callables when the cached callable must be preserved. For MDP training and
execution there already are corresponding <tt class="docutils literal">TaskCallable</tt> classes which are
automatically used, so normally there is no need to worry about this.</p>
<p>After submitting all the tasks with <tt class="docutils literal">add_task</tt> you can then call
the <tt class="docutils literal">get_results</tt> method. This method returns all the task results,
normally in a list. If there are open tasks in the scheduler
<tt class="docutils literal">get_results</tt> will wait until all the tasks are finished. You can
also check the status of the scheduler by looking at the
<tt class="docutils literal">n_open_tasks</tt> property, which tells you the number of open tasks.
After using the scheduler you should always call the <tt class="docutils literal">shutdown</tt> method,
otherwise you might get error messages from not properly closed processes.</p>
<p>Internally an instance of the base class <tt class="docutils literal">mdp.parallel.ResultContainer</tt> is
used for the storage of the results in the scheduler. By providing your own
result container to the scheduler you modify the storage. For example the
default result container is an instance of <tt class="docutils literal">OrderedResultContainer</tt></p>
</div>
<div class="section" id="parallel-nodes">
<h2><a class="toc-backref" href="#id30">Parallel Nodes</a></h2>
<p>If you want to parallelize your own nodes you have to provide parallel
extensions for them. The <tt class="docutils literal">ParallelExtensionNode</tt> base class has
the new template methods <tt class="docutils literal">fork</tt> and <tt class="docutils literal">join</tt>.
<tt class="docutils literal">fork</tt> should return a new node instance. This new instance can then be
trained somewhere else (e.g. in a different process) with the usual <tt class="docutils literal">train</tt>
method. Afterwards one calls <tt class="docutils literal">join</tt> on the original node, with the
forked node as the argument. This is effectively the same as calling
<tt class="docutils literal">train</tt> directly on the original node.</p>
<p>When writing your own parallel node extension you should only overwrite the
<tt class="docutils literal">_fork</tt> and <tt class="docutils literal">_join</tt> methods, which are automatically called by <tt class="docutils literal">fork</tt> and
<tt class="docutils literal">join</tt>. The <tt class="docutils literal">fork</tt> and <tt class="docutils literal">join</tt> take care of the standard node
attributes like the dimensions. You should also look at the source
code of a parallel node like <tt class="docutils literal">ParallelPCANode</tt> to get a better idea
of how to parallelize nodes.</p>
<p>Currently we provide the following parallel nodes:
<tt class="docutils literal">ParallelPCANode</tt>, <tt class="docutils literal">ParallelWhiteningNode</tt>, <tt class="docutils literal">ParallelSFANode</tt>,
<tt class="docutils literal">ParallelSFA2Node</tt>, <tt class="docutils literal">ParallelFDANode</tt>, <tt class="docutils literal">ParallelHistogramNode</tt>,
<tt class="docutils literal">ParallelAdaptiveCutoffNode</tt>, <tt class="docutils literal">ParallelFlowNode</tt>, <tt class="docutils literal">ParallelLayer</tt>,
<tt class="docutils literal">ParallelCloneLayer</tt> (the last three are derived from the <tt class="docutils literal">hinet</tt>
package).</p>
</div>
<div class="section" id="parallel-flows">
<h2><a class="toc-backref" href="#id31">Parallel Flows</a></h2>
<p>As shown earlier in the example a parallel flow implements the
parallel training (and execution) using a provided scheduler. The
scheduler is simply provided as an additional argument for the train
or execute method of the parallel flow. If no scheduler is provided
the parallel flow behaves just like a normal flow.</p>
<p>You can also do the parallel training in a customized way by manually
fetching tasks and assigning them to a scheduler. However, this should
rarely be required.</p>
</div>
</div>
<div class="section" id="real-life-examples">
<h1><a class="toc-backref" href="#id32">Real life examples</a></h1>
<div class="section" id="logistic-maps">
<h2><a class="toc-backref" href="#id33">Logistic maps</a></h2>
<p>In this section we show a complete example of MDP usage in a machine
learning application, and use non-linear Slow Feature Analysis for
processing of non-stationary time series. We consider a chaotic time
series derived by a logistic map (a demographic model of the
population biomass of species in the presence of limiting factors such
as food supply or disease) that is non-stationary in the sense that
the underlying parameter is not fixed but is varying smoothly in time.</p>
<p>The goal is to extract the slowly varying parameter that is hidden
in the observed time series. This example reproduces some of the
results reported in
Laurenz Wiskott, <cite>Estimating Driving Forces of Nonstationary Time Series
with Slow Feature Analysis</cite>.
<a class="reference external" href="http://arxiv.org/abs/cond-mat/0312317">arXiv.org e-Print archive</a>.</p>
<p>Generate the slowly varying driving force,
a combination of three sine waves (freqs: 5, 11, 13 Hz), and define a function
to generate the logistic map</p>
<pre class="literal-block">
&gt;&gt;&gt; p2 = mdp.numx.pi*2
&gt;&gt;&gt; t = mdp.numx.linspace(0,1,10000,endpoint=0) # time axis 1s, samplerate 10KHz
&gt;&gt;&gt; dforce = mdp.numx.sin(p2*5*t) + mdp.numx.sin(p2*11*t) + mdp.numx.sin(p2*13*t)
&gt;&gt;&gt; def logistic_map(x,r):
...     return r*x*(1-x)
...
&gt;&gt;&gt;
</pre>
<p>Note that we define <tt class="docutils literal">series</tt> to be a two-dimensional array.
Inputs to MDP must be two-dimensional arrays with variables
on columns and observations on rows. In this case we have only
one variable:</p>
<pre class="literal-block">
&gt;&gt;&gt; series = mdp.numx.zeros((10000,1),'d')
</pre>
<p>Fix the initial condition:</p>
<pre class="literal-block">
&gt;&gt;&gt; series[0] = 0.6
</pre>
<p>Generate the time series using the logistic equation.
The driving force modifies the logistic equation parameter <tt class="docutils literal">r</tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; for i in range(1,10000):
...     series[i] = logistic_map(series[i-1],3.6+0.13*dforce[i])
...
&gt;&gt;&gt;
</pre>
<p>If you have a plotting package <tt class="docutils literal">series</tt> should look like this:</p>
<img alt="chaotic time series" src="series.png" style="width: 700px;" />
<p>To reconstruct the underlying parameter, we define a <tt class="docutils literal">Flow</tt> to
perform SFA in the space of polynomials of degree 3. We first use a
node that embeds the 1-dimensional time series in a 10 dimensional
space using a sliding temporal window of size 10
(<tt class="docutils literal">TimeFramesNode(10)</tt>).  Second, we expand the signal in the space
of polynomials of degree 3 using a
<tt class="docutils literal">PolynomialExpansionNode(3)</tt>. Finally, we perform SFA on the
expanded signal and keep the slowest feature using the
<tt class="docutils literal">SFANode(output_dim=1)</tt>.</p>
<p>In order to measure the slowness of the input time series before and
after processing, we put at the beginning and at the end of the node
sequence a node that computes the $eta$-value (a measure of slowness)
of its input (<tt class="docutils literal">EtaComputerNode()</tt>):</p>
<pre class="literal-block">
&gt;&gt;&gt; flow = (mdp.nodes.EtaComputerNode() +
...         mdp.nodes.TimeFramesNode(10) +
...         mdp.nodes.PolynomialExpansionNode(3) +
...         mdp.nodes.SFANode(output_dim=1) +
...         mdp.nodes.EtaComputerNode() )
...
&gt;&gt;&gt;
</pre>
<p>Since the time series is short enough to be kept in memory
we don't need to define generators and we can feed the flow
directly with the whole signal:</p>
<pre class="literal-block">
&gt;&gt;&gt; flow.train(series)
</pre>
<p>Since the second and the third nodes are not trainable we are
going to get two warnings (<tt class="docutils literal">Training Interrupted</tt>). We can safely
ignore them. Execute the flow to get the slow feature</p>
<pre class="literal-block">
&gt;&gt;&gt; slow = flow(series)
</pre>
<p>The slow feature should match the driving force
up to a scaling factor, a constant offset and the sign.
To allow a comparison we rescale the driving force
to have zero mean and unit variance:</p>
<pre class="literal-block">
&gt;&gt;&gt; resc_dforce = (dforce - mdp.numx.mean(dforce,0))/mdp.numx.std(dforce,0)
</pre>
<p>Print covariance between the rescaled driving force and
the slow feature. Note that embedding the time series with
10 time frames leads to a time series with 9 observations less:</p>
<pre class="literal-block">
&gt;&gt;&gt; mdp.utils.cov2(resc_dforce[:-9],slow)
0.99992501533859179
</pre>
<p>Print the <em>eta-values</em> of the chaotic time series and of
the slow feature</p>
<pre class="literal-block">
&gt;&gt;&gt; print 'Eta value (time series): ', flow[0].get_eta(t=10000)
Eta value (time series):  [ 3002.53380245]
&gt;&gt;&gt; print 'Eta value (slow feature): ', flow[-1].get_eta(t=9996)
Eta value (slow feature):  [ 10.2185087]
</pre>
<p>If you have a plotting package you could plot the real driving force
is plotted together with the driving force estimated by SFA and see
that they match perfectly:</p>
<img alt="SFA estimate" src="results.png" style="width: 700px;" />
</div>
<div class="section" id="growing-neural-gas">
<h2><a class="toc-backref" href="#id34">Growing neural gas</a></h2>
<p>We generate uniformly distributed random data points confined on different
2-D geometrical objects. The Growing Neural Gas Node builds a graph with the
same topological structure.</p>
<p>Fix the random seed to obtain reproducible results:</p>
<pre class="literal-block">
&gt;&gt;&gt; mdp.numx_rand.seed(1266090063)
</pre>
<p>Some functions to generate uniform probability distributions on
different geometrical objects:</p>
<pre class="literal-block">
&gt;&gt;&gt; def uniform(min_, max_, dims):
...     &quot;&quot;&quot;Return a random number between min_ and max_ .&quot;&quot;&quot;
...     return mdp.numx_rand.random(dims)*(max_-min_)+min_
...
&gt;&gt;&gt; def circumference_distr(center, radius, n):
...     &quot;&quot;&quot;Return n random points uniformly distributed on a circumference.&quot;&quot;&quot;
...     phi = uniform(0, 2*mdp.numx.pi, (n,1))
...     x = radius*mdp.numx.cos(phi)+center[0]
...     y = radius*mdp.numx.sin(phi)+center[1]
...     return mdp.numx.concatenate((x,y), axis=1)
...
&gt;&gt;&gt; def circle_distr(center, radius, n):
...     &quot;&quot;&quot;Return n random points uniformly distributed on a circle.&quot;&quot;&quot;
...     phi = uniform(0, 2*mdp.numx.pi, (n,1))
...     sqrt_r = mdp.numx.sqrt(uniform(0, radius*radius, (n,1)))
...     x = sqrt_r*mdp.numx.cos(phi)+center[0]
...     y = sqrt_r*mdp.numx.sin(phi)+center[1]
...     return mdp.numx.concatenate((x,y), axis=1)
...
&gt;&gt;&gt; def rectangle_distr(center, w, h, n):
...     &quot;&quot;&quot;Return n random points uniformly distributed on a rectangle.&quot;&quot;&quot;
...     x = uniform(-w/2., w/2., (n,1))+center[0]
...     y = uniform(-h/2., h/2., (n,1))+center[1]
...     return mdp.numx.concatenate((x,y), axis=1)
...
&gt;&gt;&gt; N = 2000
</pre>
<p>Explicitly collect random points from some distributions:</p>
<ul>
<li><p class="first">Circumferences:</p>
<pre class="literal-block">
&gt;&gt;&gt; cf1 = circumference_distr([6,-0.5], 2, N)
&gt;&gt;&gt; cf2 = circumference_distr([3,-2], 0.3, N)
</pre>
</li>
<li><p class="first">Circles:</p>
<pre class="literal-block">
&gt;&gt;&gt; cl1 = circle_distr([-5,3], 0.5, N/2)
&gt;&gt;&gt; cl2 = circle_distr([3.5,2.5], 0.7, N)
</pre>
</li>
<li><p class="first">Rectangles:</p>
<pre class="literal-block">
&gt;&gt;&gt; r1 = rectangle_distr([-1.5,0], 1, 4, N)
&gt;&gt;&gt; r2 = rectangle_distr([+1.5,0], 1, 4, N)
&gt;&gt;&gt; r3 = rectangle_distr([0,+1.5], 2, 1, N/2)
&gt;&gt;&gt; r4 = rectangle_distr([0,-1.5], 2, 1, N/2)
</pre>
</li>
</ul>
<p>Shuffle the points to make the statistics stationary</p>
<pre class="literal-block">
&gt;&gt;&gt; x = mdp.numx.concatenate([cf1, cf2, cl1, cl2, r1,r2,r3,r4], axis=0)
&gt;&gt;&gt; x = mdp.numx.take(x,mdp.numx_rand.permutation(x.shape[0]), axis=0)
</pre>
<p>If you have a plotting package <tt class="docutils literal">x</tt> should look like this:</p>
<img alt="GNG starting distribution" src="gng_distribution.png" style="width: 700px;" />
<p>Create a <tt class="docutils literal">GrowingNeuralGasNode</tt> and train it:</p>
<pre class="literal-block">
&gt;&gt;&gt; gng = mdp.nodes.GrowingNeuralGasNode(max_nodes=75)
</pre>
<p>The initial distribution of nodes is randomly chosen:</p>
<img alt="GNG starting condition" src="gng_initial.png" style="width: 700px;" />
<p>The training is performed in small chunks in order to visualize
the evolution of the graph:</p>
<pre class="literal-block">
&gt;&gt;&gt; STEP = 500
&gt;&gt;&gt; for i in range(0,x.shape[0],STEP):
...     gng.train(x[i:i+STEP])
...     # [...] plotting instructions
...
&gt;&gt;&gt; gng.stop_training()
</pre>
<p>See <a class="reference external" href="animated_training.gif">here</a> the animation of training.</p>
<p>Visualizing the neural gas network, we'll see that it is
adapted to the topological structure of the data distribution:</p>
<img alt="GNG final condition" src="gng_final.png" style="width: 700px;" />
<p>Calculate the number of connected components:</p>
<pre class="literal-block">
&gt;&gt;&gt; n_obj = len(gng.graph.connected_components())
&gt;&gt;&gt; print n_obj
5
</pre>
</div>
<div class="section" id="locally-linear-embedding">
<h2><a class="toc-backref" href="#id35">Locally linear embedding</a></h2>
<p>Locally linear embedding (LLE) approximates the input data with a
low-dimensional surface and reduces its dimensionality by learning
a mapping to the surface. Here we consider data generated randomly
on an S-shaped 2D surface embedded in a 3D space:</p>
<img alt="S-shaped surface" src="s_shape_3D.png" style="width: 500px;" />
<p>The surface is defined by the function</p>
<pre class="literal-block">
&gt;&gt;&gt; def s_distr(npoints, hole=False):
...     &quot;&quot;&quot;Return a 3D S-shaped surface. If hole is True, the surface has
...     a hole in the middle.&quot;&quot;&quot;
...     t = mdp.numx_rand.random(npoints)
...     y = mdp.numx_rand.random(npoints)*5.
...     theta = 3.*mdp.numx.pi*(t-0.5)
...     x = mdp.numx.sin(theta)
...     z = mdp.numx.sign(theta)*(mdp.numx.cos(theta) - 1.)
...     if hole:
...         indices = mdp.numx.where(((0.3&gt;t) | (0.7&lt;t)) | ((1.&gt;y) | (4.&lt;y)))
...         return x[indices], y[indices], z[indices], t[indices]
...     else:
...         return x, y, z, t
</pre>
<p>We generate 1000 points on this surface, then define an LLENode with
parameters k=15 (number of neighbors) and output_dim=2 (the number of
dimensions of the reduced representation), then train and execute the
node to obtain the projected data:</p>
<pre class="literal-block">
&gt;&gt;&gt; n, k = 1000, 15
&gt;&gt;&gt; x, y, z, t = s_distr(n, hole=False)
&gt;&gt;&gt; data = mdp.numx.array([x,y,z]).T
&gt;&gt;&gt; lle_projected_data = mdp.nodes.LLENode(k, output_dim=2)(data)
</pre>
<p>The projected data forms a nice parametric representation of the
S-shaped surface:</p>
<img alt="LLE projection of the S-shaped surface" src="s_shape_lle_proj.png" style="width: 500px;" />
<p>The problem becomes more difficult if the surface has a hole in the
middle:</p>
<img alt="S-shaped surface with hole" src="s_shape_hole_3D.png" style="width: 500px;" />
<p>In this case, the LLE algorithm has some difficulty finding the
correct representation. The lines</p>
<pre class="literal-block">
&gt;&gt;&gt; x, y, z, t = s_distr(n, hole=True)
&gt;&gt;&gt; data = mdp.numx.array([x,y,z]).T
&gt;&gt;&gt; lle_projected_data = mdp.nodes.LLENode(k, output_dim=2)(data)
</pre>
<p>return a distorted mapping:</p>
<img alt="LLE projection of the S-shaped surface with hole" src="s_shape_hole_lle_proj.png" style="width: 500px;" />
<p>The Hessian LLE Node takes the local curvature of the surface into
account, and is able to find a better representation:</p>
<pre class="literal-block">
&gt;&gt;&gt; hlle_projected_data = mdp.nodes.HLLENode(k, output_dim=2)(data)
</pre>
<img alt="HLLE projection of the S-shaped surface with hole" src="s_shape_hole_hlle_proj.png" style="width: 500px;" />
</div>
</div>
<div class="section" id="node-list">
<h1><a class="toc-backref" href="#id36">Node List</a></h1>
<p>Here is the complete list of implemented nodes.
Refer to the
<a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/index.html">API</a>
for the full documentation and interface description.</p>
<ul>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.AdaptiveCutoffNode-class.html">AdaptiveCutoffNode</a></dt>
<dd><p class="first last">Works like the <cite>HistogramNode</cite>. The cutoff bounds are then chosen such that
a given fraction of the training data would have been clipped.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.CuBICANode-class.html">CuBICANode</a></dt>
<dd><p class="first">Perform Independent Component Analysis using the CuBICA algorithm.</p>
<p class="last">Reference: Blaschke, T. and Wiskott, L. (2003).
<em>CuBICA: Independent Component Analysis by Simultaneous Third- and
Fourth-Order Cumulant Diagonalization</em>.
IEEE Transactions on Signal Processing, 52(5), pp. 1250-1256.
More information about ICA can be found among others in
Hyvarinen A., Karhunen J., Oja E. (2001). <em>Independent Component Analysis</em>,
Wiley.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.CutoffNode-class.html">CutoffNode</a></dt>
<dd><p class="first last">Clip the data at the specified upper and lower bounds.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.EtaComputerNode-class.html">EtaComputerNode</a></dt>
<dd><p class="first">Compute the eta values of the normalized training data.
The delta value of a signal is a measure of its temporal
variation, and is defined as the mean of the derivative squared,
i.e. <tt class="docutils literal">delta(x) = <span class="pre">mean(dx/dt(t)^2)</span></tt>. <tt class="docutils literal">delta(x)</tt> is zero if
'x' is a constant signal, and increases if the temporal variation
of the signal is bigger.
The eta value is a more intuitive measure of temporal variation,
defined as <tt class="docutils literal">eta(x) = <span class="pre">T/(2*pi)</span> * sqrt(delta(x))</tt>.
If 'x' is a signal of length 'T' which consists of a sine function
that accomplishes exactly 'N' oscillations, then <tt class="docutils literal">eta(x) = N</tt>.</p>
<p class="last">Reference: Wiskott, L. and Sejnowski, T.J. (2002).
<em>Slow Feature Analysis:
Unsupervised Learning of Invariances</em>, Neural Computation,
14(4):715-770.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.FANode-class.html">FANode</a></dt>
<dd><p class="first last">Perform Factor Analysis. The current implementation should be most
efficient for long data sets: the sufficient statistics are
collected in the training phase, and all EM-cycles are performed at
its end. More information about Factor Analysis can be found in
<a class="reference external" href="http://www.ics.uci.edu/~welling/classnotes/classnotes.html">Max Welling's classnotes</a>
in the chapter &quot;Linear Models&quot;.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.FastICANode-class.html">FastICANode</a></dt>
<dd><p class="first">Perform Independent Component Analysis using the FastICA algorithm.</p>
<p class="last">Reference:
Aapo Hyvarinen (1999).
<em>Fast and Robust Fixed-Point Algorithms for Independent Component Analysis</em>,
IEEE Transactions on Neural Networks, 10(3):626-634.
More information about ICA can be found among others in
Hyvarinen A., Karhunen J., Oja E. (2001). <em>Independent Component Analysis</em>,
Wiley.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.FDANode-class.html">FDANode</a></dt>
<dd><p class="first">Perform a (generalized) Fisher Discriminant Analysis of its
input. It is a supervised node that implements FDA using a
generalized eigenvalue approach.</p>
<p class="last">More information on Fisher Discriminant Analysis can be found for
example in C. Bishop, <em>Neural Networks for Pattern Recognition</em>,
Oxford Press, pp. 105-112.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.GaussianClassifierNode-class.html">GaussianClassifierNode</a></dt>
<dd><p class="first last">Perform a supervised Gaussian classification.  Given a set of
labelled data, the node fits a gaussian distribution to each
class.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.GrowingNeuralGasNode-class.html">GrowingNeuralGasNode</a></dt>
<dd><p class="first">Learn the topological structure of the input data by building a corresponding
graph approximation.</p>
<p class="last">More information about the Growing Neural Gas algorithm can be found in B.
Fritzke, <em>A Growing Neural Gas Network Learns Topologies</em>, in G. Tesauro, D. S.
Touretzky, and T. K. Leen (editors), <em>Advances in Neural Information
Processing Systems 7</em>, pages 625-632. MIT Press, Cambridge MA, 1995.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.HistogramNode-class.html">HistogramNode</a></dt>
<dd><p class="first last">Store a fraction of the incoming data during training. This data can then
be used to analyse the histogram of the data.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.HitParadeNode-class.html">HitParadeNode</a></dt>
<dd><p class="first last">Collect the first 'n' local maxima and minima of the training signal
which are separated by a minimum gap 'd'.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.HLLENode-class.html">HLLENode</a></dt>
<dd><p class="first">Original code contributed by Jake VanderPlas.</p>
<p>Perform a Hessian Locally Linear Embedding analysis on the data.</p>
<p class="last">Implementation based on algorithm outlined in
David L. Donoho and Carrie Grimes,
<em>Hessian Eigenmaps: new locally linear embedding techniques
for high-dimensional data</em>, Proceedings of the National Academy of Sciences
100(10):5591-5596 (2003).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.ISFANode-class.html">ISFANode</a></dt>
<dd><p class="first">Perform Independent Slow Feature Analysis on the input data.</p>
<p class="last">More information about ISFA can be found in:
Blaschke, T. , Zito, T., and Wiskott, L.
<em>Independent Slow Feature Analysis and Nonlinear Blind Source Separation.</em>
Neural Computation 19(4):994-1021 (2007).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.JADENode-class.html">JADENode</a></dt>
<dd><p class="first">Original code contributed by Gabriel Beckers.</p>
<p>Perform Independent Component Analysis using the JADE algorithm.</p>
<p class="last">References:
Cardoso, J.-F, and Souloumiac, A.
<em>Blind beamforming for non Gaussian signals.</em>
Radar and Signal Processing, IEE Proceedings F, 140(6): 362-370 (1993), and
Cardoso, J.-F.
<em>High-order contrasts for independent component analysis.</em>
Neural Computation, 11(1): 157-192 (1999).
More information about ICA can be found among others in
Hyvarinen A., Karhunen J., Oja E. (2001). <em>Independent Component Analysis</em>,
Wiley.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.LinearRegressionNode-class.html">LinearRegressionNode</a></dt>
<dd><p class="first last">Compute least-square, multivariate linear regression on the input data.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.LLENode-class.html">LLENode</a></dt>
<dd><p class="first">Original code contributed by Jake VanderPlas.</p>
<p>Perform a Locally Linear Embedding analysis on the data.</p>
<p>Based on the algorithm outlined in <em>An Introduction to Locally
Linear Embedding</em> by L. Saul and S. Roweis, using improvements
suggested in <em>Locally Linear Embedding for Classification</em> by
D. deRidder and R.P.W. Duin.</p>
<p class="last">References: Sam Roweis and Lawrence Saul, <em>Nonlinear dimensionality reduction by locally linear embedding</em>, Science 290(5500):2323-2326, 2000.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.NIPALSNode-class.html">NIPALSNode</a></dt>
<dd><p class="first">Original code contributed by Michael Schmuker, Susanne Lezius, and Farzad Farkhooi.</p>
<p>Perform Principal Component Analysis using the NIPALS algorithm.
This algorithm is particularyl useful if you have more variable than
observations, or in general when the number of variables is huge and
calculating a full covariance matrix may be unfeasable. It's also more
efficient of the standard PCANode if you expect the number of significant
principal components to be a small. In this case setting output_dim to be
a certain fraction of the total variance, say 90%, may be of some help.</p>
<p>Reference for NIPALS (Nonlinear Iterative Partial Least Squares):
Wold, H.
<em>Nonlinear estimation by iterative least squares procedures.</em>
in David, F. (Editor), Research Papers in Statistics, Wiley,
New York, pp 411-444 (1966).</p>
<p class="last">More information about Principal Component Analysis, a.k.a. discrete
Karhunen-Loeve transform can be found among others in
I.T. Jolliffe, <em>Principal Component Analysis</em>, Springer-Verlag (1986).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.NoiseNode-class.html">NoiseNode</a></dt>
<dd><p class="first">Original code contributed by Mathias Franzius.</p>
<p class="last">Inject multiplicative or additive noise into the input data.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.PCANode-class.html">PCANode</a></dt>
<dd><p class="first">Filter the input data throug the most significatives of its
principal components.</p>
<p class="last">More information about Principal Component Analysis, a.k.a. discrete
Karhunen-Loeve transform can be found among others in
I.T. Jolliffe, <em>Principal Component Analysis</em>, Springer-Verlag (1986).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.PolynomialExpansionNode-class.html">PolynomialExpansionNode</a></dt>
<dd><p class="first last">Perform expansion in a polynomial space.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.QuadraticExpansionNode-class.html">QuadraticExpansionNode</a></dt>
<dd><p class="first last">Perform expansion in the space formed by all linear and quadratic
monomials</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.RBMNode-class.html">RBMNode</a></dt>
<dd><p class="first">Implementation of a Restricted Boltzmann Machine.</p>
<p class="last">For more information on RBMs, see
Geoffrey E. Hinton (2007) <a class="reference external" href="http://www.scholarpedia.org/article/Boltzmann_machine">Boltzmann machine.</a>
Scholarpedia, 2(5):1668</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.RBMWithLabelsNode-class.html">RBMWithLabelsNode</a></dt>
<dd><p class="first">Implementation of a Restricted Boltzmann Machine with softmax labels.</p>
<p>For more information on RBMs, see
Geoffrey E. Hinton (2007) <a class="reference external" href="http://www.scholarpedia.org/article/Boltzmann_machine">Boltzmann machine</a>
Scholarpedia, 2(5):1668</p>
<p class="last">Hinton, G. E, Osindero, S., and Teh, Y. W. <em>A fast learning
algorithm for deep belief nets</em>, Neural Computation, 18:1527-1554 (2006).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.SFANode-class.html">SFANode</a></dt>
<dd><p class="first">Extract the slowly varying components from the input data.</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., <em>Slow Feature Analysis: Unsupervised
Learning of Invariances</em>, Neural Computation, 14(4):715-770 (2002).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.SFA2Node-class.html">SFA2Node</a></dt>
<dd><p class="first">Get an input signal, expand it in the space of
inhomogeneous polynomials of degree 2 and extract its slowly varying
components. The <tt class="docutils literal">get_quadratic_form</tt> method returns the input-output
function of one of the learned unit as a <tt class="docutils literal">mdp.utils.QuadraticForm</tt> object.</p>
<p class="last">More information about Slow Feature Analysis can be found in
Wiskott, L. and Sejnowski, T.J., <em>Slow Feature Analysis: Unsupervised
Learning of Invariances</em>, Neural Computation, 14(4):715-770 (2002).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.TDSEPNode-class.html">TDSEPNode</a></dt>
<dd><p class="first">Perform Independent Component Analysis using the TDSEP algorithm.
Note that TDSEP, as implemented in this Node, is an online algorithm,
i.e. it is suited to be trained on huge data sets, provided that the
training is done sending small chunks of data for each time.</p>
<p class="last">Reference:
Ziehe, Andreas and Muller, Klaus-Robert (1998).
<em>TDSEP an efficient algorithm for blind separation using time structure.</em>
in Niklasson, L, Boden, M, and Ziemke, T (Editors), Proc. 8th Int. Conf.
Artificial Neural Networks (ICANN 1998).</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.TimeFramesNode-class.html">TimeFramesNode</a></dt>
<dd><p class="first">Copy delayed version of the input signal on the space dimensions.</p>
<!-- ignore --><pre class="last literal-block">
For example, for time_frames=3 and gap=2:

[ X(1) Y(1)        [ X(1) Y(1) X(3) Y(3) X(5) Y(5)
  X(2) Y(2)          X(2) Y(2) X(4) Y(4) X(6) Y(6)
  X(3) Y(3)   --&gt;    X(3) Y(3) X(5) Y(5) X(7) Y(7)
  X(4) Y(4)          X(4) Y(4) X(6) Y(6) X(8) Y(8)
  X(5) Y(5)          ...  ...  ...  ...  ...  ... ]
  X(6) Y(6)
  X(7) Y(7)
  X(8) Y(8)
  ...  ...  ]
</pre>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.WhiteningNode-class.html">WhiteningNode</a></dt>
<dd><p class="first last">'Whiten' the input data by filtering it through the most
significatives of its principal components. All output
signals have zero mean, unit variance and are decorrelated.</p>
</dd>
</dl>
</li>
<li><p class="first"><a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/mdp.nodes.XSFANode-class.html">XSFANode</a></p>
<blockquote>
<p>Perform Non-linear Blind Source Separation using Slow Feature Analysis.
This node is designed to iteratively extract statistically
independent sources from (in principle) arbitrary invertible
nonlinear mixtures. The method relies on temporal correlations in
the sources and consists of a combination of nonlinear SFA and a
projection algorithm. More details can be found in the reference
given below (once it's published).</p>
<p>More information about XSFA can be found in:
Sprekeler, H., Zito, T., and Wiskott, L. (2009).
<em>An Extension of Slow Feature Analysis for Nonlinear Blind Source Separation.</em>
Journal of Machine Learning Research, submitted</p>
</blockquote>
</li>
</ul>
<div class="admonition-didn-t-you-find-what-you-were-looking-for admonition">
<p class="first admonition-title">Didn't you find what you were looking for?</p>
<p class="last">If you want to contribute some code or a new
algorithm, please do not hesitate to submit it!</p>
</div>
</div>
<div class="section" id="additional-utilities">
<h1><a class="toc-backref" href="#id37">Additional utilities</a></h1>
<p>MDP offers some additional utilities of general interest
in the <tt class="docutils literal">mdp.utils</tt> module. Refer to the
<a class="reference external" href="http://mdp-toolkit.sourceforge.net/docs/api/index.html">API</a>
for the full documentation and interface description.</p>
<dl class="docutils">
<dt><strong>CovarianceMatrix</strong></dt>
<dd><p class="first">This class stores an empirical covariance matrix that can be updated
incrementally. A call to the <tt class="docutils literal">fix</tt> method returns the current state
of the covariance matrix, the average and the number of observations,
and resets the internal data.</p>
<p class="last">Note that the internal sum is a standard <tt class="docutils literal">__add__</tt> operation. We are not
using any of the fancy sum algorithms to avoid round off errors when
adding many numbers. If you want to contribute a <tt class="docutils literal">CovarianceMatrix</tt>
class that uses such algorithms we would be happy to include it in
MDP.  For a start see the <a class="reference external" href="http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/393090">Python recipe</a>
by Raymond Hettinger. For a
review about floating point arithmetic and its pitfalls see
this <a class="reference external" href="http://docs.sun.com/source/806-3568/ncg_goldberg.html">interesting article</a>.</p>
</dd>
<dt><strong>DelayCovarianceMatrix</strong></dt>
<dd>This class stores an empirical covariance matrix between the signal and
time delayed signal that can be updated incrementally.</dd>
<dt><strong>MultipleCovarianceMatrices</strong></dt>
<dd>Container class for multiple covariance matrices to easily
execute operations on all matrices at the same time.</dd>
<dt><strong>dig_node(node)</strong></dt>
<dd>Crawl recursively an MDP <tt class="docutils literal">Node</tt> looking for arrays.
Return (dictionary, string), where the dictionary is:
{ attribute_name: (size_in_bytes, array_reference)}
and string is a nice string representation of it.</dd>
<dt><strong>get_node_size(node)</strong></dt>
<dd>Get 'node' total byte-size using <tt class="docutils literal">cPickle</tt> with protocol=2.
(The byte-size is related the memory needed by the node).</dd>
<dt><strong>progressinfo(sequence, length, style, custom)</strong></dt>
<dd><p class="first">A fully configurable text-mode progress info box tailored to the
command-line die-hards.
To get a progress info box for your loops use it like this:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; for i in progressinfo(sequence):
...     do_something(i)
</pre>
<p>You can also use it with generators, files or any other iterable object,
but in this case you have to specify the total length of the sequence:</p>
<!-- ignore --><pre class="literal-block">
&gt;&gt;&gt; for line in progressinfo(open_file, nlines):
...     do_something(line)
</pre>
<p>A few examples of the available layouts:</p>
<!-- ignore --><pre class="literal-block">
[===================================73%==============&gt;...................]
</pre>
<p>Progress:  67%[======================================&gt;                   ]</p>
<p class="last">23% [02:01:28] - [00:12:37]</p>
</dd>
<dt><strong>QuadraticForm</strong></dt>
<dd>Define an inhomogeneous quadratic form as <tt class="docutils literal">1/2 x'Hx + f'x + c</tt>.
This class implements the quadratic form analysis methods
presented in:
Berkes, P. and Wiskott, L. On the analysis and interpretation
of inhomogeneous quadratic forms as receptive fields. <em>Neural
Computation</em>, 18(8): 1868-1895. (2006).</dd>
<dt><strong>refcast(array, dtype)</strong></dt>
<dd>Cast the array to 'dtype' only if necessary,
otherwise return a reference.</dd>
<dt><strong>rotate(mat, angle, columns, units)</strong></dt>
<dd><p class="first">Rotate in-place a NxM data matrix in the plane defined by the 'columns'
when observation are stored on rows. Observations are rotated
counterclockwise. This corresponds to the following matrix-multiplication
for each data-point (unchanged elements omitted):</p>
<!-- ignore --><pre class="last literal-block">
[  cos(angle) -sin(angle)     [ x_i ]
   sin(angle)  cos(angle) ] * [ x_j ]
</pre>
</dd>
<dt><strong>random_rot(dim, dtype)</strong></dt>
<dd>Return a random rotation matrix, drawn from the Haar distribution
(the only uniform distribution on SO(n)).
The algorithm is described in the paper
Stewart, G.W., <em>The efficient generation of random orthogonal
matrices with an application to condition estimators</em>, SIAM Journal
on Numerical Analysis, 17(3), pp. 403-409, 1980.
For more information see this <a class="reference external" href="http://en.wikipedia.org/wiki/Orthogonal_matrix#Randomization">Wikipedia entry</a>.</dd>
<dt><strong>symrand(dim_or_eigv, dtype)</strong></dt>
<dd>Return a random symmetric (Hermitian) matrix with eigenvalues
uniformly distributed on (0,1].</dd>
</dl>
<div class="section" id="html-slideshows">
<h2><a class="toc-backref" href="#id38">HTML Slideshows</a></h2>
<p>The <tt class="docutils literal">mdp.utils</tt> module contains some classes and helper function to
display animated results in a Webbrowser. This works by creating an
HTML file with embedded JavaScript code, which dynamically loads
image files (the images contain the content that you want to animate
and can for example be created with matplotlib).
MDP internally uses the open source Templete templating libray,
written by David Bau.</p>
<p>The easiest way to create a slideshow it to use one of these two helper
function:</p>
<dl class="docutils">
<dt><strong>show_image_slideshow(filenames, image_size, filename=None, title=None, **kwargs)</strong></dt>
<dd>Write the slideshow into a HTML file, open it in the browser and
return the file name. <tt class="docutils literal">filenames</tt> is a list of the images files
that you want to display in the slideshow. <tt class="docutils literal">image_size</tt> is a
2-tuple containing the width and height at which the images should
be displayed. There are also a couple of additional arguments,
which are documented in the docstring.</dd>
<dt><strong>image_slideshow(filenames, image_size, title=None, **kwargs)</strong></dt>
<dd>This function is similar to <tt class="docutils literal">show_image_slideshow</tt>, but it simply
returns the slideshow HTML code (including the JavaScript code)
which you can then embed into your own HTML file. Note that
the default slideshow CSS code is not included, but it can be
accessed in <tt class="docutils literal">mdp.utils.IMAGE_SLIDESHOW_STYLE</tt>.</dd>
</dl>
<p>Note that there are also two demos for slideshows in the <tt class="docutils literal">mdp\demo</tt>
folder.</p>
</div>
<div class="section" id="graph-module">
<h2><a class="toc-backref" href="#id39">Graph module</a></h2>
<p>MDP contains <tt class="docutils literal">mdp.graph</tt>, a lightweight package to handle directed graphs.</p>
<dl class="docutils">
<dt><strong>Graph</strong></dt>
<dd><p class="first">Represent a directed graph. This class contains several methods
to create graph structures and manipulate them, among which</p>
<ul>
<li><dl class="first docutils">
<dt><tt class="docutils literal">add_tree</tt>: Add a tree to the graph.</dt>
<dd><p class="first">The tree is specified with a nested list of tuple, in a LISP-like
notation. The values specified in the list become the values of
the single nodes.
Return an equivalent nested list with the nodes instead of the values.</p>
<p class="last">Example:</p>
</dd>
</dl>
</li>
</ul>
    <!-- ignore -->

::

    >>> a=b=c=d=e=None
    >>> g.add_tree( (a, b, (c, d ,e)) )
    # corresponds to this tree structure, with all node values set to None:

            a
           / \
          b   c
             / \
            d   e<ul class="last">
<li><p class="first"><tt class="docutils literal">topological_sort</tt>: Perform a topological sort of the nodes.</p>
</li>
<li><p class="first"><tt class="docutils literal">dfs</tt>, <tt class="docutils literal">undirected_dfs</tt>: Perform Depth First sort.</p>
</li>
<li><p class="first"><tt class="docutils literal">bfs</tt>, <tt class="docutils literal">undirected_bfs</tt>: Perform Breadth First sort.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal">connected_components</tt>: Return a list of lists containing</dt>
<dd><p class="first last">the nodes of all connected components of the graph.</p>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal">is_weakly_connected</tt>: Return True if the graph is weakly connected.</p>
</li>
</ul>
</dd>
<dt><strong>GraphEdge</strong></dt>
<dd>Represent a graph edge and all information attached to it.</dd>
<dt><strong>GraphNode</strong></dt>
<dd>Represent a graph node and all information attached to it.</dd>
<dt><strong>recursive_map(func, seq)</strong></dt>
<dd>Apply a function recursively on a sequence and all subsequences.</dd>
<dt><strong>recursive_reduce(func, seq, *argv)</strong></dt>
<dd>Apply <tt class="docutils literal">reduce(func, seq)</tt> recursively to a sequence and all its
subsequences.</dd>
</dl>
</div>
</div>
<div class="section" id="bimdp">
<h1><a class="toc-backref" href="#id40">BiMDP</a></h1>
<p>BiMDP defines a framework for more general flow sequences, involving
top-down processes (e.g. for error backpropagation) and loops. So
the <em>bi</em> in BiMDP primarily stands for <em>bidirectional</em>. It also adds
a couple of other features, like a standartized way to transport
additional data, and a HTML based flow inspection utility. Because BiMDP
is a rather large addition and changes a few things compared to
standard MDP it is not included in <tt class="docutils literal">mdp</tt> but must be imported
seperately as <tt class="docutils literal">bimdp</tt> (BiMDP is included in the standard MDP
installation):</p>
<pre class="literal-block">
&gt;&gt;&gt; import bimdp
</pre>
<p><strong>Warning:</strong> BiMDP is a new addition to MDP, so currently it should be
considered as beta-stage software. Even though it already went through
long testing and several refactoring rounds it is still not as mature and
polished as the rest of MDP. This also means that your bug findings or
suggestions for improvement will be very valuable. The API of BiMDP should be
pretty stable now, we don't expect any fundamental breakages in
the future.</p>
<p>Here is a brief summary of the most important new features in BiMDP:</p>
<ul>
<li><p class="first">Nodes can specify other nodes as jump targets, where the execution or
training will be continued. It is now possible to use loops or
backpropagation, in contrast to the strictly linear execution of a
normal MDP flow. This is enabled by the new <tt class="docutils literal">BiFlow</tt> class. The new
<tt class="docutils literal">BiNode</tt> base class adds a <tt class="docutils literal">node_id</tt> string attribute, which can be
used to target a node.</p>
<p>The complexities of arbitrary data flows are evenly split up
between <tt class="docutils literal">BiNode</tt> and <tt class="docutils literal">BiFlow</tt>: Nodes specify their data and target
using a standartized interface, which is then interpreted by the flow
(somewhat like a very primitive domain specific language). The
alternative approach would have been to use specialised flow classes or
container nodes for each use case, which ultimately comes down to a
design decision. Of course you can (and should) still take that route if
for some reson BiMDP is not an adequate solution for your problem.</p>
</li>
<li><p class="first">In addition to the standard array data, nodes can transport more data
in a message dictionary (these are really just standard Python dictionaries,
so they are <tt class="docutils literal">dict</tt> instances). The new <tt class="docutils literal">BiNode</tt> base class provides
functionality to make this as convenient as possible.</p>
</li>
<li><p class="first">An interactive HTML-based inspection for flow training and execution is
available. This allows you to step through your flow for debugging or add
custom visualizations to analyse what is going on.</p>
</li>
<li><p class="first">BiMDP supports and extends the <tt class="docutils literal">hinet</tt> and the <tt class="docutils literal">parallel</tt>
packages from MDP. BiMDP in general is compatible with MDP, so you can use
standard MDP nodes in a <tt class="docutils literal">BiFlow</tt>. You can also use <tt class="docutils literal">BiNode</tt> instances
in a standard MDP flow, as long as you don't use certain BiMDP features.</p>
</li>
</ul>
<p>The structure of BiMDP closely follows that of MDP, so there are
submodules <tt class="docutils literal">bimdp.nodes</tt>, <tt class="docutils literal">bimdp.parallel</tt>, and <tt class="docutils literal">bimdp.hinet</tt>. The
module <tt class="docutils literal">bimdp.nodes</tt> contains <tt class="docutils literal">BiNode</tt> versions of nearly all MDP nodes.
For example <tt class="docutils literal">bimdp.nodes.PCABiNode</tt> is derived from both <tt class="docutils literal">BiNode</tt>
and <tt class="docutils literal">mdp.nodes.PCANode</tt>.</p>
<p>There are currently two examples available in the <tt class="docutils literal"><span class="pre">mdp-examples</span></tt> repository,
which demonstrate how BiMDP can be used. The first example
<tt class="docutils literal">backpropagation</tt> is a simple multilayer perceptron, using
backpropagation for learning. The second example <tt class="docutils literal">binetdbn</tt> is a
proof-of-concept implementation of a deep belief network.</p>
<p>Finally note that this tutorial is intended to serve as an introduction,
covering all the basic aspects of BiMDP. For more detailed specifications
have a look at the docstrings.</p>
<div class="section" id="targets-id-s-and-messages">
<h2><a class="toc-backref" href="#id41">Targets, id's and Messages</a></h2>
<p>The return value of the <tt class="docutils literal">execute</tt> method in a normal MDP node is
restricted to a single 2d array. A BiMDP <tt class="docutils literal">BiNode</tt> on the other hand can
optionally return a tuple containing an additional message dictionary
and a target value. So in general the return value is a tuple <tt class="docutils literal">(x, msg,
target)</tt>, where <tt class="docutils literal">x</tt> is a the usual 2d array. Alternatively a
<tt class="docutils literal">BiNode</tt> is also allowed to return only the array <tt class="docutils literal">x</tt> or a 2-tuple
<tt class="docutils literal">(x, msg)</tt> (specifying no target value). Unless stated otherwise the
last entry in the tuple should not be <tt class="docutils literal">None</tt>, but all the other values
are allowed to be <tt class="docutils literal">None</tt> (so if you specify a target then <tt class="docutils literal">msg</tt> can
be <tt class="docutils literal">None</tt>, and even <tt class="docutils literal">x</tt> can be <tt class="docutils literal">None</tt>).</p>
<p>The <tt class="docutils literal">msg</tt> message is a normal Python dictionary. You can use it to
transport any data that does not fit into the <tt class="docutils literal">x</tt> 2d data array. Nodes
can take data from to the message and add data to it. The message is
propagated along with the <tt class="docutils literal">x</tt> data. If a normal MDP node is contained
in a <tt class="docutils literal">BiFlow</tt> then the message is simply passed around it. A
<tt class="docutils literal">BiNode</tt> can freely decide how to interact with the message (see the
BiNode section for more information).</p>
<p>The target value is either a string or a number. The number is the
relative position of the target node in the flow, so a target value of 1
corresponds to the following node, while -1 is the previous node. The
<tt class="docutils literal">BiNode</tt> base class also allows the specification of a <tt class="docutils literal">node_id</tt>
string in the <tt class="docutils literal">__init__</tt> method. This string can then be used as a
target value.</p>
<p>The <tt class="docutils literal">node_id</tt> string is also useful to access nodes in a <tt class="docutils literal">BiFlow</tt>
instance. The standard MDP <tt class="docutils literal">Flow</tt> class already implements
standard Python container methods, so <tt class="docutils literal">flow[3]</tt> will return the third
node in the flow. <tt class="docutils literal">BiFlow</tt> in addition enables you to use the
<tt class="docutils literal">node_id</tt> to index nodes in the flow, just like for a dictionary. Here is
a simple example:</p>
<pre class="literal-block">
&gt;&gt;&gt; pca_node = bimdp.nodes.PCABiNode(node_id=&quot;pca&quot;)
&gt;&gt;&gt; biflow = bimdp.BiFlow([pca_node])
&gt;&gt;&gt; biflow[&quot;pca&quot;]
PCABiNode(input_dim=None, output_dim=None, dtype=None, node_id=&quot;pca&quot;)
</pre>
</div>
<div class="section" id="biflow">
<h2><a class="toc-backref" href="#id42">BiFlow</a></h2>
<p>The <tt class="docutils literal">BiFlow</tt> class mostly works in the same way as the normal <tt class="docutils literal">Flow</tt>
class. We already mentioned several of the new features, like support
for targets, messages, and retrieving nodes based on their node_id.
Appart from that the only major difference is the way in which you can
provide additional arguments for nodes. For example the <tt class="docutils literal">FDANode</tt> in
MDP requires class labels in addition to the data array (telling the
node to which class each data point belongs). In the <tt class="docutils literal">Flow</tt> class the
additional data (the class labels) is provided by the same iterable as
the data. In a <tt class="docutils literal">BiFlow</tt> this is no longer allowed, since this
functionality is provided by a more general mechanism. In addition to
the <tt class="docutils literal">data_iterables</tt> keyword argument there is a new <tt class="docutils literal">msg_iterables</tt>
argument, to provide iterables for the message dictionary. The structure
of the <tt class="docutils literal">msg_iterables</tt> argument must be the same as that of
<tt class="docutils literal">data_iterables</tt>, but instead of yielding arrays it should yield
dictionaries (containing the additional data values with the
corresponding keys). Here is an example:</p>
<pre class="literal-block">
&gt;&gt;&gt; samples = mdp.numx_rand.random((100,10))
&gt;&gt;&gt; labels = mdp.numx.arange(100)
&gt;&gt;&gt; flow = bimdp.BiFlow([mdp.nodes.PCANode(), bimdp.nodes.FDABiNode()])
&gt;&gt;&gt; flow.train([[samples],[samples]], [None,[{&quot;cl&quot;: labels}]])
</pre>
<p>The <tt class="docutils literal">_train</tt> method of <tt class="docutils literal">FDANode</tt> requires the <tt class="docutils literal">cl</tt> argument, so this
is used as the key value. Note that we have to use the <tt class="docutils literal">BiNode</tt>
version of <tt class="docutils literal">FDANode</tt>, called <tt class="docutils literal">FDABiNode</tt> (alomost every MDP node has a
<tt class="docutils literal">BiNode</tt> version following this naming scheme). The <tt class="docutils literal">BiNode</tt> class provides
the <tt class="docutils literal">cl</tt> value from the message to the <tt class="docutils literal">_train</tt> method.</p>
<p>In a normal <tt class="docutils literal">Flow</tt> the additional arguments can only be given to the
node which is currently in training. This limitation does not apply to a
<tt class="docutils literal">BiFlow</tt>, where the message can be accessed by all nodes (more on this
later). Message iterators can also be used during execution. There is a
<tt class="docutils literal">msg_iterable</tt> argument in <tt class="docutils literal">BiFlow.execute</tt> as well. Of course
messages can be also returned by <tt class="docutils literal">BiFlow.execute</tt>, so the return value
has the form <tt class="docutils literal">(y, msg)</tt>. If iterables were used then the <tt class="docutils literal">BiFlow</tt> not
only concatenates the <tt class="docutils literal">y</tt> arrays, but also tries to join the <tt class="docutils literal">msg</tt>
dictionaries into a single one. Arrays in the <tt class="docutils literal">msg</tt> will be
concatenated, for all other values the plus operator is used.</p>
<p>The <tt class="docutils literal">train</tt> method of <tt class="docutils literal">BiFlow</tt> also has an additional argument
called <tt class="docutils literal">stop_messages</tt>, which can be used to provide message iterables
for <tt class="docutils literal">stop_trianing</tt>. The <tt class="docutils literal">execute</tt> mehtod on the other hand has an
argument <tt class="docutils literal">target_iterable</tt>, which can be used to specify the initial
target in the flow execution.</p>
</div>
<div class="section" id="binode">
<h2><a class="toc-backref" href="#id43">BiNode</a></h2>
<p>We now want to give an overview the <tt class="docutils literal">BiNode</tt> API, which is mostly an
extension of the <tt class="docutils literal">Node</tt> API. First we want to look at the possible return
values of a <tt class="docutils literal">BiNode</tt> and briefly explain their meaning:</p>
<dl class="docutils">
<dt><tt class="docutils literal">execute</tt></dt>
<dd><tt class="docutils literal">x</tt> or <tt class="docutils literal">(x, msg)</tt> or <tt class="docutils literal">(x, msg, target)</tt>. Normal execution continues,
directly jumping to the target if one is specified.</dd>
</dl>
<p><tt class="docutils literal">train</tt></p>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">None</tt> terminates training.</li>
<li><tt class="docutils literal">x</tt> or <tt class="docutils literal">(x, msg)</tt> or <tt class="docutils literal">(x, msg, target)</tt>. Means that execution is
continued and that this node will be reached again to terminate training.
If the result has the form <tt class="docutils literal">(msg, None)</tt> then the <tt class="docutils literal">msg</tt> is dropped
(so it is not required to 'clear' the message manually to terminate
training).</li>
</ul>
</blockquote>
<p><tt class="docutils literal">stop_training, stop_message</tt></p>
<blockquote>
<ul class="simple">
<li><tt class="docutils literal">None</tt> terminates the stop_message propagation.</li>
<li><tt class="docutils literal">(msg, target)</tt>. If no target is specified then the remaining <tt class="docutils literal">msg</tt>
is dropped (terminates the propagation).</li>
</ul>
</blockquote>
<p>Of course all these methods also accept messages. Compared to <tt class="docutils literal">Node</tt>
methods they have a new <tt class="docutils literal">msg</tt> argument. The <tt class="docutils literal">target</tt> part on the
other hand is only used by the <tt class="docutils literal">BiFlow</tt>.</p>
<p>As you can see from <tt class="docutils literal">train</tt>, the training does not always stop when
the training node is reached. Instead it is possible to continue with
the execution to come back later. For example this is used in the
backpropagation example (in the MDP examples repository). There is also
a new <tt class="docutils literal">stop_message</tt> message. If <tt class="docutils literal">stop_training</tt> returns a result
then the <tt class="docutils literal">BiFlow</tt> enters a mode where it propagates the result based
on the given target by calling <tt class="docutils literal">stop_message</tt>. This can be used to
propagate results from the node training or to prepare nodes for their
upcoming training.</p>
<p>Some of these new options might be confusing at first. However, you
can simply ignore those that you don't need and concentrate on the
features that are useful for your current project. For example you can
use messages without ever worrying about targets.</p>
<p>There are three more additions to the <tt class="docutils literal">BiNode</tt> API:</p>
<blockquote>
<dl class="docutils">
<dt><tt class="docutils literal">node_id</tt></dt>
<dd>This is a read-only property, which returns the node id
(which is <tt class="docutils literal">None</tt> if it wasn't specified). Note that the <tt class="docutils literal">__init__</tt>
method of <tt class="docutils literal">BiNodes</tt> generally accepts a <tt class="docutils literal">node_id</tt> keyword argument.</dd>
<dt><tt class="docutils literal">bi_reset</tt></dt>
<dd>This method is called by the <tt class="docutils literal">BiFlow</tt> before and after training and
execution (and the <tt class="docutils literal">stop_training</tt> / <tt class="docutils literal">stop_message</tt> propagation). It
can be overriden by derived classes to reset internal state variables.</dd>
<dt><tt class="docutils literal">is_bi_training</tt></dt>
<dd>This method is similar to the <tt class="docutils literal">is_training</tt> method of stndard MDP nodes.
It can be used to signal that a node is doing some data gathering. A node
might for example do some training during the normal execute (e.g., a
neural network might adjust internal weights while it is already returning
results). Generally this method isn't that important, but the
<tt class="docutils literal">ParallelBiFlow</tt> uses it to determine if nodes can simply be copied or
must be forked</dd>
</dl>
</blockquote>
</div>
<div class="section" id="inspection">
<h2><a class="toc-backref" href="#id44">Inspection</a></h2>
<p>Using jumps and messages can result in complex data flows. Therefore
BiMDP offers some convenient inspection capabilites to help with
debugging and analyzing what is going on. This functionality is based on
the static HTML view from the <tt class="docutils literal">mdp.hinet</tt> module. Instead of a static
view of the flow you get an animated slideshow of the flow training or
execution. An example is provided in
<tt class="docutils literal">bimdp/test/demo_hinet_inspection.py</tt>. You can simply call
<tt class="docutils literal">bimdp.show_execution(flow, data)</tt> instead of the normal
<tt class="docutils literal">flow.execute(data)</tt>. This will automatically perform the inspection
and open it in your webbrowser. Similar functionality is available for
training. Just call <tt class="docutils literal">bimdp.show_execution(flow, data_iterables)</tt>,
which will perform training as in <tt class="docutils literal">flow.train(data_iterables)</tt>. Have a
look at the docstrings to learn about additional options.</p>
<blockquote>
<img alt="bimdp inspection example" src="bimdp_inspection.png" style="width: 550px;" />
</blockquote>
<p>The BiMDP inspection is also useful to visualize the data processing
that is happening inside a flow. This is especially handy if you are
trying to build or understand new algorithms and want to know what is
going on. Therefore we made it very easy to customize the HTML views in
the inspection. One simple example is provided in
<tt class="docutils literal">bimdp/test/demo_custom_inspection.py</tt>, where we use matplotlib to
plot the data and present it inside the HTML view. Note that
<tt class="docutils literal">bimdp.show_training</tt> and <tt class="docutils literal">bimdp.show_execution</tt> are just helper
functions. If you need more flexibility you can directly access the
machinery below (but this is rather messy and hardly ever needed).</p>
</div>
<div class="section" id="extending-binode-and-message-handling">
<h2><a class="toc-backref" href="#id45">Extending BiNode and Message Handling</a></h2>
<p>As in the <tt class="docutils literal">Node</tt> class any derived <tt class="docutils literal">BiNode</tt> classes should not
directly overwrite the public <tt class="docutils literal">execute</tt> or <tt class="docutils literal">train</tt> methods but
instead the private versions with an underscore in front (for training
you can of course also overwrite <tt class="docutils literal">_get_train_seq</tt>). In addtion to the
dimensionality checks performed on <tt class="docutils literal">x</tt> by the <tt class="docutils literal">Node</tt> class this
enables a couple of message handling features. This also applies to the
new <tt class="docutils literal">_stop_message</tt> method. On the other hand <tt class="docutils literal">bi_reset</tt> and
<tt class="docutils literal">is_bi_training</tt> can be directly overwritten (like <tt class="docutils literal">is_training</tt> in
<tt class="docutils literal">Node</tt>), there are no private methods for these.</p>
<p>The automatic message handling is a major feature in <tt class="docutils literal">BiNode</tt> and
relies on the dynamic nature of Python. In the <tt class="docutils literal">FDABiNode</tt> and
<tt class="docutils literal">BiFlow</tt> example we have alrady seen how a value from the message is
automatically passed to the <tt class="docutils literal">_train</tt> method, because the key of the
value is also the name of a keyword argument.</p>
<p>Public methods like <tt class="docutils literal">execute</tt> in <tt class="docutils literal">BiNode</tt> accept not only a data
array <tt class="docutils literal">x</tt>, but also a message dictionary <tt class="docutils literal">msg</tt>. When given a message
they perform introspection to determine the arguments for the
corresponding private methods (like <tt class="docutils literal">_train</tt>). If there is a matching
key for an argument in the message then the value is provided as a
keyword argument. It remains in the dictionary and can therefore be used
by other nodes in the flow as well.</p>
<p>A private method like <tt class="docutils literal">_train</tt> has the same return options as the
public <tt class="docutils literal">train</tt> method, so one can for example return a tupple <tt class="docutils literal">(x,
msg)</tt>. The <tt class="docutils literal">msg</tt> in the return value from <tt class="docutils literal">_train</tt> is then used by
<tt class="docutils literal">train</tt> to update the original <tt class="docutils literal">msg</tt>. Thereby <tt class="docutils literal">_train</tt> can
overwrite or add new values to the message. There are also some special
features (&quot;magic&quot;) to make handling messages more convenient:</p>
<ul class="simple">
<li>You can use message keys of the form <tt class="docutils literal"><span class="pre">node_id-&gt;argument_key</span></tt> to
address parts of the message to a specific node. When the node with the
corresponding id is reached then the value is not only provided as an
argument, but the key is also deleted from the message. If the
argument_key is not an argument of the method then the whole key is
simply erased.</li>
<li>If a private method like <tt class="docutils literal">_train</tt> has a keyword argument called
<tt class="docutils literal">msg</tt> then the complete message is provided. The message from the
return value replaces the original message in this case. For example
this makes it possible to delete parts of the message (instead of just
updating them with new values).</li>
<li>The key <tt class="docutils literal">&quot;method&quot;</tt> is treated in a special way. Instead of calling the
standard private method like <tt class="docutils literal">_train</tt> (or <tt class="docutils literal">_execute</tt>, depending on the
called public method) the &quot;method&quot; value will be used as the method
name, with an underscore in front. For example the message <tt class="docutils literal">{&quot;method&quot;:
&quot;classify&quot;}</tt> has the effect that a method <tt class="docutils literal">_classify</tt> will be called.
Note that this feature can be combined with the extension mechanism,
when methods are added at runtime.</li>
<li>The key <tt class="docutils literal">&quot;target&quot;</tt> is treated in a special way. If the called private
method does not return a target value (e.g., if it just returned <tt class="docutils literal">x</tt>)
then the <tt class="docutils literal">&quot;target&quot;</tt> value is used as target return value (e.g, instead of
<tt class="docutils literal">x</tt> the return value of <tt class="docutils literal">execute</tt> would then have the form <tt class="docutils literal">x,
None, target</tt>).</li>
<li>If the key &quot;method&quot; has the value <tt class="docutils literal">inverse</tt> then, as expected, the
<tt class="docutils literal">_inverse</tt> method is called. However, additionally the checks from
<tt class="docutils literal">inverse</tt> are run on the data array. If <tt class="docutils literal">_inverse</tt> does not return a
target value then the target -1 is returned. So with the message
<tt class="docutils literal">{&quot;method&quot;: &quot;inverse&quot;}</tt> one can execute a <tt class="docutils literal">BiFlow</tt> in inverse node
(note that one also has to provide the last node in the flow as the
initial target to the flow).</li>
<li>This more of a <tt class="docutils literal">BiFlow</tt> feature, but the target value specified in
<tt class="docutils literal">bimdp.EXIT_TARGET</tt> (currently set to <tt class="docutils literal">&quot;exit&quot;</tt>) causes <tt class="docutils literal">BiFlow</tt> to
terminate the execution and to return the last return value.</li>
<li>To make it possible to call <tt class="docutils literal">execute</tt> and
<tt class="docutils literal">inverse</tt> via <tt class="docutils literal">stop_message</tt> there is some magic going on if these are
specified via the <tt class="docutils literal">&quot;method&quot;</tt> key: In addition to the normal automatic
extraction of an <tt class="docutils literal">&quot;x&quot;</tt> key from the message the array output of the node
is also stored back as <tt class="docutils literal">&quot;x&quot;</tt> in the message (overwriting the previous
value). Additionally the target is given a default value of 1 or -1
(so setting the <tt class="docutils literal">&quot;method&quot;</tt> value is sufficient for normal execution
or inverse during the <tt class="docutils literal">stop_message</tt> phase).</li>
</ul>
<p>Of course all these features can be combined, or can be ignored when they
are not needed.</p>
</div>
<div class="section" id="hinet-in-bimdp">
<h2><a class="toc-backref" href="#id46">HiNet in BiMDP</a></h2>
<p>BiMDP is mostly compatibel with the hierarchical networks introduced in
<tt class="docutils literal">mdp.hinet</tt>. For the full BiMDP functionality it is of of course
required to use the BiMDP versions of the the building blocks.</p>
<p>The <tt class="docutils literal">bimdp.hinet</tt> module provides a <tt class="docutils literal">BiFlowNode</tt> class, which is
offers the same functionality as a <tt class="docutils literal">FlowNode</tt> but with the added
capability of handling messages, targets, and all other BiMDP concepts.</p>
<p>There is also a new <tt class="docutils literal">BiSwitchboard</tt> base class, which is able to deal
with messages. Arrays present in the message are mapped with the
switchboard routing if the second axis matches the switchboard dimension
(this works for both execute and inverse).</p>
<p>Finally there is a <tt class="docutils literal">CloneBiLayer</tt> class, which is the BiMDP version of
the <tt class="docutils literal">CloneLayer</tt> class in <tt class="docutils literal">mdp.hinet</tt>. To support all the features
of BiMDP some significant functionality has been added to this class.
The most important new aspect is the <tt class="docutils literal">use_copies</tt> property. If it is
set to <tt class="docutils literal">True</tt> then multiple deep copies are used instead of just a
reference to the same node. This makes it possible to use internal
variables in a node that persist while the node is left and later
reentered. You can set this property as often as you like (note that
there is of course some overhead for the deep copying). You can also set
the <tt class="docutils literal">use_copies</tt> property via the message mechanism by simply adding a
<tt class="docutils literal">&quot;use_copies&quot;</tt> with the required boolean value. The <tt class="docutils literal">CloneBiLayer</tt>
class also looks for this key in outgoing messages (so it can be send
from nodes inside the layer).</p>
</div>
<div class="section" id="parallel-in-bimdp">
<h2><a class="toc-backref" href="#id47">Parallel in BiMDP</a></h2>
<p>The parallelisation capabilites introduced in <tt class="docutils literal">mdp.parallel</tt> can be
also used for BiMDP. The <tt class="docutils literal">bimdp.parallel</tt> module provides a
<tt class="docutils literal">ParallelBiFlow</tt> class which can be used like the normal
<tt class="docutils literal">ParallelFlow</tt>. No changes to schedulers are required.</p>
<p>The most important difference between the parallelization in standard
MDP and BiMDP is that BiNodes can signal via the <tt class="docutils literal">is_bi_training</tt>
method wether they should be forked instead of the usual deep copy.
Unlike the <tt class="docutils literal">is_training</tt> method there can be multiple nodes for which
<tt class="docutils literal">is_bi_training</tt> returns <tt class="docutils literal">True</tt>. All these forked nodes are of
course also correctly joined.</p>
<p>Note that a <tt class="docutils literal">ParallelBiFlow</tt> uses a special callable class. So if you
want to use a custom callable you will have to make a few modifications
(compared to the standard callable class used by <tt class="docutils literal">ParallFlow</tt>).</p>
</div>
</div>
<div class="section" id="future-development">
<h1><a class="toc-backref" href="#id48">Future Development</a></h1>
<p>MDP is currently maintained by a core team of 4 developers, but it is
open to user contributions. Users have already contributed some of the
nodes, and more contributions are currently being reviewed for
inclusion in future releases of the package. The package development
can be followed online on the public git code
<a class="reference external" href="http://mdp-toolkit.git.sourceforge.net">repositories</a> or cloned with:</p>
<pre class="literal-block">
git clone git://mdp-toolkit.git.sourceforge.net/gitroot/mdp-toolkit/mdp-toolkit
git clone git://mdp-toolkit.git.sourceforge.net/gitroot/mdp-toolkit/docs
git clone git://mdp-toolkit.git.sourceforge.net/gitroot/mdp-toolkit/examples
git clone git://mdp-toolkit.git.sourceforge.net/gitroot/mdp-toolkit/contrib
</pre>
<p>Questions, bug reports, and feature requests are typically handled by
the user <a class="reference external" href="http://sourceforge.net/mail/?group_id=116959">mailing list</a></p>
</div>
<div class="section" id="contributors">
<h1><a class="toc-backref" href="#id49">Contributors</a></h1>
<p>In this final section we want to thank all users who have contributed
code and bug reports to the MDP project. Strictly in alphabetical order:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.gbeckers.nl/">Gabriel Beckers</a></li>
<li>Alberto Escalante</li>
<li><a class="reference external" href="http://www.bccn-berlin.de/People/farkhooi">Farzad Farkhooi</a></li>
<li>Mathias Franzius</li>
<li><a class="reference external" href="http://apsy.gse.uni-magdeburg.de/main/index.psp?page=hanke/main&amp;lang=en&amp;sec=0">Michael Hanke</a></li>
<li><a class="reference external" href="http://dirac.cnrs-orleans.fr/~hinsen/">Konrad Hinsen</a></li>
<li><a class="reference external" href="http://www.samueljohn.de/">Samuel John</a></li>
<li>Susanne Lezius</li>
<li><a class="reference external" href="http://userpage.fu-berlin.de/~schmuker/">Michael Schmuker</a></li>
<li><a class="reference external" href="http://www.astro.washington.edu/vanderplas/">Jake VanderPlas</a></li>
</ul>
</div>
</div>
<div class="footer">
<hr class="footer" />
Generated on: 2010-05-07 10:51 UTC.
Generated by <a class="reference external" href="http://docutils.sourceforge.net/">Docutils</a> from <a class="reference external" href="http://docutils.sourceforge.net/rst.html">reStructuredText</a> source.

</div>
</div>

<div id="footer">

<hr />
<a href="http://sourceforge.net/projects/mdp-toolkit"> <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=116959&amp;type=12" width="88" height="31" border="0" alt="Get Modular toolkit for Data Processing MDP at SourceForge.net. Fast, secure and Free Open Source software downloads" /></a>

<a href="http://validator.w3.org/check?uri=referer;verbose=1"><img border="0" src="valid-html401.png" alt="Valid HTML 4.01!" height="31" width="88" /></a>
<!-- Piwik -->
<script type="text/javascript">
var pkBaseURL = (("https:" == document.location.protocol) ? "https://apps.sourceforge.net/piwik/mdp-toolkit/" : "http://apps.sourceforge.net/piwik/mdp-toolkit/");
document.write(unescape("%3Cscript src='" + pkBaseURL + "piwik.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
piwik_action_name = '';
piwik_idsite = 1;
piwik_url = pkBaseURL + "piwik.php";
piwik_log(piwik_action_name, piwik_idsite, piwik_url);
</script>
<object><noscript><p><img src="http://apps.sourceforge.net/piwik/mdp-toolkit/piwik.php?idsite=1" alt="piwik"/></p></noscript></object>
<!-- End Piwik Tag -->
<script language="javascript" type="text/javascript">
<!--
document.write("<p />Last modified: "+document.lastModified+"")
-->
</script>

</div>

</div>

</body>
</html>

