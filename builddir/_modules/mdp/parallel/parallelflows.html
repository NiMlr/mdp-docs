

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mdp.parallel.parallelflows &mdash; Modular toolkit for Data Processing (MDP)</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="Modular toolkit for Data Processing (MDP)" href="../../../index.html"/>
        <link rel="up" title="mdp.parallel" href="../parallel.html"/> 
<meta name="viewport" content="width=740" />


  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/logo_animation.gif" class="logo" />
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../documentation.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../how_to_cite_mdp.html">How to cite MDP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contact.html">Contact</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MDP</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../../mdp.html">mdp</a> &raquo;</li>
        
          <li><a href="../parallel.html">mdp.parallel</a> &raquo;</li>
        
      <li>mdp.parallel.parallelflows</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mdp.parallel.parallelflows</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Module for parallel flows that can handle the parallel training / execution.</span>

<span class="sd">Corresponding classes for task callables and ResultContainer are defined here</span>
<span class="sd">as well.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="k">import</span> <span class="nb">zip</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="k">import</span> <span class="nb">str</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="k">import</span> <span class="nb">next</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="k">import</span> <span class="nb">range</span>

<span class="kn">import</span> <span class="nn">mdp</span>
<span class="kn">from</span> <span class="nn">mdp</span> <span class="k">import</span> <span class="n">numx</span> <span class="k">as</span> <span class="n">n</span>

<span class="kn">from</span> <span class="nn">.parallelnodes</span> <span class="k">import</span> <span class="n">NotForkableParallelException</span>
<span class="kn">from</span> <span class="nn">.scheduling</span> <span class="k">import</span> <span class="p">(</span>
    <span class="n">TaskCallable</span><span class="p">,</span> <span class="n">ResultContainer</span><span class="p">,</span> <span class="n">OrderedResultContainer</span><span class="p">,</span> <span class="n">Scheduler</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mdp.hinet</span> <span class="k">import</span> <span class="n">FlowNode</span>


<span class="c1">### Helper code for node purging before transport. ###</span>

<div class="viewcode-block" id="_DummyNode"><a class="viewcode-back" href="../../../mdp/mdp.parallel.parallelflows.html#mdp.parallel._DummyNode">[docs]</a><span class="k">class</span> <span class="nc">_DummyNode</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dummy node class for empty nodes.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="_DummyNode.is_trainable"><a class="viewcode-back" href="../../../mdp/mdp.parallel.parallelflows.html#mdp.parallel._DummyNode.is_trainable">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_trainable</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">False</span></div>
    
<div class="viewcode-block" id="_DummyNode._execute"><a class="viewcode-back" href="../../../mdp/mdp.parallel.parallelflows.html#mdp.parallel._DummyNode._execute">[docs]</a>    <span class="k">def</span> <span class="nf">_execute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">err</span> <span class="o">=</span> <span class="s2">&quot;This is only a dummy created by &#39;parallel._purge_flownode&#39;.&quot;</span>
        <span class="k">raise</span> <span class="n">mdp</span><span class="o">.</span><span class="n">NodeException</span><span class="p">(</span><span class="n">err</span><span class="p">)</span></div></div>


<span class="n">_DUMMY_NODE</span> <span class="o">=</span> <span class="n">_DummyNode</span><span class="p">()</span>

<div class="viewcode-block" id="_purge_flownode"><a class="viewcode-back" href="../../../mdp/mdp.parallel.parallelflows.html#mdp.parallel._purge_flownode">[docs]</a><span class="k">def</span> <span class="nf">_purge_flownode</span><span class="p">(</span><span class="n">flownode</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replace nodes that are &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i_node</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flownode</span><span class="o">.</span><span class="n">_flow</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">_train_phase_started</span> <span class="ow">or</span> <span class="n">node</span><span class="o">.</span><span class="n">use_execute_fork</span><span class="p">()):</span>
            <span class="n">flownode</span><span class="o">.</span><span class="n">_flow</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="n">i_node</span><span class="p">]</span> <span class="o">=</span> <span class="n">_DUMMY_NODE</span></div>


<span class="c1">### Train task classes ###</span>

<span class="k">class</span> <span class="nc">FlowTaskCallable</span><span class="p">(</span><span class="n">TaskCallable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for all flow callables.</span>

<span class="sd">    It deals activating the required extensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Store the currently active extensions.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_used_extensions</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">get_active_extensions</span><span class="p">()</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlowTaskCallable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_environment</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Activate the used extensions.&quot;&quot;&quot;</span>
        <span class="c1"># deactivate all active extensions for safety</span>
        <span class="n">mdp</span><span class="o">.</span><span class="n">deactivate_extensions</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">get_active_extensions</span><span class="p">())</span>
        <span class="n">mdp</span><span class="o">.</span><span class="n">activate_extensions</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_used_extensions</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FlowTrainCallable</span><span class="p">(</span><span class="n">FlowTaskCallable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements a single training phase in a flow for a data block.</span>

<span class="sd">    A FlowNode is used to simplify the forking process and to</span>
<span class="sd">    encapsulate the flow.</span>

<span class="sd">    You can also derive from this class to define your own callable class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flownode</span><span class="p">,</span> <span class="n">purge_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Store everything for the training.</span>

<span class="sd">        keyword arguments:</span>
<span class="sd">        flownode -- FlowNode containing the flow to be trained.</span>
<span class="sd">        purge_nodes -- If True nodes not needed for the join will be replaced</span>
<span class="sd">            with dummy nodes to reduce the footprint.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="n">flownode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_purge_nodes</span> <span class="o">=</span> <span class="n">purge_nodes</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlowTrainCallable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do the training and return only the trained node.</span>

<span class="sd">        data -- training data block (array or list if additional arguments are</span>
<span class="sd">            required)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">is</span> <span class="n">n</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># note the local training in ParallelFlow relies on the flownode</span>
        <span class="c1"># being preserved, so derived classes should preserve it as well</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_purge_nodes</span><span class="p">:</span>
            <span class="n">_purge_flownode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span>

    <span class="k">def</span> <span class="nf">fork</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">fork</span><span class="p">(),</span>
                              <span class="n">purge_nodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_purge_nodes</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TrainResultContainer</span><span class="p">(</span><span class="n">ResultContainer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Container for parallel nodes.</span>

<span class="sd">    Expects flownodes as results and joins them to save memory.</span>
<span class="sd">    A list containing one flownode is returned, so this container can replace</span>
<span class="sd">    the standard list container without any changes elsewhere.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainResultContainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">add_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">task_index</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="n">result</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_results</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">flownode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">flownode</span><span class="p">,]</span>
    

<span class="c1">### Execute task classes ###</span>

<span class="k">class</span> <span class="nc">FlowExecuteCallable</span><span class="p">(</span><span class="n">FlowTaskCallable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements data execution through a Flow.</span>
<span class="sd">    </span>
<span class="sd">    A FlowNode is used to simplify the forking process and to</span>
<span class="sd">    encapsulate the flow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flownode</span><span class="p">,</span> <span class="n">nodenr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">purge_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Store everything for the execution.</span>

<span class="sd">        flownode -- FlowNode for the execution</span>
<span class="sd">        nodenr -- optional nodenr argument for the flow execute method</span>
<span class="sd">        purge_nodes -- If True nodes not needed for the join will be replaced</span>
<span class="sd">            with dummy nodes to reduce the footprint.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="n">flownode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_nodenr</span> <span class="o">=</span> <span class="n">nodenr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_purge_nodes</span> <span class="o">=</span> <span class="n">purge_nodes</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlowExecuteCallable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the execution result.</span>

<span class="sd">        x -- data chunk</span>
<span class="sd">        </span>
<span class="sd">        If use_fork_execute is True for the flownode then it is returned</span>
<span class="sd">        in the result tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nodenr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodenr</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">use_execute_fork</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_purge_nodes</span><span class="p">:</span>
                <span class="n">_purge_flownode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fork</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">fork</span><span class="p">(),</span> <span class="n">nodenr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodenr</span><span class="p">,</span>
                              <span class="n">purge_nodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_purge_nodes</span><span class="p">)</span>
    

<span class="k">class</span> <span class="nc">ExecuteResultContainer</span><span class="p">(</span><span class="n">OrderedResultContainer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Default result container with automatic restoring of the result order.</span>

<span class="sd">    This result container should be used together with BiFlowExecuteCallable.</span>
<span class="sd">    Both the execute result (x and possibly msg) and the forked BiFlowNode</span>
<span class="sd">    are stored.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize attributes.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExecuteResultContainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">add_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">task_index</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Remove the forked BiFlowNode from the result and join it.&quot;&quot;&quot;</span>
        <span class="n">excecute_result</span><span class="p">,</span> <span class="n">forked_flownode</span> <span class="o">=</span> <span class="n">result</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExecuteResultContainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">add_result</span><span class="p">(</span><span class="n">excecute_result</span><span class="p">,</span>
                                                       <span class="n">task_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">forked_flownode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="n">forked_flownode</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">forked_flownode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_results</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the ordered results.</span>

<span class="sd">        The joined BiFlowNode is returned in the first result list entry,</span>
<span class="sd">        for the following result entries BiFlowNode is set to None.</span>
<span class="sd">        This reduces memory consumption while staying transparent for the</span>
<span class="sd">        ParallelBiFlow.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">excecute_results</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">ExecuteResultContainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_results</span><span class="p">()</span>
        <span class="n">flownode_results</span> <span class="o">=</span> <span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">,]</span>
                              <span class="o">+</span> <span class="p">([</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">excecute_results</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">excecute_results</span><span class="p">,</span> <span class="n">flownode_results</span><span class="p">))</span>

<span class="c1">### ParallelFlow Class ###</span>

<span class="k">class</span> <span class="nc">ParallelFlowException</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">FlowException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Standard exception for problems with ParallelFlow.&quot;&quot;&quot;</span>
    <span class="k">pass</span>


<span class="k">class</span> <span class="nc">NoTaskException</span><span class="p">(</span><span class="n">ParallelFlowException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Exception for problems with the task creation.&quot;&quot;&quot;</span>
    <span class="k">pass</span>


<span class="k">class</span> <span class="nc">ParallelFlow</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">Flow</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A parallel flow provides the methods for parallel training / execution.</span>

<span class="sd">    Nodes in the flow which are not derived from ParallelNode are trained in</span>
<span class="sd">    the normal way. The training is also done normally if fork() raises a</span>
<span class="sd">    TrainingPhaseNotParallelException. This can be intentionally used by the</span>
<span class="sd">    node to request local training without forking.</span>
<span class="sd">    Parallel execution on the other hand should work for all nodes, since it</span>
<span class="sd">    only relies on the copy method of nodes.</span>
<span class="sd">    The stop_training method is always called locally, with no forking or</span>
<span class="sd">    copying involved.</span>

<span class="sd">    Both parallel training and execution can be done conveniently by providing</span>
<span class="sd">    a scheduler instance to the train or execute method.</span>
<span class="sd">    It is also possible to manage the tasks manually. This is done via the</span>
<span class="sd">    methods setup_parallel_training (or execution), get_task and use_results.</span>
<span class="sd">    The code of the train / execute method can serve as an example how to use</span>
<span class="sd">    these methods and process the tasks by a scheduler.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the internal variables.</span>

<span class="sd">        Note that the crash_recovery flag is is not supported, so it is</span>
<span class="sd">        disabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;crash_recovery&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">flow</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                                           <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterables</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># all training data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterator</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># iterator for current training</span>
        <span class="c1"># index of currently trained node, also used as flag for training</span>
        <span class="c1"># takes value None for not training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># used during training</span>
        <span class="c1"># iterable for execution data</span>
        <span class="c1"># also signals if parallel execution is underway</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># buffer for next task</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_callable_class</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_execute_callable_class</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@mdp</span><span class="o">.</span><span class="n">with_extension</span><span class="p">(</span><span class="s2">&quot;parallel&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_iterables</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">train_callable_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">overwrite_result_container</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train all trainable nodes in the flow.</span>

<span class="sd">        If a scheduler is provided the training will be done in parallel on the</span>
<span class="sd">        scheduler.</span>

<span class="sd">        data_iterables -- A list of iterables, one for each node in the flow.</span>
<span class="sd">            The iterators returned by the iterables must</span>
<span class="sd">            return data arrays that are then used for the node training.</span>
<span class="sd">            See Flow.train for more details.</span>
<span class="sd">            If a custom train_callable_class is used to preprocess the data</span>
<span class="sd">            then other data types can be used as well.</span>
<span class="sd">        scheduler -- Value can be either None for normal training (default</span>
<span class="sd">            value) or a Scheduler instance for parallel training with the</span>
<span class="sd">            scheduler.</span>
<span class="sd">            If the scheduler value is an iterable or iterator then it is</span>
<span class="sd">            assumed that it contains a scheduler for each training phase.</span>
<span class="sd">            After a node has been trained the scheduler is shutdown. Note that</span>
<span class="sd">            you can e.g. use a generator to create the schedulers just in time.</span>
<span class="sd">            For nodes which are not trained the scheduler can be None.</span>
<span class="sd">        train_callable_class -- Class used to create training callables for the</span>
<span class="sd">            scheduler. By specifying your own class you can implement data</span>
<span class="sd">            transformations before the data is actually fed into the flow</span>
<span class="sd">            (e.g. from 8 bit image to 64 bit double precision).</span>
<span class="sd">            Note that the train_callable_class is only used if a scheduler was</span>
<span class="sd">            provided. By default NodeResultContainer is used.</span>
<span class="sd">        overwrite_result_container -- If set to True (default value) then</span>
<span class="sd">            the result container in the scheduler will be overwritten with an</span>
<span class="sd">            instance of NodeResultContainer (unless it is already an instance</span>
<span class="sd">            of NodeResultContainer). This improves the memory efficiency.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Warning: If this method is updated you also have to update train</span>
        <span class="c1">#          in ParallelCheckpointFlow.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">ParallelFlowException</span><span class="p">(</span><span class="s2">&quot;Parallel training is underway.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_callable_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A train_callable_class was specified but no scheduler &quot;</span>
                       <span class="s2">&quot;was given, so the train_callable_class has no effect.&quot;</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">ParallelFlowException</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">ParallelFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data_iterables</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_callable_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">train_callable_class</span> <span class="o">=</span> <span class="n">FlowTrainCallable</span>
            <span class="n">schedulers</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># do parallel training</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_parallel_training</span><span class="p">(</span>
                                    <span class="n">data_iterables</span><span class="p">,</span>
                                    <span class="n">train_callable_class</span><span class="o">=</span><span class="n">train_callable_class</span><span class="p">,</span>
                                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="c1"># prepare scheduler</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">Scheduler</span><span class="p">):</span>
                    <span class="c1"># scheduler contains an iterable with the schedulers</span>
                    <span class="c1"># self._i_train_node was set in setup_parallel_training</span>
                    <span class="n">schedulers</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
                    <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># dispose schedulers for pretrained nodes</span>
                        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">scheduler</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
                            <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># all nodes are already trained, dispose schedulers</span>
                        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">scheduler</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
                            <span class="c1"># the last scheduler will be shutdown in finally</span>
                            <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span>
                    <span class="n">last_trained_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">schedulers</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="c1"># check that the scheduler is compatible</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span>
                    <span class="n">overwrite_result_container</span> <span class="ow">and</span>
                    <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">result_container</span><span class="p">,</span>
                                    <span class="n">TrainResultContainer</span><span class="p">))):</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">result_container</span> <span class="o">=</span> <span class="n">TrainResultContainer</span><span class="p">()</span>
                <span class="c1">## train all nodes</span>
                <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_training</span><span class="p">:</span>
                    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_available</span><span class="p">:</span>
                        <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_task</span><span class="p">()</span>
                        <span class="n">scheduler</span><span class="o">.</span><span class="n">add_task</span><span class="p">(</span><span class="o">*</span><span class="n">task</span><span class="p">)</span>
                    <span class="n">results</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_results</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">results</span> <span class="o">==</span> <span class="p">[]:</span>
                        <span class="n">err</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Could not get any training tasks or results &quot;</span>
                               <span class="s2">&quot;for the current training phase.&quot;</span><span class="p">)</span>
                        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">use_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
                    <span class="c1"># check if we have to switch to next scheduler</span>
                    <span class="k">if</span> <span class="p">((</span><span class="n">schedulers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span>
                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span>
                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">&gt;</span> <span class="n">last_trained_node</span><span class="p">)):</span>
                        <span class="c1"># dispose unused schedulers</span>
                        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">-</span> <span class="n">last_trained_node</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">scheduler</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
                            <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span>
                        <span class="n">last_trained_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span>
                        <span class="c1"># check that the scheduler is compatible</span>
                        <span class="k">if</span> <span class="p">((</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span>
                            <span class="n">overwrite_result_container</span> <span class="ow">and</span>
                            <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">result_container</span><span class="p">,</span>
                                            <span class="n">TrainResultContainer</span><span class="p">))):</span>
                            <span class="n">scheduler</span><span class="o">.</span><span class="n">result_container</span> <span class="o">=</span> <span class="n">TrainResultContainer</span><span class="p">()</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="c1"># reset iterable references, which cannot be pickled</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterables</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterator</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">schedulers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_parallel_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_iterables</span><span class="p">,</span>
                                <span class="n">train_callable_class</span><span class="o">=</span><span class="n">FlowTrainCallable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prepare the flow for handing out tasks to do the training.</span>

<span class="sd">        After calling setup_parallel_training one has to pick up the</span>
<span class="sd">        tasks with get_task, run them and finally return the results via</span>
<span class="sd">        use_results. tasks are available as long as task_available returns</span>
<span class="sd">        True. Training may require multiple phases, which are each closed by</span>
<span class="sd">        calling use_results.</span>

<span class="sd">        data_iterables -- A list of iterables, one for each node in the flow.</span>
<span class="sd">            The iterators returned by the iterables must</span>
<span class="sd">            return data arrays that are then used for the node training.</span>
<span class="sd">            See Flow.train for more details.</span>
<span class="sd">            If a custom train_callable_class is used to preprocess the data</span>
<span class="sd">            then other data types can be used as well.</span>
<span class="sd">        train_callable_class -- Class used to create training callables for the</span>
<span class="sd">            scheduler. By specifying your own class you can implement data</span>
<span class="sd">            transformations before the data is actually fed into the flow</span>
<span class="sd">            (e.g. from 8 bit image to 64 bit double precision).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_training</span><span class="p">:</span>
            <span class="n">err</span> <span class="o">=</span> <span class="s2">&quot;Parallel training is already underway.&quot;</span>
            <span class="k">raise</span> <span class="n">ParallelFlowException</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_callable_class</span> <span class="o">=</span> <span class="n">train_callable_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_check_iterables</span><span class="p">(</span><span class="n">data_iterables</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="n">FlowNode</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_next_train_phase</span><span class="p">()</span>

<div class="viewcode-block" id="ParallelFlow._next_train_phase"><a class="viewcode-back" href="../../../mdp/mdp.parallel.html#mdp.parallel.ParallelFlow._next_train_phase">[docs]</a>    <span class="k">def</span> <span class="nf">_next_train_phase</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Find the next phase or node for parallel training.</span>

<span class="sd">        When it is found the corresponding internal variables are set.</span>
<span class="sd">        Nodes which are not derived from ParallelNode are trained locally.</span>
<span class="sd">        If a fork() fails due to a TrainingPhaseNotParallelException</span>
<span class="sd">        in a certain train phase, then the training is done locally as well</span>
<span class="sd">        (but fork() is tested again for the next phase).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># find next node that can be forked, if required do local training</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">):</span>
            <span class="n">current_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">current_node</span><span class="o">.</span><span class="n">is_training</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">continue</span>
            <span class="n">data_iterable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterables</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">fork</span><span class="p">()</span>
                <span class="c1"># fork successful, prepare parallel training</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;start parallel training phase of &quot;</span> <span class="o">+</span>
                           <span class="s2">&quot;node no. </span><span class="si">%d</span><span class="s2"> in parallel flow&quot;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_iterable</span><span class="p">)</span>
                <span class="n">first_task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_train_task</span><span class="p">()</span>
                <span class="c1"># make sure that the iterator is not empty</span>
                <span class="k">if</span> <span class="n">first_task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">current_node</span><span class="o">.</span><span class="n">get_current_train_phase</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">err_str</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The training data iteration for node &quot;</span>
                                   <span class="s2">&quot;no. </span><span class="si">%d</span><span class="s2"> could not be repeated for the &quot;</span>
                                   <span class="s2">&quot;second training phase, you probably &quot;</span>
                                   <span class="s2">&quot;provided an iterator instead of an &quot;</span>
                                   <span class="s2">&quot;iterable.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                        <span class="k">raise</span> <span class="n">mdp</span><span class="o">.</span><span class="n">FlowException</span><span class="p">(</span><span class="n">err_str</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">err_str</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The training data iterator for node &quot;</span>
                                   <span class="s2">&quot;no. </span><span class="si">%d</span><span class="s2"> is empty.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                        <span class="k">raise</span> <span class="n">mdp</span><span class="o">.</span><span class="n">FlowException</span><span class="p">(</span><span class="n">err_str</span><span class="p">)</span>
                <span class="n">task_data_chunk</span> <span class="o">=</span> <span class="n">first_task</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># Only first task contains the new callable (enable caching).</span>
                <span class="c1"># A fork is not required here, since the callable is always</span>
                <span class="c1"># forked in the scheduler.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="o">=</span> <span class="p">(</span><span class="n">task_data_chunk</span><span class="p">,</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">_train_callable_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">))</span>
                <span class="k">break</span>
            <span class="k">except</span> <span class="n">NotForkableParallelException</span> <span class="k">as</span> <span class="n">exception</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;could not fork node no. </span><span class="si">%d</span><span class="s2">: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">exception</span><span class="p">)))</span>
                    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;start nonparallel training phase of &quot;</span> <span class="o">+</span>
                           <span class="s2">&quot;node no. </span><span class="si">%d</span><span class="s2"> in parallel flow&quot;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_local_train_phase</span><span class="p">(</span><span class="n">data_iterable</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;finished nonparallel training phase of &quot;</span> <span class="o">+</span>
                           <span class="s2">&quot;node no. </span><span class="si">%d</span><span class="s2"> in parallel flow&quot;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_stop_training_hook</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">stop_training</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_post_stop_training_hook</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="p">]</span><span class="o">.</span><span class="n">is_training</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># training is finished</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">=</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="ParallelFlow._local_train_phase"><a class="viewcode-back" href="../../../mdp/mdp.parallel.html#mdp.parallel.ParallelFlow._local_train_phase">[docs]</a>    <span class="k">def</span> <span class="nf">_local_train_phase</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_iterable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform a single training phase locally.</span>

<span class="sd">        The internal _train_callable_class is used for the training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">current_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="p">]</span>
        <span class="n">task_callable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_callable_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">,</span>
                                                   <span class="n">purge_nodes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">empty_iterator</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">i_task</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_iterable</span><span class="p">):</span>
            <span class="n">empty_iterator</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="c1"># Note: if x contains additional args assume that the</span>
            <span class="c1"># callable can handle this</span>
            <span class="n">task_callable</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;    finished nonparallel task no. </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i_task</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">empty_iterator</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">current_node</span><span class="o">.</span><span class="n">get_current_train_phase</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">err_str</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The training data iteration for node &quot;</span>
                           <span class="s2">&quot;no. </span><span class="si">%d</span><span class="s2"> could not be repeated for the &quot;</span>
                           <span class="s2">&quot;second training phase, you probably &quot;</span>
                           <span class="s2">&quot;provided an iterator instead of an &quot;</span>
                           <span class="s2">&quot;iterable.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">raise</span> <span class="n">mdp</span><span class="o">.</span><span class="n">FlowException</span><span class="p">(</span><span class="n">err_str</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">err_str</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The training data iterator for node &quot;</span>
                           <span class="s2">&quot;no. </span><span class="si">%d</span><span class="s2"> is empty.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">raise</span> <span class="n">mdp</span><span class="o">.</span><span class="n">FlowException</span><span class="p">(</span><span class="n">err_str</span><span class="p">)</span></div>

<div class="viewcode-block" id="ParallelFlow._post_stop_training_hook"><a class="viewcode-back" href="../../../mdp/mdp.parallel.html#mdp.parallel.ParallelFlow._post_stop_training_hook">[docs]</a>    <span class="k">def</span> <span class="nf">_post_stop_training_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Hook method that is called after stop_training is called.&quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="ParallelFlow._create_train_task"><a class="viewcode-back" href="../../../mdp/mdp.parallel.html#mdp.parallel.ParallelFlow._create_train_task">[docs]</a>    <span class="k">def</span> <span class="nf">_create_train_task</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create and return a single training task without callable.</span>

<span class="sd">        Returns None if data iterator end is reached.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_data_iterator</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span></div>

    <span class="nd">@mdp</span><span class="o">.</span><span class="n">with_extension</span><span class="p">(</span><span class="s2">&quot;parallel&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iterable</span><span class="p">,</span> <span class="n">nodenr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">execute_callable_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">overwrite_result_container</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train all trainable nodes in the flow.</span>

<span class="sd">        If a scheduler is provided the execution will be done in parallel on</span>
<span class="sd">        the scheduler.</span>

<span class="sd">        iterable -- An iterable or iterator that returns data arrays that are</span>
<span class="sd">            used as input to the flow. Alternatively, one can specify one</span>
<span class="sd">            data array as input.</span>
<span class="sd">            If a custom execute_callable_class is used to preprocess the data</span>
<span class="sd">            then other data types can be used as well.</span>
<span class="sd">        nodenr -- Same as in normal flow, the flow is only executed up to the</span>
<span class="sd">            nodenr.</span>
<span class="sd">        scheduler -- Value can be either None for normal execution (default</span>
<span class="sd">            value) or a Scheduler instance for parallel execution with the</span>
<span class="sd">            scheduler.</span>
<span class="sd">        execute_callable_class -- Class used to create execution callables for</span>
<span class="sd">            the scheduler. By specifying your own class you can implement data</span>
<span class="sd">            transformations before the data is actually fed into the flow</span>
<span class="sd">            (e.g. from 8 bit image to 64 bit double precision).</span>
<span class="sd">            Note that the execute_callable_class is only used if a scheduler was</span>
<span class="sd">            provided. If a scheduler is provided the default class used is</span>
<span class="sd">            NodeResultContainer.</span>
<span class="sd">        overwrite_result_container -- If set to True (default value) then</span>
<span class="sd">            the result container in the scheduler will be overwritten with an</span>
<span class="sd">            instance of OrderedResultContainer (unless it is already an</span>
<span class="sd">            instance of OrderedResultContainer). Otherwise the results might</span>
<span class="sd">            have a different order than the data chunks, which could mess up</span>
<span class="sd">            any subsequent analysis.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">ParallelFlowException</span><span class="p">(</span><span class="s2">&quot;Parallel training is underway.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">execute_callable_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A execute_callable_class was specified but no &quot;</span>
                       <span class="s2">&quot;scheduler was given, so the execute_callable_class &quot;</span>
                       <span class="s2">&quot;has no effect.&quot;</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">ParallelFlowException</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">ParallelFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">nodenr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">execute_callable_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">execute_callable_class</span> <span class="o">=</span> <span class="n">FlowExecuteCallable</span>
        <span class="c1"># check that the scheduler is compatible</span>
        <span class="k">if</span> <span class="n">overwrite_result_container</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">result_container</span><span class="p">,</span>
                              <span class="n">ExecuteResultContainer</span><span class="p">):</span>
                <span class="n">scheduler</span><span class="o">.</span><span class="n">result_container</span> <span class="o">=</span> <span class="n">ExecuteResultContainer</span><span class="p">()</span>
        <span class="c1"># do parallel execution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span> <span class="o">=</span> <span class="n">FlowNode</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup_parallel_execution</span><span class="p">(</span>
                                <span class="n">iterable</span><span class="p">,</span>
                                <span class="n">nodenr</span><span class="o">=</span><span class="n">nodenr</span><span class="p">,</span>
                                <span class="n">execute_callable_class</span><span class="o">=</span><span class="n">execute_callable_class</span><span class="p">)</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_available</span><span class="p">:</span>
                <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_task</span><span class="p">()</span>
                <span class="n">scheduler</span><span class="o">.</span><span class="n">add_task</span><span class="p">(</span><span class="o">*</span><span class="n">task</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_results</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_results</span><span class="p">())</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># reset remaining iterator references, which cannot be pickled</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">setup_parallel_execution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iterable</span><span class="p">,</span> <span class="n">nodenr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">execute_callable_class</span><span class="o">=</span><span class="n">FlowExecuteCallable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prepare the flow for handing out tasks to do the execution.</span>

<span class="sd">        After calling setup_parallel_execution one has to pick up the</span>
<span class="sd">        tasks with get_task, run them and finally return the results via</span>
<span class="sd">        use_results. use_results will then return the result as if the flow was</span>
<span class="sd">        executed in the normal way.</span>

<span class="sd">        iterable -- An iterable or iterator that returns data arrays that are</span>
<span class="sd">            used as input to the flow. Alternatively, one can specify one</span>
<span class="sd">            data array as input.</span>
<span class="sd">            If a custom execute_callable_class is used to preprocess the data</span>
<span class="sd">            then other data types can be used as well.</span>
<span class="sd">        nodenr -- Same as in normal flow, the flow is only executed up to the</span>
<span class="sd">            nodenr.</span>
<span class="sd">        execute_callable_class -- Class used to create execution callables for</span>
<span class="sd">            the scheduler. By specifying your own class you can implement data</span>
<span class="sd">            transformations before the data is actually fed into the flow</span>
<span class="sd">            (e.g. from 8 bit image to 64 bit double precision).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">ParallelFlowException</span><span class="p">(</span><span class="s2">&quot;Parallel training is underway.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_execute_callable_class</span> <span class="o">=</span> <span class="n">execute_callable_class</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iterable</span> <span class="o">=</span> <span class="p">[</span><span class="n">iterable</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
        <span class="n">first_task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_execute_task</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">first_task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">errstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The execute data iterator is empty.&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">mdp</span><span class="o">.</span><span class="n">FlowException</span><span class="p">(</span><span class="n">errstr</span><span class="p">)</span>
        <span class="n">task_data_chunk</span> <span class="o">=</span> <span class="n">first_task</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Only first task contains the new callable (enable caching).</span>
        <span class="c1"># A fork is not required here, since the callable is always</span>
        <span class="c1"># forked in the scheduler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="o">=</span> <span class="p">(</span><span class="n">task_data_chunk</span><span class="p">,</span>
                           <span class="bp">self</span><span class="o">.</span><span class="n">_execute_callable_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="p">,</span>
                                                        <span class="n">purge_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<div class="viewcode-block" id="ParallelFlow._create_execute_task"><a class="viewcode-back" href="../../../mdp/mdp.parallel.html#mdp.parallel.ParallelFlow._create_execute_task">[docs]</a>    <span class="k">def</span> <span class="nf">_create_execute_task</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create and return a single execution task.</span>

<span class="sd">        Returns None if data iterator end is reached.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># TODO: check if forked task is forkable before enforcing caching</span>
            <span class="k">return</span> <span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span></div>

    <span class="k">def</span> <span class="nf">get_task</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a task either for either training or execution.</span>

<span class="sd">        A a one task buffer is used to make task_available work.</span>
<span class="sd">        tasks are available as long as need_result returns False or all the</span>
<span class="sd">        training / execution is done. If no tasks are available a</span>
<span class="sd">        NoTaskException is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_train_task</span><span class="p">()</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_execute_task</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">NoTaskException</span><span class="p">(</span><span class="s2">&quot;No data available for execution task.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">task</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">NoTaskException</span><span class="p">(</span><span class="s2">&quot;No task available for execution.&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_parallel_training</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return True if parallel training is underway.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_parallel_executing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return True if parallel execution is underway.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">task_available</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return True if tasks are available, otherwise False.</span>

<span class="sd">        If False is returned this can indicate that results are needed to</span>
<span class="sd">        continue training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_task</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">use_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Use the result from the scheduler.</span>

<span class="sd">        During parallel training this will start the next training phase.</span>
<span class="sd">        For parallel execution this will return the result, like a normal</span>
<span class="sd">        execute would.</span>

<span class="sd">        results -- Iterable containing the results, normally the return value</span>
<span class="sd">            of scheduler.ResultContainer.get_results().</span>
<span class="sd">            The individual results can be the return values of the tasks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_training</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                <span class="c1"># the flownode contains the original nodes</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;finished parallel training phase of node no. &quot;</span> <span class="o">+</span>
                       <span class="s2">&quot;</span><span class="si">%d</span><span class="s2"> in parallel flow&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_stop_training_hook</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">stop_training</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_stop_training_hook</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span><span class="p">]</span><span class="o">.</span><span class="n">is_training</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_next_train_phase</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_executing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_exec_data_iterator</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">use_execute_fork</span><span class="p">():</span>
                <span class="n">flownodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">flownode</span> <span class="ow">in</span> <span class="n">flownodes</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">flownode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_flownode</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">flownode</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">n</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ParallelCheckpointFlow</span><span class="p">(</span><span class="n">ParallelFlow</span><span class="p">,</span> <span class="n">mdp</span><span class="o">.</span><span class="n">CheckpointFlow</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Parallel version of CheckpointFlow.</span>

<span class="sd">    Note that train phases are always closed, so e.g. CheckpointSaveFunction</span>
<span class="sd">    should not expect open train phases. This is necessary since otherwise</span>
<span class="sd">    stop_training() would be called remotely.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the internal variables.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoints</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelCheckpointFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span>
                                                     <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                                                     <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_iterables</span><span class="p">,</span> <span class="n">checkpoints</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">train_callable_class</span><span class="o">=</span><span class="n">FlowTrainCallable</span><span class="p">,</span>
              <span class="n">overwrite_result_container</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train all trainable nodes in the flow.</span>

<span class="sd">        Same as the train method in ParallelFlow, but with additional support</span>
<span class="sd">        of checkpoint functions as in CheckpointFlow.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelCheckpointFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                        <span class="n">data_iterables</span><span class="o">=</span><span class="n">data_iterables</span><span class="p">,</span>
                        <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
                        <span class="n">train_callable_class</span><span class="o">=</span><span class="n">train_callable_class</span><span class="p">,</span>
                        <span class="n">overwrite_result_container</span><span class="o">=</span><span class="n">overwrite_result_container</span><span class="p">,</span>
                        <span class="n">checkpoints</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_parallel_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_iterables</span><span class="p">,</span> <span class="n">checkpoints</span><span class="p">,</span>
                                <span class="n">train_callable_class</span><span class="o">=</span><span class="n">FlowTrainCallable</span><span class="p">,</span>
                                <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Checkpoint version of parallel training.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_check_checkpoints</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelCheckpointFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">setup_parallel_training</span><span class="p">(</span>
                                    <span class="n">data_iterables</span><span class="p">,</span>
                                    <span class="n">train_callable_class</span><span class="o">=</span><span class="n">train_callable_class</span><span class="p">,</span>
                                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="ParallelCheckpointFlow._post_stop_training_hook"><a class="viewcode-back" href="../../../mdp/mdp.parallel.html#mdp.parallel.ParallelCheckpointFlow._post_stop_training_hook">[docs]</a>    <span class="k">def</span> <span class="nf">_post_stop_training_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if we reached a checkpoint.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelCheckpointFlow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_post_stop_training_hook</span><span class="p">()</span>
        <span class="n">i_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_i_train_node</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="n">i_node</span><span class="p">]</span><span class="o">.</span><span class="n">get_remaining_train_phase</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">i_node</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_checkpoints</span><span class="p">))</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoints</span><span class="p">[</span><span class="n">i_node</span><span class="p">]):</span>
                <span class="nb">dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoints</span><span class="p">[</span><span class="n">i_node</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">[</span><span class="n">i_node</span><span class="p">])</span>
                <span class="c1"># store result, just like in the original CheckpointFlow</span>
                <span class="k">if</span> <span class="nb">dict</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span></div>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
      Last updated on 2020-02-17 1:33:02 PM Coordinated Universal Time.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'3.5',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/language_data.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   
<div class="footer">
    <hr />
    <table>
      <tr>
        <td class="footer-left">
           <a href="http://sourceforge.net/projects/mdp-toolkit">
 <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=116959&amp;type=12"
      width="120" height="30" border="0" alt="MDP@SF.NET"/> </a>
        </td>
        <td class="footer-center">
          Last updated on
             2020-02-17 1:33:02 PM Coordinated Universal Time
        </td>
        <td class="footer-right">
         <form class="search" action="../../../search.html" method="get">
          <input type="submit" value="Search" />
          <input type="text" name="q" size="18" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
         </form>
        </td>
    </table>  
    <!-- Piwik -->
    <script type="text/javascript">
	var pkBaseURL = (("https:" == document.location.protocol) ? "https://sourceforge.net/apps/piwik/mdp-toolkit/" : "http://sourceforge.net/apps/piwik/mdp-toolkit/");
	document.write(unescape("%3Cscript src='" + pkBaseURL + "piwik.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
	piwik_action_name = '';
	piwik_idsite = 1;
	piwik_url = pkBaseURL + "piwik.php";
	piwik_log(piwik_action_name, piwik_idsite, piwik_url);
    </script>
    <object><noscript>
	    <p>
		<img src="http://sourceforge.net/apps/piwik/mdp-toolkit/piwik.php?idsite=1"
		     alt="piwik" />
	    </p>
    </noscript></object>
    <!-- End Piwik Tag -->
</div>   


</body>
</html>